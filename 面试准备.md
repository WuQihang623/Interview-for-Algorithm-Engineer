<h1 style="text-align:center;">面试内容
</h1>
<h2 style="text-align:center;"> 计算机基础知识</h2>

### 如何将RGB图像转成灰度图像

常见的转换方法是将RGB三个通道的值按照一定的权重进行加权平均，得到对应的灰度值。这些权重通常是根据人眼对不同颜色的敏感程度来确定的，例如常用的权重系数是：**灰度值 = 0.2989 * R + 0.5870 * G + 0.1140 * B**

将灰度图转成RGB转换过程中会丢失颜色信息。一种简单的方法是将灰度值作为RGB的三个通道的值。另一种常见的方法是使用伪彩色映射（Pseudocolor mapping），即将灰度图的灰度值映射到一个伪彩色表中，将不同灰度值映射到不同的颜色上。

### 你了解设计模式吗？在python中如何实现单例模式？

- 单例模式（Singleton Pattern）：确保一个类只有一个实例，并提供一个全局访问点。
- 工厂模式（Factory Pattern）：定义一个用于创建对象的接口，但将实际的创建工作推迟到子类中。
- 抽象工厂模式（Abstract Factory Pattern）：提供一个创建一系列相关或相互依赖对象的接口，而无需指定其具体类。
- 建造者模式（Builder Pattern）：将一个复杂对象的构建与其表示分离，使得同样的构建过程可以创建不同的表示。

#### 如何实现单例模式

1. 在定义类前采用修饰器@singleton

2. 使用了类方法 `__new__`，在对象创建之前进行检查，确保只有一个实例被创建，并且保证了线程安全性。

   ```python
   class Singleton:
       _instance = None
   
       def __new__(cls, *args, **kwargs):
           if cls._instance is None:
               cls._instance = super().__new__(cls, *args, **kwargs)
           return cls._instance
   
   class MyClass(Singleton):
       def __init__(self, name):
           self.name = name
   
   # 创建实例
   obj1 = MyClass("instance1")
   obj2 = MyClass("instance2")
   
   print(obj1.name)  # 输出：instance1
   print(obj2.name)  # 输出：instance1（因为它是同一个实例）
   ```

   #### 单例模式的优点：

   1. **全局唯一实例：** 保证一个类只有一个实例，对于频繁使用的对象，可以节省系统资源。
   2. **延迟实例化：** 可以延迟实例的创建，直到需要时再进行初始化。
   3. **避免竞态条件：** 在多线程环境中，单例模式可以避免竞态条件，确保只有一个实例被创建。

   #### 单例模式的缺点：

   1. **全局状态：** 单例模式引入了全局状态，可能导致程序的复杂性增加。
   2. **隐藏依赖：** 单例模式可能隐藏了类之间的依赖关系，使得代码难以理解和维护。

   #### 适用场景：

   1. 当一个系统只需要一个实例来协调行为时。

   2. 当实例需要被频繁访问，而不希望通过参数传递来获取实例。

   3. 当一个类的实例需要被全局访问，并且这个实例负责协调操作。

   #### 如何实现工厂模式

   ```pyth
   class Product:
       def operation(self):
           pass
   
   class ConcreteProduct1(Product):
       def operation(self):
           return "Operation from ConcreteProduct1"
   
   class ConcreteProduct2(Product):
       def operation(self):
           return "Operation from ConcreteProduct2"
   
   class SimpleFactory:
       @staticmethod
       def create_product(product_type):
           if product_type == "product1":
               return ConcreteProduct1()
           elif product_type == "product2":
               return ConcreteProduct2()
           else:
               raise ValueError("Invalid product type")
   
   # 使用工厂创建对象
   factory = SimpleFactory()
   product1 = factory.create_product("product1")
   product2 = factory.create_product("product2")
   
   print(product1.operation())  # 输出：Operation from ConcreteProduct1
   print(product2.operation())  # 输出：Operation from ConcreteProduct2
   
   ```

   #### 工厂模式的优点：

   1. **封装对象创建逻辑：** 工厂模式将对象的创建过程封装在工厂类中，客户端无需了解对象的创建过程，只需通过工厂类获取对象。
   2. **可扩展性：** 新增产品类时，只需要扩展工厂类而不需要修改客户端代码，符合开闭原则。
   3. **解耦：** 将具体产品类和客户端代码解耦，使得系统更易于维护和扩展。

   #### 工厂模式的缺点：

   1. **类数量增多：** 随着产品类的增多，可能导致工厂类的数量也增多，使得代码结构复杂。
   2. **不符合开闭原则的变体：** 在简单工厂模式中，每次新增产品都需要修改工厂类，不符合开闭原则。

   #### 工厂模式适用情景：

   1. 当一个系统需要独立于其创建、组合和表示时。

   2. 当一个系统需要多个产品系列中的一种，并且客户端不关心这些对象的创建细节。

   3. 当一个系统需要动态配置对象。

   

### C++有什么特性？

1. **面向对象编程（Object-Oriented Programming, OOP）**：C++ 支持面向对象编程，包括类、对象、继承、多态等概念，使得代码结构更加模块化和可重用。
2. **泛型编程（Generic Programming）**：C++ 支持泛型编程，通过模板（template）实现通用的算法和数据结构，提高了代码的灵活性和复用性。
3. **多重继承（Multiple Inheritance）**：C++ 允许一个类继承自多个基类，这使得在设计复杂系统时能够更好地表达对象之间的关系。
4. **内存管理**：C++ 具有灵活的内存管理能力，包括手动内存管理（使用 new 和 delete 操作符）和智能指针（如 std::shared_ptr、std::unique_ptr），可以有效地管理内存资源。
5. **运算符重载（Operator Overloading）**：C++ 允许用户重载运算符，使得用户定义的类型可以像内置类型一样使用运算符进行操作。
6. **异常处理（Exception Handling）**：C++ 提供了异常处理机制，通过 try-catch 块捕获和处理异常，使得程序能够更加健壮和可靠。
7. **标准模板库（Standard Template Library, STL）**：STL 是 C++ 标准库的一部分，包含了丰富的通用数据结构和算法，如容器（vector、list、map 等）和算法（排序、查找、遍历等），提高了编程效率和代码质量。
8. **高效的性能**：C++ 是一种高性能的语言，具有接近于底层的控制和优化能力，适用于开发对性能要求较高的系统和应用程序。
9. **平台独立性**：C++ 是一种跨平台的语言，可以在多种操作系统上编译和运行，如 Windows、Linux、macOS 等。
10. **友元函数和友元类（Friend Function and Friend Class）**：C++ 允许将函数或类声明为另一个类的友元，使得这些函数或类能够访问该类的私有成员。
11. **支持低级操作**：C++ 允许直接操作内存，包括指针、引用、位操作等，使得开发者能够更加灵活地进行系统级别的编程。

#### 面向对象编程的特点和核心概念：

1. **类与对象**：
   - 类（Class）是一种用户自定义的数据类型，用于描述一类对象的共同属性和行为。类可以看作是对象的模板或蓝图。
   - 对象（Object）是类的实例，具体化了类的属性和行为。对象是程序中的实体，具有状态、行为和标识。
2. **封装（Encapsulation）**：
   - 封装是指将数据和操作封装在类的内部，隐藏了对象的内部实现细节，只提供公共的接口供外部访问。
   - 封装可以提高代码的安全性和可维护性，减少了对象之间的耦合度。
3. **继承（Inheritance）**：
   - 继承是一种机制，允许一个类（子类）继承另一个类（父类）的属性和方法，并且可以在此基础上进行扩展或修改。
   - 继承可以提高代码的重用性，减少了重复编写代码的工作量。
4. **多态（Polymorphism）**：
   - 多态是指同一个操作作用于不同的对象上时，可以产生不同的行为。
   - 多态通过函数重载、运算符重载和虚函数等机制实现，使得程序更加灵活和可扩展。

面向对象编程的主要目标是提高代码的**重用性、可扩展性和可维护性**，通过封装、继承和多态等特性，使得程序结构更加清晰和易于理解。

#### 标准模板库

STL 主要包含以下三个组件：

1. **容器（Containers）**： 容器是用来存储数据的数据结构，STL 提供了多种类型的容器，包括顺序容器和关联容器。
   - 顺序容器（Sequential Containers）：如 vector、list、deque、array 等，用于按顺序存储和访问元素。
   - 关联容器（Associative Containers）：如 set、map、multiset、multimap 等，用于按键值进行快速查找和访问元素。
2. **算法（Algorithms）**： 算法是对容器中的数据进行操作和处理的函数，STL 提供了一系列的通用算法，如排序、查找、合并、计算等。 这些算法通过迭代器（Iterators）来访问容器中的元素，使得算法与数据结构解耦合，提高了算法的复用性和灵活性。
3. **迭代器（Iterators）**： 迭代器是一种类似于指针的对象，用于在容器中遍历和访问元素，STL 提供了多种类型的迭代器，如输入迭代器、输出迭代器、正向迭代器、随机访问迭代器等。 迭代器提供了统一的接口，使得算法可以在不同类型的容器上进行操作，而无需关心容器的具体实现细节。

#### 虚函数的概念

通过在基类中声明虚函数，派生类可以覆盖（override）基类中的虚函数，从而在运行时确定调用的是哪个版本的函数。这种机制称为动态绑定（dynamic binding）或运行时多态（runtime polymorphism）。

**1. 在基类中声明虚函数：** 在基类中将函数声明为虚函数，使用 `virtual` 关键字进行修饰。这样，派生类可以选择性地覆盖基类中的虚函数。

```C++
class Base {
public:
    virtual void show() {
        cout << "Base class function" << endl;
    }
};
```

**2. 在派生类中覆盖虚函数：** 派生类可以重新定义基类中的虚函数，实现自己特定的行为。派生类中的函数声明必须与基类中的虚函数声明相匹配。

```C++
class Derived : public Base {
public:
    void show() override {
        cout << "Derived class function" << endl;
    }
};
```

3. **动态绑定：** 当基类指针（或引用）指向派生类对象时，通过虚函数的动态绑定机制，在运行时确定调用的是派生类中的版本还是基类中的版本。这样可以根据对象的实际类型来调用对应的函数。

```C++
int main() {
    Base* ptr = new Derived();
    ptr->show(); // 调用的是Derived类中的show函数
    delete ptr;
    return 0;
}
```

### C++构造函数和析构函数可以是虚函数吗

1. 构造函数不可以是虚构函数

2. 析构函数常常是虚函数：析构函数可以声明为虚函数，且类有继承时，析构函数常常必须为虚函数。

   若析构函数是虚函数，delete 时，基类和子类都会被释放；若析构函数不是虚函数，delete 时，只有基类会被释放，而子类没有释放，存在内存泄漏的隐患。

   

### C++继承时，构造函数的调用顺序

在子类的构造函数中，首先会调用父类的构造函数，然后再执行子类自己的构造函数体。

### const成员函数

这些函数在被调用时不会修改对象的状态。`const`成员函数内部不能修改类的非静态成员变量，但可以调用其他`const`成员函数，可以访问和调用类的成员变量，但不能修改非`mutable`成员变量的值。

```C++
class MyClass {
public:
    void nonConstFunction() {
        // 这个函数可以修改对象的状态
    }

    void constFunction() const {
        // 这个函数不会修改对象的状态
    }
};
```

### static成员函数

静态成员函数与非静态成员函数有所不同，它们不依赖于任何特定的对象实例，因此**可以在不创建对象实例的情况下直接调用**。**静态成员函数可以访问类的静态成员变量和其他静态成员函数，但不能访问非静态成员变量或非静态成员函数。**

```C++
class MyClass {
private:
    static int staticMember;

public:
    static void staticMemberFunction() {
        std::cout << "Static member function called." << std::endl;
        std::cout << "Static member value: " << staticMember << std::endl;
    }

    static int getStaticMember() {
        return staticMember;
    }
};
```

### 内联函数

通过在函数定义前加上关键字 `inline` 来将函数声明为内联函数。编译器会尽量将内联函数的调用处替换为函数体的实际代码，以提高程序的执行效率。但是，编译器会根据情况决定是否真正进行内联展开，因此 `inline` 关键字只是对编译器的一个建议，编译器有权忽略这个建议。

1. 减少了函数调用的开销，因为直接展开代码而不是通过调用栈来执行。
2. 提高了程序的执行效率，特别是对于频繁调用的短小函数。
3. 可以避免一些宏定义带来的副作用和错误。

但是，内联函数也有一些限制和缺点：

1. 内联函数的代码会被直接插入到调用处，可能会导致程序体积增大。
2. 内联函数适合用于短小的函数，如果函数体过长，可能会导致代码膨胀，甚至降低程序的性能。
3. 每个调用内联函数的地方都要复制一份函数的代码，可能会导致代码冗余。



### 友元函数

友元函数的声明通常放在类的声明中，并在函数声明前加上 `friend` 关键字。这样做的效果是，声明为友元函数的函数可以直接访问类的私有成员和保护成员，就像它们是类的成员函数一样。

### vector扩容

标准库中的 `std::vector` 是一种动态数组，它可以根据需要动态地增长或缩小其大小。

1. 当向 `std::vector` 中添加新元素时，会首先检查容器的当前大小（即元素个数）和容量（即可容纳的元素个数）。
2. 如果当前大小小于容量，则说明向 `std::vector` 添加新元素时不需要扩容，直接将元素添加到数组末尾即可。
3. 如果当前大小等于容量，则说明容器已经满了，需要进行扩容操作。通常情况下，容量的增长是按照某种固定的倍数进行的（如翻倍或加倍）。
4. 扩容操作包括分配新的内存空间，并将原有元素拷贝到新的内存空间中。



### 程序运行时堆和栈的区别

**存储结构上：**

栈是一种线性结构，存储方式是先进后出（FILO）。数据在栈中是连续存储的，栈的空间由系统自动管理，通常会在程序运行时分配一块固定大小的内存。每当一个函数被调用时，系统会自动为该函数分配一块栈空间，函数的局部变量和函数参数等数据都存储在这个栈空间中。

堆是一种非连续的存储结构，它的存储方式没有固定的顺序。堆的空间由程序员手动分配和释放，通常通过 `new` 和 `delete`、`malloc` 和 `free` 等操作来进行管理。

**生命周期：**

栈中的数据的生命周期与函数调用的生命周期相关联。

堆中的数据的生命周期由程序员手动控制。



### vector是怎么删除元素的，时间复杂度

1. 使用 `erase()` 函数删除指定位置的元素， 时间复杂度为`O(n)`，要将元素往前移：

```C++
std::vector<int> vec = {1, 2, 3, 4, 5};
vec.erase(vec.begin() + 2); // 删除索引为2的元素（值为3）
```

2. 使用 `pop_back()` 函数删除末尾的元素， 时间复杂度为`O(1)`：

```C++
std::vector<int> vec = {1, 2, 3, 4, 5};
vec.pop_back(); // 删除末尾的元素（值为5）
```



### 指针和引用的区别

**操作符：**

指针使用 `*` 来声明指针变量和解引用操作符，使用 `&` 来获取变量的地址。

引用使用 `&` 来声明引用变量，并在声明时必须初始化为另一个变量，之后就可以像使用原始变量一样使用引用。

**nullptr：**指针可以是空指针（nullptr），引用不能是空的

**重复赋值：**

指针可以在其生命周期内多次改变指向的对象，可以重新赋值为不同的地址。

引用一旦初始化后，就不能改变引用的对象，它始终引用同一个对象，因此无法重新赋值为其他对象。

**内存管理：**

指针需要手动管理内存，需要显式地分配和释放内存，存在内存泄漏和悬空指针的风险。

引用不需要手动管理内存，它只是给变量起了一个别名，不占用额外的内存，因此不会出现内存泄漏和悬空引用的问题。



### 数组，链表，哈希表的区别

**数组：**

数组是一种线性结构，由相同类型的元素按顺序排列组成。

数组的元素在内存中是连续存储的，因此可以通过索引直接访问任何元素。

数组的大小是固定的，一旦分配了空间，就不能动态改变数组的大小。

插入和删除元素时，需要移动其他元素以保持连续性，因此在插入和删除操作上的效率较低。

**链表：**

链表是一种线性结构，由一系列节点组成，每个节点包含数据和指向下一个节点的指针。

链表的节点在内存中可以是不连续的，因此可以动态地分配和释放节点。

链表的大小可以动态增长或缩小，不受固定大小的限制。

插入和删除元素时，只需要改变节点的指针，不需要移动其他元素，因此在插入和删除操作上的效率较高。

**哈希表：**

哈希表是一种键值对存储结构，通过哈希函数将键映射到存储桶（Bucket）中。

哈希表的查找、插入和删除操作的平均时间复杂度是常数时间 O(1)，但最坏情况下可能会退化为线性时间 O(n)

哈希表的空间利用率通常较高，但是需要根据实际情况合理选择哈希函数和解决冲突的方法。



### 拷贝构造

拷贝构造函数（Copy Constructor）是C++中的特殊构造函数之一，用于创建一个新对象并将其初始化为另一个同类型的对象的副本。

```C++
class MyClass {
public:
    // 拷贝构造函数
    MyClass(const MyClass& other) {
        // 在这里进行拷贝构造的具体操作，通常是将成员变量逐一复制
        // 可以通过 const 引用来避免对参数对象进行修改
    }
};
```

在拷贝构造函数中，通常需要将参数对象的成员变量逐一复制到新对象中，以创建一个新对象并保留原始对象的副本。为了避免修改参数对象，通常将参数对象声明为 **const 引用**。

如果没有显式定义拷贝构造函数，编译器会自动生成一个默认的拷贝构造函数。默认的拷贝构造函数会对对象的每个成员变量进行**浅复制**。

但是对于包含**指针成员**的类，通常需要自定义拷贝构造函数以执行**深度复制**，以避免浅复制带来的问题，比如浅拷贝可能会导致多个对象共享同一块内存空间，当一个对象的内存释放时，会影响到其他对象的数据。



### 浅拷贝和深拷贝

浅拷贝是指将一个对象的值复制到另一个对象，对于指针成员，只是简单地复制指针的值（地址），而不是复制指针所指向的内容。

深拷贝是指将一个对象的值复制到另一个对象，对于指针成员，会创建一个新的内存空间，并将指针所指向的内容也复制一份到新的内存空间中。

```C++
class ShallowCopy {
public:
    int* data;

    ShallowCopy(const ShallowCopy& other) : data(other.data) {}  // 浅拷贝构造函数

    // 析构函数释放 data 指向的内存
    ~ShallowCopy() { delete data; }
};

class DeepCopy {
public:
    int* data;

    DeepCopy(const DeepCopy& other) : data(new int(*other.data)) {}  // 深拷贝构造函数

    // 析构函数释放 data 指向的内存
    ~DeepCopy() { delete data; }
};
```



### Python与C++的区别

1. **语法和风格：**
   - Python 的语法简洁清晰，采用缩进来表示代码块，没有大括号；而 C++ 的语法更加严格，采用分号和大括号来表示代码块。
   - Python 是一种解释型语言，代码无需编译即可执行；而 C++ 是一种编译型语言，需要先编译成机器码再执行。
2. **类型系统：**
   - Python 是一种动态类型语言，变量的类型在运行时确定，无需显式声明；而 C++ 是一种静态类型语言，变量的类型在编译时确定，需要显式声明。
   - Python 的变量可以指向任意类型的对象；而 C++ 的变量必须指定特定类型，并且类型在编译时是固定的。
3. **内存管理：**
   - Python 使用自动内存管理机制（垃圾回收），通过引用计数和循环垃圾收集来管理内存；而 C++ 需要程序员手动管理内存，包括分配和释放内存。
   - Python 的内存管理机制简化了程序开发，但可能导致性能损失和内存泄漏；而 C++ 的手动内存管理更加灵活，但需要程序员更加小心地处理内存。
4. **面向对象编程：**
   - Python 和 C++ 都支持面向对象编程，但其实现方式有所不同。
   - Python 的面向对象特性更加简单和灵活，支持动态继承、动态绑定等特性；而 C++ 的面向对象特性更加丰富，包括多重继承、虚函数、纯虚函数等。
5. **标准库和生态系统：**
   - Python 拥有丰富的标准库和第三方库，涵盖了几乎所有领域的应用开发，如科学计算、网络编程、Web 开发等；而 C++ 的标准库和第三方库相对较少，需要程序员自己去选择和引入。
6. **适用场景：**
   - Python 适用于快速开发、原型设计、数据分析、科学计算等领域，尤其擅长处理复杂的业务逻辑和高层次的任务。
   - C++ 适用于系统编程、游戏开发、嵌入式系统、性能优化等领域，尤其擅长处理底层的系统级别的任务和对性能要求较高的场景。

### 在项目中使用过多线程吗，是如何实现的？

1. **线程创建：** 程序员可以使用操作系统提供的线程库（如 POSIX 线程库 pthreads）或语言级别的多线程库（如 Java 中的 `Thread` 类）来创建线程。线程创建时需要指定线程函数或线程对象，并传递相应的参数。
2. **线程调度：** 操作系统负责线程的调度和管理，根据线程的优先级、状态和调度策略来决定哪个线程可以获得 CPU 时间片执行任务。常见的调度策略包括先来先服务（FCFS）、最短作业优先（SJF）、轮转调度（Round Robin）等。
3. **线程同步：** 多线程程序中，可能会存在共享资源的竞争和冲突，需要通过同步机制来确保多个线程之间的数据访问安全。常见的线程同步机制包括互斥锁（Mutex）、信号量（Semaphore）、条件变量（Condition Variable）等。
4. **线程通信：** 多线程程序中的线程之间可能需要进行通信和协作，以实现共同的任务目标。线程通信机制可以通过共享内存、消息队列、管道等方式来实现。
5. **线程销毁：** 线程执行完成后，需要及时释放线程所占用的资源，以防止资源泄漏和系统资源浪费。操作系统负责线程的销毁和资源回收。

#### 1. 互斥锁（Mutex）：

**概念：** 互斥锁是一种用于保护临界区的同步机制，用于确保在同一时刻只有一个线程可以访问共享资源。当一个线程进入临界区时，它会尝试获得互斥锁，如果互斥锁已被其他线程占用，则该线程会被阻塞，直到互斥锁被释放。

**特点：**

- 互斥锁是一种二进制锁，只有两种状态：锁定和解锁。
- 只有成功获得互斥锁的线程才能进入临界区，其他线程被阻塞。
- 互斥锁是一种独占锁，只能由获得锁的线程释放。

#### 2. 信号量（Semaphore）：

**概念：** 信号量是一种计数器，用于控制同时访问共享资源的线程数量。当一个线程进入临界区时，它会尝试获取信号量，如果信号量的值大于 0，则表示资源可用，线程可以继续执行；如果信号量的值等于 0，则表示资源不可用，线程会被阻塞，直到资源可用。

**特点：**

- 信号量是一种整数型变量，可以有多种取值。
- 信号量可以用于解决生产者-消费者问题、限流等场景。
- 信号量可以是二进制信号量（取值为 0 或 1）或计数信号量（取值大于等于 0）。

#### 3. 条件变量（Condition Variable）：

**概念：** 条件变量是一种线程间的通信机制，用于在多线程环境中实现线程的等待和唤醒操作。条件变量通常与互斥锁配合使用，等待线程在条件不满足时会阻塞，并释放互斥锁；当条件满足时，唤醒线程并重新获取互斥锁，继续执行。

**特点：**

- 条件变量用于解决线程间的同步和通信问题，允许线程在特定条件下等待或唤醒。
- 条件变量通常与互斥锁一起使用，互斥锁用于保护条件变量的访问和修改。



### TCP与UDP有什么区别，各有什么优劣？

TCP（传输控制协议）和UDP（用户数据报协议）是两种不同的传输层协议，用于在计算机网络中传输数据。它们之间的主要差异包括：

1. **连接性**：
   - TCP 是面向连接的协议，它在通信之前需要建立连接，然后在通信结束时释放连接。这种连接方式保证了数据的可靠性和顺序性。
   - UDP 是无连接的协议，通信双方在发送数据之前不需要建立连接，也不需要维护连接状态。因此，UDP 的通信速度通常比 TCP 快，但不具备 TCP 那种可靠性和顺序性。
2. **可靠性**：
   - TCP 提供可靠的数据传输。它通过序号、确认和重传机制来确保数据的可靠性，即使在网络出现丢包或者出现延迟的情况下，也能够保证数据的完整性和顺序性。
   - UDP 不提供可靠的数据传输保证。它仅仅提供了数据的最基本的传输功能，不对数据进行确认和重传，因此在网络不可靠或者对实时性要求较高的情况下使用。
3. **头部开销**：
   - TCP 的头部开销相对较大，因为它需要包含序号、确认号、窗口大小等控制信息，以及可选的选项字段。
   - UDP 的头部开销较小，只包含了源端口、目标端口、长度和校验和等基本信息，因此 UDP 的头部开销比 TCP 小。
4. **适用场景**：
   - TCP 适用于对数据传输可靠性要求较高的场景，如文件传输、网页浏览、电子邮件等。
   - UDP 适用于实时性要求较高，且对数据可靠性要求不高的场景，如音视频流媒体、在线游戏等。

**TCP优点：**

- 可靠性高，适用于对数据传输可靠性要求较高的场景，如文件传输、网页浏览等。

**TCP缺点：**

- 建立连接和断开连接的开销较大，影响了数据传输的实时性。
- 数据传输的实时性较差，不适用于对实时性要求较高的场景，如实时视频传输、游戏等。

**UDP优点：**

- 建立连接和断开连接的开销小，适用于对数据传输实时性要求较高的场景，如实时音频传输、视频会议等。
- 数据传输的实时性好，不受连接建立和断开的影响。

**UDP缺点：**

- 不可靠性高，数据传输可能会丢失或乱序，需要应用层进行数据重传和确认。
- 不提供流量控制机制，容易导致网络拥塞，影响数据传输的稳定性。

#### TCP为什么要经历三次握手

1. **确认双方能够正常收发数据：** 第一次握手中，客户端向服务器发送连接请求报文段，服务器收到后进行确认，并进入 SYN-RCVD 状态；第二次握手中，服务器向客户端发送连接确认报文段，客户端收到后也进入 ESTABLISHED 状态；第三次握手中，客户端向服务器发送确认报文段，服务器收到后也进入 ESTABLISHED 状态。通过这个过程，确保了双方都能够正常收发数据。
2. **防止已失效的连接请求报文段被服务端误认为是新的连接请求：** 如果只进行两次握手，那么在某些情况下，客户端发送的连接请求报文段可能会在网络中被延迟，导致服务端收到后进行确认，但客户端并未收到确认，此时客户端会认为连接未建立成功，并重新发送连接请求。而服务端可能会收到之前已经确认过的连接请求报文段，并错误地认为是新的连接请求。因此，通过第三次握手，可以避免这种情况发生，确保已失效的连接请求报文段不会被误认为是新的连接请求。
3. **确保连接的可靠性：** 三次握手过程中，客户端和服务端都有机会确认对方的身份和能力，并且在第三次握手中，客户端和服务端都对连接进行了确认，从而确保了连接的可靠性和稳定性。



### TCP的四次挥手过程

终止一个已建立的 TCP 连接。这个过程涉及到两端（客户端和服务器端）互相发送控制信息以关闭连接。

1. 第一次挥手：客户端向服务器端发送一个 TCP 报文段，设置 FIN 标志位，表示客户端已经没有数据要发送了，但仍然可以接收数据。客户端进入 FIN_WAIT_1 状态，等待服务器端的确认。
2. 第二次挥手：服务器端收到客户端发送的 FIN 报文段后，会发送一个 ACK 报文段作为确认。此时服务器端进入 CLOSE_WAIT 状态，表示已经收到了客户端的关闭请求，但服务器端还有数据需要发送给客户端。
3. 第三次挥手：当服务器端确定数据都发送完毕后，会向客户端发送一个 FIN 报文段，表示服务器端已经没有数据要发送了。服务器端进入 LAST_ACK 状态。
4. 第四次挥手：客户端收到服务器端发送的 FIN 报文段后，客户端会发送一个 ACK 报文段作为确认。客户端进入 TIME_WAIT 状态，等待可能出现的延迟报文。



### 介绍虚继承

虚继承的主要作用是解决由多重继承导致的菱形继承（Diamond Inheritance）问题。在多重继承中，如果一个类同时继承自两个或多个具有共同基类的类，而这些共同基类又有一个共同的派生类，就会形成菱形继承的结构。这样的继承结构可能会导致问题，例如数据成员在派生类中出现多次，导致冗余或不一致性。**虚继承通过在派生类对共同基类进行虚继承来解决这个问题。使用虚继承后，共同基类在继承体系中只会被派生类保留一份**

```C++
class Base {
public:
    int data;
};

class Derived1 : virtual public Base {
    // Derived1 可以通过 Base 继承的 data
};

class Derived2 : virtual public Base {
    // Derived2 可以通过 Base 继承的 data
};

class FinalDerived : public Derived1, public Derived2 {
    // 可以直接访问 Base 继承的 data，而不会有二义性
};
```

### C++的结构体内存对齐

尽管内存是以字节为单位，但是大部分处理器并不是按字节块来存取内存的.它一般会以双字节,四字节,8字节,16字节甚至32字节为单位来存取内存，我们将上述这些存取单位称为内存存取粒度。32位系统处理器只能从地址为4的倍数的内存开始读取数据。内存对齐的目的是为了提高内存访问的效率，减少内存访问的次数，以及避免因为非对齐内存访问而引起的性能损失。

C++ 中结构体的内存对齐规则如下：

1. 结构体的每个成员变量的起始地址必须是其自身大小的整数倍。
2. 结构体的大小必须是其最大成员大小的整数倍。

### Linux常用命令

`ls`: 列出当前目录的内容。

`ls -l`: 以长格式列出当前目录的内容，包括文件权限、所有者、大小等信息。

`ls -a`: 列出当前目录的所有内容，包括隐藏文件。

`pwd`: 显示当前工作目录的完整路径。

`rm -f`: 强制删除，不提示确认。

`rm -r`: 递归删除目录及其内容。

`rmdir`: 删除一个空目录。

`cp -r`: 递归复制目录及其内容。

`cat`: 查看文件内容。

`more` 或 `less`: 分页查看文件内容，可以滚动查看大文件。

`nano` 或 `vi` 或 `vim`: 编辑文件的内容。

`chmod`: 改变文件或目录的权限。

```Ba
-R：递归地修改目录及其下属文件和子目录的权限。
符号：+ 表示添加权限，- 表示删除权限，= 表示设置权限。
权限：r 表示读权限，w 表示写权限，x 表示执行权限。
用户类型：u 表示所有者，g 表示所属组，o 表示其他用户，a 表示所有用户。
chmod u+x file.txt：给文件的所有者添加执行权限。
chmod a=rwx directory：设置目录的所有用户都具有读、写、执行权限。
```

`find`: 在指定目录中搜索文件。

```bash
find 路径 [选项] [表达式]
-name 模式：按照文件名模式进行搜索，模式可以使用通配符。
-type 类型：指定要搜索的文件类型，如 f 表示普通文件，d 表示目录。
-size 大小：按照文件大小进行搜索，可以使用 + 表示大于，- 表示小于，以及 c 表示字节、k 表示千字节、M 表示兆字节等单位。
-mtime +n/-n：按照修改时间进行搜索，+n 表示修改时间在 n 天之前，-n 表示修改时间在 n 天之内。
find /path/to/search -type d -mtime -7 -exec ls -ld {} \;：搜索指定目录下最近 7 天内修改过的目录，并输出详细信息。
```

**linux如何查看某个文件的第几列：**

**linux如何查看某个文件有多少行：**



### git命令

1. `git add .`：将当前目录下的所有修改或新增的文件添加到暂存区。

2. `git commit -m ""`：将暂存区中的文件提交到本地仓库，并附上一条提交说明。

3. `git push`：将本地仓库中的提交推送到远程仓库。

4. `git pull`：从远程仓库获取最新的版本并合并到本地仓库。

5. `git log`：查看提交日志，显示提交历史记录。

6. `git status`：查看当前工作目录状态，显示有关文件的状态信息。

7. `git merge`：将其他分支的修改合并到当前分支。

   git merge <branch-to-merge>

   如果在合并过程中发生冲突（即两个分支上的更改相互冲突），Git 将会暂停合并并将冲突标记在文件中。在这种情况下，你需要手动解决冲突，然后执行 `git merge --continue` 来完成合并。

8. `git checkout`：用于切换分支或恢复文件。

9. `git clone`：从远程仓库克隆项目到本地。

10. `git revert`：撤销某次提交的修改，但会创建一个新的提交来记录撤销操作。

    例如，`git revert <commit>` 将创建一个新的提交，来撤销指定的 `<commit>` 的更改。

11.  `git reset` 可以将当前分支的 HEAD 移动到指定的提交或分支，同时将工作目录和暂存区的内容回滚到指定的状态。它有不同的模式：

    `git reset --soft <commit>`: 保留工作目录和暂存区不变，只移动 HEAD 到指定提交。

    `git reset --mixed <commit>` (默认行为): 移动 HEAD 到指定提交，并重置暂存区，但保留工作目录不变。

    `git reset --hard <commit>`: 移动 HEAD 到指定提交，并重置暂存区和工作目录，使其与指定提交完全一致。



### scp和cp的区别

`scp`和`cp`都是用于文件复制的命令，但它们之间有几个重要的区别：

1. **scp**：是基于SSH协议的安全文件传输工具。用于在本地计算机和远程计算机之间传输文件，可以跨越网络进行文件复制。通过加密传输数据，提供了更高的安全性，适用于在不可信的网络环境中传输文件。
2. **cp**：是Linux/Unix系统中用于复制文件和目录的标准命令。用于在本地文件系统内复制文件，不能直接跨越网络进行文件复制。在本地计算机上复制文件，不涉及网络传输，因此通常比`scp`更快。



### 深度优先遍历和广度优先遍历

#### 广度优先搜索（BFS）：

- **原理**：BFS从图中的某个起始节点开始，首先遍历起始节点的所有邻居节点，然后再依次遍历邻居节点的邻居节点，以此类推，直到遍历完整个图为止。它按层级的顺序逐层遍历，先访问距离起始节点最近的节点。
- **数据结构**：通常使用队列（Queue）来实现BFS，起始节点入队，然后出队时将其邻居节点入队，依次进行。
- **应用**：BFS常用于寻找最短路径、查找图的连通性、生成迷宫等。

#### 深度优先搜索（DFS）：

- **原理**：DFS从图中的某个起始节点开始，首先遍历起始节点的一个邻居节点，然后再以同样的方式遍历该邻居节点的一个邻居节点，依次进行直到到达最深的节点，然后再回溯到前面的节点，以同样的方式遍历其他未被访问过的邻居节点，直到所有节点都被访问过。
- **数据结构**：通常使用递归或者栈（Stack）来实现DFS。递归方式下，函数自己调用自己，栈方式下，手动维护一个栈来保存节点。
- **应用**：DFS常用于图的遍历、拓扑排序、寻找连通分量、解决迷宫问题等。

### 区别：

1. **访问顺序**：BFS按层级顺序遍历，先访问离起始节点最近的节点；DFS先遍历当前节点的一个分支，直到末端再回溯到前面的节点继续搜索其他分支。
2. **数据结构**：BFS通常使用队列，而DFS通常使用递归或者栈。
3. **空间复杂度**：BFS在存储未访问节点时需要较大的空间，因为它需要将所有的当前层节点都存储起来，而DFS则不需要这样做，因此在空间上DFS比BFS更节省。
4. **时间复杂度**：在相同的条件下，BFS和DFS的时间复杂度一样，都是 O*(*V*+*E)，其中V是节点数，E 是边数。
5. **应用场景**：具体应用场景不同。BFS适用于解决最短路径等问题，而DFS适用于拓扑排序、连通分量等问题。



### C++如何做多线程

```C++
#include <iostream>
#include <thread>

// 线程函数
void threadFunction(int threadID) {
    std::cout << "Thread " << threadID << " is running." << std::endl;
}

int main() {
    // 创建并启动多个线程
    std::thread t1(threadFunction, 1);
    std::thread t2(threadFunction, 2);

    // 主线程继续执行其他工作
    std::cout << "Main thread is running." << std::endl;

    // 等待线程执行完毕
    t1.join();
    t2.join();

    // 所有线程执行完毕后，程序结束
    std::cout << "All threads have finished." << std::endl;

    return 0;
}
```

### C++线程和协程的概念

#### 线程：

线程是操作系统调度的基本单位，是程序中独立执行的一段代码。

1. 每个线程都有自己的执行上下文，包括堆栈、寄存器和程序计数器。
2. 线程之间共享进程的内存空间，可以直接访问共享的全局变量和数据。
3. 多线程可以实现并发执行，提高程序的响应性和性能。

C++11 引入了 `std::thread` 类，用于创建和管理线程。通过 `std::thread` 类，可以创建新的线程，并将函数或可调用对象作为线程的入口点。

#### 协程：

协程是一种轻量级的线程，是由程序控制的执行单元，可以在执行过程中暂停和恢复。

1. 协程不依赖于操作系统的线程调度，而是由程序显式控制。
2. 协程之间可以实现协作式的调度，而不是竞争式的调度。
3. 协程可以通过暂停和恢复的操作来避免线程切换的开销，提高程序的效率。
4. C++20 引入了协程（Coroutines）的支持，通过 `co_await`、`co_yield` 等关键字来实现协程。协程的主要特点是可以在函数内部保存状态，并在之后恢复执行，而不需要重新创建新的线程。这使得协程在编写异步代码和状态机等场景下非常有用。

### 为什么要有协程

在并发编程中，协程允许在单个线程内执行多个独立的函数，并且这些函数之间可以暂停和恢复执行，以实现非抢占式的多任务处理。相比于线程和进程，协程的创建和切换成本更低。在异步编程中，协程可以用于编写高效且清晰的非阻塞代码。



### C++垃圾回收机制

C++标准库中并没有提供内置的垃圾回收机制，依赖于手动管理内存。但是现代C++中可以采用如下方法：

1. **智能指针（Smart Pointers）**：C++11引入了智能指针（如`std::shared_ptr`、`std::unique_ptr`、`std::weak_ptr`），它们可以自动管理资源的生命周期，避免了手动管理内存的一些问题，如内存泄漏和悬挂指针。
2. **RAII（Resource Acquisition Is Initialization）**：RAII是一种C++编程技术，它利用对象生命周期与资源的关系来管理资源，通过在对象构造函数中获取资源，析构函数中释放资源，从而确保资源的正确管理。RAII可用于管理任何资源，不仅限于内存。
3. **智能容器（Smart Containers）**：除了智能指针外，还有一些智能容器（如`std::vector`、`std::string`等）可以自动管理动态分配的内存，并提供了更安全和方便的内存管理方式。
4. **垃圾收集库**：虽然C++标准库本身没有提供垃圾回收机制，但可以使用第三方库来实现垃圾收集。例如，有一些开源的C++垃圾收集库（如Boehm GC、Libgc等），可以用于在C++中实现垃圾回收。



### 详细讲讲智能指针

智能指针是C++标准库提供的一种智能内存管理工具，用于管理动态分配的内存资源，自动进行内存分配和释放，从而避免了手动管理内存带来的一系列问题，如内存泄漏、野指针等。C++11引入了三种主要的智能指针：`std::unique_ptr`、`std::shared_ptr`和`std::weak_ptr`。

1. std::unique_ptr:`std::unique_ptr`是一种独占所有权的智能指针，每个指针都独占管理一个对象，不能被复制，但可以被移动。当`std::unique_ptr`超出作用域时，它所管理的资源会被自动释放。

   ```C++
   #include <memory>
   
   int main() {
       std::unique_ptr<int> ptr(new int(5)); // 创建一个指向int类型的独占智能指针
       return 0; // 在main函数结束时，ptr超出作用域，所管理的内存会被自动释放
   }
   ```

2. std::shared_ptr:`std::shared_ptr`是一种共享所有权的智能指针，多个指针可以共同管理一个对象，通过引用计数来跟踪对象的生命周期。当最后一个`std::shared_ptr`指针超出作用域时，它所管理的资源会被释放。

   ```C++
   #include <memory>
   
   int main() {
       std::shared_ptr<int> ptr1(new int(5)); // 创建一个指向int类型的共享智能指针
       std::shared_ptr<int> ptr2 = ptr1; // 多个共享指针指向同一资源
       return 0; // 在main函数结束时，ptr1和ptr2超出作用域，所管理的内存会被自动释放
   }
   ```

3. std::weak_ptr:`std::weak_ptr`是`std::shared_ptr`的一种辅助智能指针，它不会增加引用计数，不会影响对象的生命周期。主要用于解决`std::shared_ptr`循环引用导致的内存泄漏问题。

### 智能指针优缺点

**优点：**1. 自动内存管理，自动管理内存的分配和释放，无需程序员手动释放内存。这减少了内存泄漏的可能性，并提高了程序的健壮性和稳定性。2. 支持多线程安全： `std::shared_ptr`提供了多线程安全的内存管理机制，可以在多线程环境下安全地共享资源。

**缺点：**1.性能开销，智能指针可能会引入一定的性能开销，包括内存分配和释放的额外开销，以及引用计数等机制的运行开销。2.  `std::shared_ptr`可能会出现循环引用的问题，导致资源无法正确释放，从而引发内存泄漏。



### 防止内存泄漏的方法 

内存泄漏是指程序在动态分配内存后，没有释放该内存，导致该内存不能再次被使用，从而造成系统的内存资源浪费。下面是一些防止内存泄漏的方法：

1. **使用智能指针**：
   - 使用智能指针（如`std::unique_ptr`和`std::shared_ptr`）来管理动态分配的内存。这些智能指针可以自动管理资源的生命周期，当指针超出作用域时，会自动释放内存，从而避免了手动释放内存时可能遗漏的问题。
2. **使用 RAII（资源获取即初始化）原则**：
   - 在类的构造函数中分配资源，在析构函数中释放资源。这样可以确保资源在对象生命周期结束时被正确释放，即使在发生异常或者其他错误的情况下也能够正常释放资源。
3. **避免动态分配内存**：
   - 在可能的情况下，尽量避免使用`new`和`delete`进行动态内存分配和释放。可以考虑使用栈上分配的对象或者使用标准库容器来管理对象。



### 如何判定是否离开了作用域

1. **变量的生命周期**：在程序中，每个变量都有其生命周期，即它在程序执行期间存在的时间段。当变量所在的作用域结束时，该变量的生命周期也就结束了。因此，可以通过观察变量的声明位置和其所在的代码块范围来确定变量是否离开了作用域。
2. **变量的析构函数调用**：如果一个对象具有析构函数，在对象离开作用域时，其析构函数将被自动调用。因此，可以通过观察对象的析构函数是否被调用来判断对象是否已经离开了作用域。
3. **智能指针的生命周期**：如果一个对象被智能指针管理，可以通过观察智能指针的生命周期来确定对象是否离开了作用域。当智能指针离开作用域时，它所管理的对象也会被销毁。



### C++内部如何存储复数和浮点数

1. 浮点数（Floating-point numbers）：

- **单精度浮点数（float）**：占用4个字节（32位），由1位符号位、8位指数位和23位尾数位组成。
- **双精度浮点数（double）**：占用8个字节（64位），由1位符号位、11位指数位和52位尾数位组成。

浮点数使用科学计数法来表示，其内部存储格式为：符号位 * 尾数 * (2 ^ 指数)。

2. 复数（Complex numbers）：

C++标准库提供了`std::complex`模板类用于表示复数。复数由实部和虚部组成，分别是浮点数类型。通常实部和虚部可以是`float`、`double`或者`long double`类型。这些类型的存储方式与上述描述的浮点数存储方式相同。



### vector resize和reserve区别

在C++中，`vector`是一个动态数组容器，它可以根据需要动态调整大小以容纳更多的元素。

```C++
// 增加了容器的大小，新元素将被默认初始化。
// 减少了容器的大小，超出新大小的元素将被销毁。
// 可能会导致重新分配内存，因此在调用时会影响到容器的性能
vec.resize(10); // 将vec的大小调整为10
```

```C++
// 用于分配内存以容纳至少指定数量的元素，但不会改变容器中元素的数量。
// 不会初始化新元素，它只是分配足够的内存以容纳后续添加的元素
// 不会影响容器的大小，只是预留了足够的内存空间。
vec.reserve(10); // 为至少 10 个元素分配内存
```



### 缓存命中率

缓存命中率是衡量计算系统中缓存效率的指标之一。它表示在对缓存进行访问时，所获取到的数据在缓存中已经存在的比率。

1. 合理的缓存大小：缓存大小不宜太小，以免无法存储足够多的数据；也不宜太大，以免造成资源浪费。
2. 缓存替换策略：选择合适的替换策略，如最近最少使用（LRU）、先进先出（FIFO）等，以确保缓存中存储的是最有可能被再次访问的数据。
3. 数据局部性：利用空间局部性和时间局部性原理，预测未来可能被访问的数据，并将其缓存起来。
4. 数据预取：根据程序访存的规律，提前将可能被使用的数据加载到缓存中。



### 红黑树

红黑树（Red-Black Tree）是一种**自平衡的二叉查找树**（二叉查找树，左子树的所有值小于根节点，右子树的所有值均大于根节点的值），它保持了良好的平衡性，以确保在最坏情况下的**时间复杂度为 O(log n)**，其中 n 是树中节点的数量。

红黑树的特点：1. 节点是红色或者黑色；2. 根节点是黑色；3. 所有叶子节点都是黑色（nullptr）；4.每个红色节点必须有两个黑色的子节点；5.从任一节点到其每个叶子的所有[简单路径](https://zh.wikipedia.org/wiki/道路_(图论))都包含相同数目的黑色节点。

![An example of a red-black tree](https://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Red-black_tree_example.svg/450px-Red-black_tree_example.svg.png)

优点： 1. 红黑树通过节点的颜色属性来保证了它的自平衡性，确保了树的高度保持在较小的范围内（约为 log(N)），从而保持了快速的查找、插入和删除操作的时间复杂度为 O(logN)。2. 红黑树的自平衡性保证了在执行插入和删除操作后，树的结构会保持平衡，不会出现极端不平衡的情况。

缺点：1. 红黑树的插入和删除操作相对复杂，需要对树的结构进行调整和平衡，这增加了实现的复杂性。2. 红黑树中每个节点需要额外存储颜色信息，这增加了树的空间开销。



### map 和 unordered map区别，空间很多时间很紧，选择哪一个

1. `map` 是基于**红黑树**实现的，因此它的查找、插入和删除操作的时间复杂度是 O(log n).

   优点： 有序性，这是map结构最大的优点；

   缺点：空间占用率高，因为map内部实现了红黑树；

   适用于那些有顺序要求的问题，用map会更高效一些

2. `unordered_map` 是基于**哈希表**实现的，它的查找、插入和删除操作的平均时间复杂度是 O(1)；

   优点：内部实现了哈希表，因此其查找速度非常的快

   缺点：哈希表的建立比较耗费时间

   适用于：对于查找问题，unordered_map会更加高效一些

#### map里面[]和at有什么区别

`[]` 运算符不进行边界检查。如果尝试访问一个不存在的键，则会创建一个具有该键的新元素，并将其值默认初始化

`at()` 方法进行边界检查。如果尝试访问一个不存在的键，则会抛出 `std::out_of_range` 异常。



### malloc申请存储的空间有办法释放吗

```C++
#include <stdlib.h>

int main() {
    // 使用malloc分配内存
    int* ptr = (int*)malloc(sizeof(int) * 10);
    
    if (ptr == NULL) {
        // 内存分配失败
        exit(1);
    }
    
    // 使用内存空间...
    
    // 释放内存空间
    free(ptr);
    
    return 0;
}

```



### new和malloc的区别

`new`和`malloc`都是用于在堆（heap）上分配内存的C++操作符和函数，但它们有几个重要的区别：

1. **返回类型**：
   - `new`是C++中的操作符，返回的是分配类型的指针。
   - `malloc`是C标准库中的函数，返回的是`void*`指针，需要进行类型转换后才能使用。
2. **类型安全性**：
   - `new`是类型安全的，它会自动计算要分配的内存空间的大小，并将指针转换为正确的类型。
   - `malloc`不是类型安全的，需要手动计算要分配的内存空间的大小，并进行类型转换。
3. **构造函数的调用**：
   - 使用`new`分配的内存空间会调用对象的构造函数进行初始化。
   - 使用`malloc`分配的内存空间不会调用对象的构造函数，需要手动调用构造函数进行初始化。
4. **对数组的处理**：
   - `new`可以直接分配数组，并使用`delete[]`来释放数组。
   - `malloc`只能分配单个内存块，并使用`free`来释放内存，对于数组需要手动计算空间大小和释放内存。
5. **异常处理**：
   - `new`在分配内存失败时会抛出`std::bad_alloc`异常，需要使用`try-catch`块来捕获异常。
   - `malloc`在分配内存失败时返回`NULL`指针，需要手动检查返回值来处理分配失败的情况。



### 零拷贝

零拷贝（Zero-copy）是一种优化技术，旨在减少数据在系统中的复制次数，从而提高数据传输的效率和性能。在传统的数据传输过程中，数据通常需要在应用程序的用户空间和内核空间之间进行多次复制，而零拷贝技术可以通过在不同层次上优化数据传输过程来减少这种复制。

1. **用户空间到内核空间的零拷贝**：传统上，当应用程序需要将数据从用户空间传输到内核空间时，数据通常需要通过系统调用来复制到内核缓冲区中。而使用零拷贝技术，应用程序可以直接将数据的指针传递给系统调用，而无需实际复制数据，从而避免了一次复制。
2. **内核空间到内核空间的零拷贝**：在一些情况下，数据在内核空间之间传输时也可能会发生复制。零拷贝技术可以通过使用共享内存区域或直接DMA（直接内存访问）等技术，避免数据在内核空间之间的复制。
3. **内核空间到用户空间的零拷贝**：类似地，当数据需要从内核空间传输到用户空间时，零拷贝技术可以避免额外的数据复制。例如，内核可以直接将数据发送到应用程序的用户空间缓冲区，而无需额外复制数据。



### 什么是namespace

在C++中，namespace（命名空间）是一种组织代码和避免命名冲突的机制。它允许开发人员将一系列相关的变量、函数和类封装在一个独立的作用域中。通过使用命名空间，可以有效地组织大型项目的代码，并且可以更轻松地与其他库和代码集成而不会发生名称冲突。

命名空间的主要目的之一是避免命名冲突。当在不同的库或模块中使用相同名称时，命名空间可以确保它们不会相互干扰。



### python的threading和pyqt的有什么区别

1. 线程模型：

   Python 的 threading 模块提供了多线程编程的基本支持，使用标准的线程模型。它允许创建和管理线程，但在 Python 中由于全局解释器锁（GIL）的存在，多线程并不能实现真正的并行执行，而是通过线程间的切换来模拟并发。

   PyQt 也提供了多线程编程的支持，但它是基于 Qt 库提供的线程模型的。Qt 的线程模型是基于事件循环的，通过将耗时操作放在单独的线程中执行，以保持主线程的响应性。PyQt 的多线程功能通常与信号槽机制结合使用，以便在不同线程之间进行通信和同步。

2. 事件循环：

   在 threading 模块中，Python 的线程是基于操作系统的线程实现的，没有事件循环的概念，线程的执行由操作系统调度。

   在 PyQt 中，每个线程都有自己的事件循环。主线程的事件循环通常由 Qt 主循环负责，而子线程的事件循环需要手动启动和管理。

3. 线程通信：

   在 threading 模块中，线程之间的通信可以通过共享变量、锁、条件变量等机制实现。

   在 PyQt 中，线程之间的通信通常使用信号槽机制。



### 对python的修饰器有什么理解

用于在函数或类的定义前面添加一个修饰器，以实现对函数或类的装饰或修改。

`@torch.no_grad()`: 在装饰器内部，它定义了一个内部函数 `wrapper`，该函数使用 `with torch.no_grad():` 上下文管理器来包裹被修饰的函数 `func` 的调用。在进入 `torch.no_grad()` 上下文管理器的代码块时，会调用 `torch.set_grad_enabled(False)`，该函数会设置梯度跟踪的开关为关闭状态。在退出上下文管理器的代码块时，会调用 `torch.set_grad_enabled(True)` 来恢复梯度跟踪的开关状态。

```python
def no_grad(func):
    def wrapper(*args, **kwargs):
        with torch.no_grad():
            return func(*args, **kwargs)
    return wrapper

@no_grad
def inference(model, input_data):
    output = model(input_data)
    return output
```

`@staticmethod`：用于声明静态方法。

`@classmethod`：用于声明类方法。

`@property`：将一个方法转化为只读属性。

1. 类方法（classmethod）和静态方法（staticmethod）都是类级别的方法。
2. 类方法的第一个参数是 `cls`，表示类本身，可以用来访问类的属性和方法。静态方法没有特殊的参数，与普通的函数一样。类方法通过 `cls` 参数可以访问和操作类的属性和方法，因此可以被子类继承和覆盖。静态方法与类的状态无关，因此无法通过继承来改变其行为。
3. 类方法可以访问和修改类的状态（类变量），并且可以调用其他类方法。静态方法不能直接访问类变量或者调用其他类方法，因为它们与类的状态无关。



### http和https区别



### 为什么https安全，怎么加密怎么传输的



### 说说https加密流程



### Python 的基本数据类型？

Python 的语言类型定义是**强类型**的**动态语言**。

- 强类型和弱类型指的是是否会发生类型的隐式转换；
- 动态语言还是静态语言指的是编译期还是运行期确定类型；

Python 中变量一旦确定类型，如果在之后的代码中赋值给此变量其他类型是可以的。

```python
# 下面的转换是不允许的，但这在弱类型的 Javascript 中是允许的
foo = 'abc'
abc = 123
print(foo + abc)
```

Python 变量的类型是在运行期间才确定的，因此 Python 属于动态语言，这有别与 Java，Java 在变量声明时就必须指定变量的类型。



- Number（数字）(包括整型、浮点型、复数、布尔型等)
- String（字符串）
- List（列表）
- Tuple（元组），不可变数据
- Set（集合）
- Dictionary（字典）



### Python 中的 `*args` 和 `**kwargs` 表示什么？

`*args`：它允许函数接受任意数量的位置参数。这意味着你可以在调用函数时传递任意数量的位置参数，而不需要预先指定它们的数量。

`**kwargs`：它允许函数接受任意数量的关键字参数。

```python
def my_function(*args, **kwargs):
    for arg in args:
        print(arg)
    for key, value in kwargs.items():
        print(f"{key}: {value}")

my_function(1, 2, 3, name="Alice", age=30)  # 输出: 1 2 3  name: Alice  age: 30
```



### Python 语法中 “==” 和 “is” 的区别是什么？

`==` 运算符用于比较两个对象的值是否相等。

`is` 运算符用于比较两个对象的身份（内存地址）是否相同。



### Python 中为什么没有函数重载？

Python是一种动态类型语言，变量的类型是在运行时确定的，而不是在编译时确定的。由于函数参数的类型是动态的，因此编译器无法在编译时根据参数类型来确定调用哪一个函数。

在Python中，函数的多态性是通过鸭子类型（duck typing）来实现的，即不关心对象的具体类型，只关心对象是否具有特定的方法或行为。因此，不需要函数重载来处理不同类型的参数，只需确保传递的对象能够正确地响应函数调用即可。



### 介绍用过 Python 标准库中的哪些模块？

##### os模块

**文件操作：**

- `os.getcwd()`: 获取当前工作目录的路径。
- `os.chdir(path)`: 改变当前工作目录到指定路径。
- `os.listdir(path='.')`: 返回指定目录下的所有文件和目录的列表。
- `os.mkdir(path)`: 创建新目录。
- `os.makedirs(path)`: 递归创建多级目录。
- `os.remove(path)`: 删除指定路径的文件。
- `os.rmdir(path)`: 删除指定路径的目录（只能删除空目录）。
- `os.removedirs(path)`: 递归删除多级目录。
- `os.rename(src, dst)`: 重命名文件或目录。
- `os.path.exists(path)`: 判断指定路径是否存在。

**环境变量操作：**

- `os.environ`: 返回一个包含环境变量的字典。

**系统信息：**

- `os.name`: 返回操作系统名称，`posix` 表示 Linux、Unix 或 macOS，`nt` 表示 Windows。
- `os.uname()`: 返回包含系统信息的元组（仅在 Unix/Linux 系统中可用）。
- `os.system(command)`: 执行系统命令。

**路径操作：**

- `os.path.join(path1, path2, ... )`: 将多个路径组合成一个路径。
- `os.path.abspath(path)`: 返回指定路径的绝对路径。
- `os.path.basename(path)`: 返回指定路径的基名（文件名或最后一级目录名）。
- `os.path.dirname(path)`: 返回指定路径的目录名



#### logging

- `logging.getLogger(name)`: 获取一个名为 `name` 的日志记录器对象，如果不指定 `name`，则返回根日志记录器对象。
- `logger.setLevel(level)`: 设置日志记录器的日志级别，只有级别高于或等于指定级别的日志才会被记录。
- `logger.addHandler(handler)`: 添加一个处理程序到日志记录器。
- `logger.removeHandler(handler)`: 从日志记录器中移除指定的处理程序。
- `logger.debug(msg, *args, **kwargs)`: 记录调试级别的日志消息。
- `logger.info(msg, *args, **kwargs)`: 记录信息级别的日志消息。
- `logger.warning(msg, *args, **kwargs)`: 记录警告级别的日志消息。
- `logger.error(msg, *args, **kwargs)`: 记录错误级别的日志消息。
- `logger.critical(msg, *args, **kwargs)`: 记录严重错误级别的日志消息。



### 什么是鸭子类型（duck typing）？

鸭子类型（duck typing）是一种动态类型语言中的编程风格，其核心思想是“如果它走起路来像鸭子，叫起来也像鸭子，那么它就是鸭子”。换句话说，鸭子类型关注的是对象的行为（具有特定的方法或属性），而不是对象的类型。

在 Python 语言中，有很多 bytes-like 对象（如：bytes、bytearray、array.array、memoryview）、file-like 对象（如：StringIO、BytesIO、GzipFile、socket）、path-like对象（如：str、bytes）。

- 其中 file-like 对象都能支持 read 和 write 操作，可以像文件一样读写，这就是所谓的对象有鸭子的行为就可以判定为鸭子的判定方法。
- 再比如 Python 中列表的 extend 方法，它需要的参数并不一定要是列表，只要是可迭代对象就没有问题。



### 什么是 Lambda 函数？

Lambda 函数也叫匿名函数，它是功能简单用一行代码就能实现的小型函数。Python 中的 Lambda 函数只能写一个表达式，这个表达式的执行结果就是函数的返回值，不用写 return 关键字。

```python
items = [12, 5, 7, 10, 8, 19]
items = list(map(lambda x: x ** 2, filter(lambda x: x % 2, items)))
print(items)    # [25, 49, 361]
```



### 正则表达式的 match 方法和 search 方法有什么区别？

`match()` 方法从字符串的开头开始匹配模式，如果字符串的开头匹配成功，则返回匹配对象；如果字符串的开头不匹配，则返回 `None`。即 `match()` 方法只匹配字符串的开头部分。

`search()` 方法在整个字符串中搜索匹配的模式，如果找到了匹配的子串，则返回匹配对象；如果找不到匹配的子串，则返回 `None`。即 `search()` 方法会在整个字符串中搜索，而不仅仅是从开头开始。

```python
import re
text = "The cat sat on the mat."
# 使用 match() 方法
match_result = re.match(r'cat', text)  # match_result=None

# 使用 search() 方法
search_result = re.search(r'cat', text) # search_result.group()="cat"
```



### 什么是 Python 中的魔术方法？

| 魔术函数                                                     | 作用               |
| ------------------------------------------------------------ | ------------------ |
| `__new__`、`__init__`、`__del__`                             | 创建和销毁对象相关 |
| `__eq__`、`__ne__`、`__lt__`、`__gt__`、`__le__`、`__ge__`   | 关系运算符相关     |
| `__pos__`、`__neg__`、`__invert__`                           | 一元运算符相关     |
| `__lshift__`、`__rshift__`、`__and__`、`__or__`、`__xor__`   | 位运算相关         |
| `__enter__`、`__exit__`                                      | 上下文管理器协议   |
| `__iter__`、`__next__`、`__reversed__`                       | 迭代器协议         |
| `__int__`、`__long__`、`__float__`、`__oct__`、`__hex__`     | 类型/进制转换相关  |
| `__str__`、`__repr__`、`__hash__`、`__dir__`                 | 对象表述相关       |
| `__len__`、`__getitem__`、`__setitem__`、`__contains__`、`__missing__` | 序列相关           |
| `__add__`、`__sub__`、`__mul__`、`__div__`、`__floordiv__`、`__mod__` | 算术运算符相关     |



### 说一下你对 Python 中模块和包的理解？

每个 Python 文件就是一个模块，而保存这些文件的文件夹就是一个包，但是这个作为 Python 包的文件夹必须要有一个名为 `__init__.py` 的文件，否则无法导入这个包。

模块和包解决了 Python 中命名冲突的问题，不同的包下可以有同名的模块，不同的模块下可以有同名的变量、函数或类。



### 什么情况下会出现 `KeyError`、`TypeError`、`ValueError`？

通过一个字典 a，执行 `int(a['x'])` 这个操作就有可能引发上述三种类型的异常。

- 如果字典中没有键 x，会引发 KeyError；

- 如果键 x 对应的值不是 str、float、int、bool 以及 bytes-like 类型，在调用 int 函数构造 int 类型的对象时，会引发 TypeError；

- 如果 a[x] 是一个字符串或者字节串，而对应的内容又无法处理成 int 时，将引发 ValueError。

  ```python
  int('abc')  # ValueError: invalid literal for int() with base 10: 'abc'
  ```

  

### 说说对 Python 中的浅拷贝和深拷贝的理解？

浅拷贝创建一个新对象，但只复制了原始对象中的顶层元素，而不会复制嵌套对象的子对象。如果原始对象包含引用其他对象的子对象，则浅拷贝只是复制了这些引用，而不是实际的对象。因此，**当对浅拷贝进行修改时，原始对象中的嵌套对象也会受到影响**。

浅拷贝可以使用 `copy()` 方法来实现，或者使用切片操作符 `[:]`。

深拷贝创建一个新对象，并递归地复制原始对象及其所有子对象。深拷贝会完全复制原始对象的所有内容，因此**修改深拷贝的副本不会影响原始对象**。

深拷贝可以使用 `copy.deepcopy()` 方法来实现。



### 狼羊过河问题，用搜索求出最短过河次数，并且输出方案。



### 给定一个整数数组A，数组的长度为n。再给定一个整数k，你的任务是找出所有满足以下条件的连续子序列：子序列的和除以k的余数等于该子序列的长度。请计算并输出这样的子序列的数量。



### 给定两个整数n和x，你的任务是构造一个长度为n的排列，该排列以x开头，并且满足排列中相邻元素的绝对差值之和最大。构造出这样的排列，并输出。



### 原地旋转输入矩阵

```C++
class Solution {
public:
    void rotate(vector<vector<int>>& matrix) {
        int N = matrix.size();
        for(int offset=0;offset<N/2;offset++){
            for(int j=offset;j<N-offset-1;j++){
                int num = matrix[offset][j];
                matrix[offset][j] = matrix[N-j-1][offset];
                matrix[N-j-1][offset] = matrix[N-offset-1][N-j-1];
                matrix[N-offset-1][N-j-1] = matrix[j][N-offset-1];
                matrix[j][N-offset-1] = num;
            }
        }
    }
};
```

### 说一下 Python 中变量的作用域？

**局部作用域**指的是在函数内部定义的变量的作用范围。这些变量只能在函数内部访问，不能在函数外部访问。

**全局作用域**指的是在模块（文件）顶层定义的变量的作用范围。这些变量可以在整个模块中的任何地方访问。

**内置作用域**指的是Python内置的函数和变量的作用范围。这些函数和变量可以在任何地方直接使用，无需导入任何模块。

**嵌套作用域**指的是在函数内部定义的函数的作用范围。这些内部函数可以访问外部函数的变量。





<h1 style="text-align:center;">AI算法题
</h1>
### 如何进行特征选择，不同的方法好处是什么

1. **过滤法：**在训练模型之前对特征进行评估和排序，然后选择最相关的特征。常用的评估指标包括信息增益、方差、卡方检验、互信息等。

   优点： 计算速度快，易于实现，不需要训练模型就能够完成特征选择。

2. **包装法：**通过尝试不同的特征子集来训练模型，并根据模型性能来选择最佳特征子集。常见的包装方法包括递归特征消除（Recursive Feature Elimination, RFE）、正向选择和反向消除等。

   优势：能够找到最佳特征子集，通常在特征与目标变量之间存在复杂关系时表现良好。

3. **嵌入法：**将特征选择嵌入到模型训练的过程中，如 Lasso 回归、决策树、随机森林等模型可以自动选择最佳特征。

   优势：能够结合模型的性能和特征选择，通常能够找到对模型性能最有利的特征子集。

4. **降维方法：**如主成分分析（Principal Component Analysis, PCA）和线性判别分析（Linear Discriminant Analysis, LDA）等。

   优势：能够在保留数据大部分信息的同时减少特征的数量，有助于降低模型复杂度和处理高维数据的计算成本。



### 主成分分析

它通过线性变换将高维数据投影到低维空间中，同时最大程度地保留原始数据的方差。

1. **数据中心化：**对原始数据进行中心化处理，即减去各个特征的均值，使得数据的均值为零。这一步骤可以确保在进行线性变换时，**新的特征空间中的原点在数据的中心**。
2. **协方差矩阵计算：**协方差矩阵反映了各个特征之间的关系，其中每个元素表示对应特征之间的协方差。
3. **特征值的分解：**对协方差矩阵进行特征值分解（Eigen decomposition），得到特征值（eigenvalues）和对应的特征向量（eigenvectors）。特征向量表示了数据在新特征空间中的方向，而特征值则表示了数据在这些方向上的方差大小。
4. **选择主成分：**根据特征值的大小排序特征向量，选择最大的 k 个特征值对应的特征向量作为主成分（Principal Components）。这些主成分是原始数据在新特征空间中的主要方向，它们能够最大程度地保留原始数据的方差。
5. **投影：**将原始数据投影到选定的主成分上，得到低维的特征表示。

![img](https://images2015.cnblogs.com/blog/1042406/201612/1042406-20161231162149992-1521335659.png)

### 贝叶斯定理的作用

通常，事件A在事件B已发生的条件下发生的概率，与事件B在事件A已发生的条件下发生的概率是不一样的。然而，这两者是有确定的关系的，贝叶斯定理就是这种关系的陈述。贝叶斯公式的一个用途，即透过已知的三个概率而推出第四个概率。



### 贝叶斯公式

$$
P(A|B)=\frac{P(B|A)P(A)}{P(B)}
$$

$P(A|B)$是在给定B的条件下A的概率，成为后验概率；

$P(B|A)$是在给定A的条件下B的概率，称为似然度；

$P(A)$是A的先验概率；



### 贝叶斯推断

1. 确定先验分布：先验分布可以是任何合适的概率分布，它反映了我们在观察到数据之前对参数的信念。
2. 计算似然函数：似然函数描述了参数值和数据之间的关系。
3. 应用贝叶斯定理：后验分布正比于先验分布与似然函数的乘积

$$
P(\theta|D)∝P(D|\theta)P(\theta)
$$

$P(\theta|D)$表示给定观察到的数据$D$后参数$\theta$的后验分布，$P(D|\theta)$是在给定参数$\theta$的情况下观察到数据$D$的概率（似然函数），$P(\theta)$是参数的先验分布。

### 条件概率

条件概率是指在已知某一事件发生的条件下，另一事件发生的概率。
$$
P(A|B)=\frac{P(A\bigcap B)}{P(B)}
$$

### 全概率公式

全概率公式是指当某一事件可以被划分为多个互斥事件的并集时，可以利用这些互斥事件的概率以及它们与另一事件的交集的概率来计算另一事件的概率。
$$
P(A)=\sum_{i=1}^nP(A|B_i)P(B_i)
$$


### 介绍SVM和逻辑回归

#### SVM

支持向量机（Support Vector Machine，SVM）是一种经典的监督学习算法，主要用于分类和回归分析。SVM 的目标是找到一个最优的超平面，使得正负样本中距离超平面最近的数据点到超平面的距离（即支持向量）尽可能远，这个距离称为间隔（margin）。线性可分的情况并不总是存在，因此引入软间隔分类器。软间隔分类器允许一些数据点不满足线性可分的条件，但是仍然试图最大化间隔，并引入了惩罚项来限制分类错误的数量。SVM 可以通过核技巧处理非线性可分的情况。核技巧允许在高维空间中计算点之间的内积，而不需要显式地计算高维空间的坐标。这使得 SVM 在处理非线性问题时非常有效，常用的核函数包括线性核、多项式核、高斯核等。SVM 的目标是求解一个凸优化问题，通过最小化损失函数来找到最优的超平面参数。常见的损失函数包括合页损失函数（hinge loss）。

#### 逻辑回归

逻辑回归使用一个称为“逻辑函数”或“Sigmoid函数”的函数作为假设函数，Sigmoid函数的公式如下：
$$
h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}
$$
其中$h_\theta(x)$是预测概率，$x$是特征向量，$\theta$是模型参数向量。

对数损失函数：$J(\theta)=-[ylog(h_\theta(x))+(1-y)log(1-h_\theta(x))]$

逻辑回归是一种简单而有效的分类算法，特别适用于二分类问题，并且具有良好的解释性。

#### 二者的区别：

1. 目标函数：SVM的目标是找到一个最大间隔超平面，即使得两个类别的样本距离超平面的间隔最大化。其损失函数基于间隔和支持向量的概念。逻辑回归的目标是最小化对数损失函数，使得模型的预测概率与实际标签尽可能一致。
2. 输出：SVM是一种判别模型，其输出是一个样本属于某个类别的决策边界。逻辑回归输出的是样本属于某个类别的概率。
3. 鲁棒性:SVM对异常值相对较为敏感，因为它的目标是最大化间隔，异常值可能会对决策边界产生较大影响。逻辑回归对异常值的影响相对较小。
4. 计算复杂度：逻辑回归的计算复杂度通常较低，因为它的损失函数是凸函数，可以直接使用梯度下降等优化算法。SVM在大规模数据集上的计算复杂度较高，尤其是在使用核技巧时。
5. 模型可解释性：逻辑回归提供了直接的系数解释，可以解释每个特征对结果的影响。SVM在高维空间中操作，因此其模型的解释性通常较差。



### 逻辑回归和线性回归的区别

**线性回归：**主要用于建立自变量（特征）与连续因变量之间的关系。使用线性函数拟合自变量与因变量之间的关系，模型形式为$y=\beta_0+\beta_1x_1+...+\beta_nx_n+\epsilon$，通常使用最小二乘法来估计模型的参数，目标是最小化实际观测值和模型预测值之间的平方误差；

**逻辑回归：**用于建立自变量与离散因变量之间的关系，使用Sigmoid函数拟合自变量与因变量之间的关系，$P(Y=1|X)=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+...+\beta_nx_n)}}$，通常使用极大似然估计来估计模型参数，目标是最大化给定数据下观测到的样本概率的乘积。



### 最小二乘法

最小二乘法（Least Squares Method）是一种常用的数学优化技术，用于寻找一组参数，使得给定的数学模型与观测数据之间的残差平方和最小。

假设我们有一组观测数据 $(x_i, y_i)$，我们想要找到一个模型 $f(x; \theta)$，其中 $\theta$ 是参数向量，使得模型的预测值 $f(x_i; \theta)$ 尽可能地接近观测值 $y_i$。我们可以定义一个损失函数，例如残差平方和（Residual Sum of Squares，RSS）：
$$
RSS(\theta)=\sum_{i=1}^n(y-f(x;\theta))^2
$$
对于线性模型，即 $f(x; \theta) = \theta_0 + \theta_1 x$，最小二乘法的解析解可以通过求解正规方程（Normal Equations）得到：
$$
\theta=(X^TX)^{-1}X^Ty
$$
其中 $X$ 是设计矩阵，包含了所有的观测数据 $x_i$，$y$ 是观测数据的向量。



### 介绍KMeans

聚类属于非监督学习，K均值聚类是最基础常用的聚类算法。它的基本思想是，通过迭代寻找K个簇（Cluster）的一种划分方案，使得聚类结果对应的损失函数最小。其中，损失函数可以定义为各个样本距离所属簇中心点的误差平方和：
$$
J(c,u)=\sum_1^n||x_i-\mu_{c_i}||^2
$$
其中$x_i$代表第i个样本，$c_i$是$x_i$所属的簇，$u_{c_i}$是对应簇的中心点。

具体步骤：1.数据预处理，标准化，过滤异常点；2.随机选取K个中心；3。定义损失函数；4.对每个样本，将其分配到距离最近的中心$c_i^t<-argmin_k||x_i-u_k^t||^2$，重新计算类中心$u_k^{t+1}<-argmin_u\sum_{i_i^t=k}^b||x_i-\mu||^2$

优点：1. 高效可伸缩，计算复杂度 为$O(NKt)$接近于线性（N是数据量，K是聚类总数，t是迭代轮数）；2.收敛速度快，原理相对通俗易懂，可解释性强。

缺点：1. 受初始值和异常点影响，聚类结果可能不是全局最优而是局部最优。2. K是超参数，一般需要按经验选择；3. 样本点只能划分到单一的类中。

### EM算法

EM（Expectation-Maximum）算法即期望最大化算法，是最常见的隐变量估计方法。EM算法是一种迭代优化策略，每一次迭代都分为两步：期望步（E）、极大步（M）。**EM算法的提出最初是为了解决数据缺失情况下的参数估计问题**，基本思想是首先根据已有的观测数据，通过极大似然估计估计出模型的参数；再根据上一步估计出的参数值估计缺失数据的值；最后根据估计出的缺失数据和原有的观测数据重新对参数值进行估计，反复迭代直到收敛。

**结论：EM算法可以保证收敛到一个稳定点，即EM算法是一定收敛的。**



### GBDT和Adaboost的区别





### 图像边缘检测算子有哪些？

1. Sobel算子：一种基于一阶梯度的边缘检测算子，用于寻找图像中的水平和垂直边缘。Sobel算子包含两个卷积核：一个用于计算水平方向的梯度，另一个用于计算垂直方向的梯度。

$$
G_x=\begin{matrix}
-1&0&1\\
-2&0&2\\
-1&0&1\\
\end{matrix}
$$

$$
G_y=\begin{matrix}
-1&-2&-1\\
0&0&0\\
1&2&1\\
\end{matrix}
$$



2. Prewitt算子：类似于Sobel算子，也是一种基于一阶梯度的边缘检测算子，用于寻找图像中的水平和垂直边缘。

$$
G_x=\begin{matrix}
-1&0&1\\
-1&0&1\\
-1&0&1\\
\end{matrix}
$$

$$
G_y=\begin{matrix}
1&1&1\\
0&0&0\\
1&1&1\\
\end{matrix}
$$



3. Laplacian算子：
   $$
   f'(x,y)=-4f(x,y)+f(x-1,y)+f(x+1,y)+f(x,y-1)+f(x,y+1)
   $$




### Canny算子的流程

1. 高斯滤波：滤波的主要目的是降噪，一般的图像处理算法都需要先进行降噪。

2. 计算梯度值和梯度方向：通过点乘一个sobel或其他算子得到不同方向的梯度值$g_x(m,n)$,$g_y(m,n)$。综合梯度通过以下公式计算梯度值和梯度方向：
   $$
   G(m,n)=\sqrt{g_x(m,n)^2+g_y(m,n)^2}
   $$

   $$
   \theta=arctan \frac{g_y(m,n)}{g_x(m,n)}
   $$

3. 非极大值抑制：使边缘的宽度尽可能为1个像素点，如果一个像素点属于边缘， 那么这个像素点在梯度方向上的梯度值是最大的，否则不是边缘，将灰度值设置为0.

4. 使用上下阈值来检测边缘：设置两个threshold，分别为maxVal和minVal。其中大于maxVal的都被检测为边缘，而低于minval的都被检测为非边缘。对于中间的像素点，如果与确定为边缘的像素点邻接，则判定为边缘；否则为非边缘。



### 仿射变换

图像的仿射变换是指通过线性变换来对图像进行**平移**、**旋转**、**缩放**和**剪切**等操作，而不改变图像的**平行性**和**直线性**。

一个集合的仿射变换为$f(x)=Ax+b$
$$
\left[
\begin{matrix}
    x' \\ y' \\ 1
\end{matrix}
\right]
=
\left[
\begin{matrix}
R_{00} & R_{01} & T_x \\
R_{10} & R_{11} & T_y \\
0 & 0 & 1
\end{matrix}
\right]
\left[
\begin{matrix}
x \\ y \\ 1
\end{matrix}
\right]
$$
平移变换：
$$
M=
\left[
\begin{matrix}
1 & 0 & T_x \\
0 & 1 & T_y \\
0 & 0 & 1
\end{matrix}
\right]
$$

$$
x'=x+T_x
$$

$$
y'=y+T_y
$$



旋转变换：
$$
M=
\left[
\begin{matrix}
cos\theta & -sin\theta & 0 \\
sin\theta & cos\theta & 0 \\
0 & 0 & 1
\end{matrix}
\right]
$$

$$
x'=cos\theta x - sin\theta y
$$

$$
y'=sin\theta x + cos\theta y
$$

我们这边采用的方法其实就是人工选取3组对应点，将三组对应点带入到仿射变换矩阵中，做一个方程组的求解；



### 最邻近插值

首先假设原图是一个像素大小为W*H的图片，缩放后的图片是一个像素大小为w*h的图片，这时候我们是已知原图中每个像素点上的像素值（即灰度值等）的（⚠️像素点对应像素值的坐标都是整数）。这个时候已知缩放后有一个像素点为(x,y)，想要得到该像素点的像素值，那么就要根据缩放比例去查看其对应的原图的像素点的像素值，然后将该像素值赋值给该缩放后图片的像素点(x,y)

- 根据横轴，即宽可得：X/x = W/w
- 根据纵轴，即高可得：Y/y = H/h
- 那么能够得到 f（X,Y）= f( W/w * x, H/h *y)

因为缩放比例的原因，会导致像素点( W/w * x, H/h *y)中的值不是整数，所以最邻近插值采用向下取整的方法。



### 双线性插值

![img](https://pic1.zhimg.com/80/v2-cb1e4844e85e1442f045d52c00f0f35c_1440w.webp)

我们需要计算出P点的像素值。我们取P点邻近的四个点Q11,Q12,Q21,Q22，并假设在邻近范围内，点的像素值是呈线性变化的。
$$
f(R1)=\frac{x2-x}{x2-x1}f(Q11)+\frac{x-x1}{x2-x1}f(Q21)
$$

$$
f(R2)=\frac{x2-x}{x2-x1}f(Q12)+\frac{x-x1}{x2-x1}f(Q22)
$$

$$
f(P)=\frac{y2-y}{y2-y1}f(R1)+\frac{y-y1}{y2-y1}f(R2)
$$

### 双三次线性插值

双三次插值法需要P点近邻的16个点来加权。

首先构造一个BiCubic函数，它是用来根据近邻点与P点的相对位置来计算该点前的权值的一个函数：

得到权值后，我们只需要将这16个点的像素值加权起来即可，插值计算的公式如下：













### 怎么构建图像金字塔

一种是高斯[金字塔](https://so.csdn.net/so/search?q=金字塔&spm=1001.2101.3001.7020)（Gaussian Pyramid），另一种的拉普拉斯金字塔（Laplacian Pyramid）。

高斯金字塔通过不断对图像进行模糊且下采样而获得，下采样的因子一般是2倍。随着分辨率越来越小，图像会越来越模糊，高斯金字塔的最底层就是原始图像本身。

拉普拉斯金字塔在高斯金字塔的基础上，对所有层进行上采样（一般也是2倍上采样），然后使用原高斯金字塔结果减去通分辨率的上采样结果得到每一层差异，即为拉普拉斯金字塔。

拉普拉斯金字塔中，大部分的数值接近于0，所以一定程度上可以用于图像压缩。拉普拉斯金字塔还常用于图像融合，基于拉普拉斯金字塔的图像融合，融合边界的过渡往往会相对自然一些。

上下采样均需要做模糊，下采样中做模糊是为了防止锯齿现象，上采样中做模糊是因为图像金字塔分解中的上采样比较“特别”，不做模糊不行。

下采样：对图像做模糊，然后直接每隔一个像素抽一个数据即可实现2倍下采样。

上采样：将每个像素扩展成2x2的小区域，原像素放在左上角，其他3个位置补0，然后将卷积核乘以4，再对扩展后的图像做模糊即可。



### 分水岭算法原理

图像的灰度空间很像地球表面的整个地理结构，每个像素的灰度值代表高度。其中的灰度值较大的像素连成的线可以看做山脊，也就是分水岭。其中的水就是用于二值化的gray threshold level，二值化阈值可以理解为水平面，比水平面低的区域会被淹没，刚开始用水填充每个孤立的山谷(局部最小值)。

当水平面上升到一定高度时，水就会溢出当前山谷，可以通过在分水岭上修大坝，从而避免两个山谷的水汇集，这样图像就被分成2个像素集，一个是被水淹没的山谷像素集，一个是分水岭线像素集。最终这些大坝形成的线就对整个图像进行了分区，实现对图像的分割。

![img](https://pic2.zhimg.com/80/v2-ea8ce5a64e744c8c56c492b03a039265_720w.webp)



### 匈牙利匹配原理

1. 行归约，减去每行最小值。 2. 列归约，减去每列最小值。 3. 做循环： 3A. 用尽量少的线覆盖住矩阵中所有的零元素。 3B. 判断是否终止循环，条件，线的数量是否等于节点的数量。 3C. 创造更多的零元素，寻找所有元素中没有被线覆盖的最小元素，将其他未被线覆盖的元素减去最小的元素，同时要将最小元素加到线的交叉处。

### SIFT特征点提取

尺度不变特征变换(Scale-invariant feature transform， 简称SIFT)是图像局部特征提取的现代方法。SIFT特征是图像的局部特征，其对**旋转、尺度缩放、亮度**变化保持不变性，对**视角变化、仿射变换、噪声**也保持一定程度的稳定性。

SIFT算法的实质是在不同的尺度空间上查找关键点(特征点)，并计算出关键点的方向。

1. **尺度空间极值检测（Scale-space Extrema Detection）**：在不同的尺度下，通过高斯模糊操作生成尺度空间，然后在尺度空间中寻找极值点，这些极值点代表了图像中可能的关键点位置。
2. **关键点定位（Keypoint Localization）**：对于检测到的极值点，通过比较其在尺度空间中的极值性质来剔除低对比度和边缘响应不明显的点，从而得到最终的关键点位置。
3. **方向分配（Orientation Assignment）**：对于每个关键点，通过计算其周围像素的梯度方向来确定主要方向，然后将关键点的描述符旋转到该方向，从而增强了其旋转不变性。
4. **描述子生成（Descriptor Generation）**：以关键点为中心，提取周围区域的梯度信息，并将这些信息转化为一个128维的向量作为该关键点的描述子。



### 图像配准

![img](https://pic3.zhimg.com/80/v2-63aba3dec4ed09966c8122e9fa1ad5d6_720w.webp)



### RANSAC

RANSAC（Random Sample Consensus，随机样本一致性）是一种经典的迭代方法，用于估计数学模型参数，以从包含噪声的数据中找到正确的模型参数。RANSAC适用于处理具有大量噪声的数据，可以在其中找到最优的模型参数。

1. **随机采样**：从数据集中随机选择一定数量的样本，以构建一个初始的模型。
2. **模型拟合**：使用随机选择的样本来拟合一个模型。模型的参数根据选定的算法（例如线性回归、多项式拟合、几何模型等）来计算。
3. **内点检验**：对于每个数据点，计算它与拟合模型的适应度（通常是距离或误差度量）。如果数据点适合于当前模型（即在某个阈值范围内），则将其标记为内点；否则将其标记为外点。
4. **模型评估**：根据内点的数量来评估拟合的模型的质量。内点数量越多，模型越可能是正确的。
5. **迭代**：重复上述过程多次（迭代次数由用户指定），选择具有最大内点数量的模型作为最终的模型。



### 神经网络优化器有哪些

1. **随机梯度下降 (SGD)：** 是最基本的优化算法之一，每次迭代随机选择一批样本进行梯度计算和参数更新。虽然简单，但在实践中可能会受到局部极小值、学习率调整等问题的影响。
2. **动量优化器（Momentum）：** 通过引入动量项来加速收敛过程，可以在参数更新时考虑历史梯度的加权和。常见的动量优化器包括标准动量（SGD with Momentum）、Nesterov 动量等。
3. **AdaGrad：** 自适应学习率的方法之一，通过对每个参数的学习率进行适应性调整，使得稀疏梯度对应的学习率较大，而频繁出现的梯度对应的学习率较小。
4. **RMSProp：** 根据梯度的历史信息来调整学习率，对 AdaGrad 进行了改进，通过指数加权移动平均的方式对历史梯度进行衰减。
5. **Adam：** 结合了动量优化器和 RMSProp 的优点，在计算动量和学习率时考虑了梯度的一阶矩估计（均值）和二阶矩估计（方差），并使用偏差修正来提高稳定性。



### 训练网络时怎么判断有没有过拟合，欠拟合

#### 过拟合：

1. **训练集和验证集损失对比**：监控训练集和验证集的损失。如果训练集的损失持续下降，而验证集的损失却开始上升，则可能出现了过拟合。
2. **训练集和验证集性能对比**：除了损失外，还可以比较训练集和验证集的性能指标，如准确率、精确率、召回率等。如果训练集的性能指标较高，而验证集的性能指标较低，则可能存在过拟合。
3. **可视化训练过程**：可视化训练过程中的损失和性能曲线，观察它们的趋势变化。如果训练集和验证集的曲线出现了分歧，可能出现了过拟合。

#### 欠拟合：

1. **监控训练集和验证集的损失**：如果训练集和验证集的损失都很高，说明模型可能存在欠拟合。这意味着模型无法捕捉数据中的足够信息。
2. **观察模型性能**：监控模型在训练集和验证集上的性能指标。如果性能指标较低，可能是因为模型过于简单，无法很好地拟合数据。
3. **增加模型容量**：如果发现模型出现了欠拟合，可以尝试增加模型的容量，例如增加网络的层数、神经元数量等。
4. **尝试其他模型**：如果增加模型容量仍然无法解决欠拟合问题，可以尝试使用其他更复杂的模型，或者调整模型的架构以更好地适应数据。



### 常见的损失函数

1. 均方误差：适用于回归任务，用于衡量预测值与真实值之间的平均差异。

   优点：在训练过程中，优化过程较为简单，且可微分，有利于梯度下降。

   缺点：对异常值敏感，可能导致模型过度关注异常值。

2. 交叉熵损失：交叉熵损失在分类任务中表现良好，它将概率分布的差异转化为损失值，使得模型更加关注于正确类别的预测。

   优点：对于分类任务，尤其是二分类或多分类任务，交叉熵损失通常是首选的损失函数。

   缺点：不适用于回归任务，而且可能出现梯度消失问题。

3. 平均绝对误差：MAE 对异常值相对较为鲁棒，因为它是误差的绝对值的平均值。

   优点：MAE 对异常值不那么敏感，能够更好地反映数据的实际分布情况。

   缺点：不易优化，因为它在零点附近不是光滑可微的。

4. Hinge损失：Hinge损失适用于支持向量机等分类器，对于大间隔分类有较好的性能。

   优点：对于分类任务，能够产生稀疏的解，适用于处理大规模数据。

   缺点：不适用于概率输出的模型，不能直接用于多类别分类问题。

5. Dice损失：主要用于图像分割任务，能够衡量预测分割与真实分割之间的重叠程度。

   优点：适用于处理像素级别的分割任务，尤其在医学图像分割领域有较好的表现。

   缺点：可能无法直接推广到其他类型的任务。



### 讲一讲常见的激活函数和优缺点

1. **Sigmoid 函数**：$\sigma(x)=\frac{1}{1+e^{-x}}$​

   优点：1.输出范围在 (0, 1) 之间，可以用于二分类问题的输出；2. 具有良好的导数性质，易于求导。

   缺点：1. Sigmoid 函数的梯度在接近饱和区域时会变得很小，导致梯度消失问题；2 . 执行指数运算，计算机运行得较慢。3. 函数输出不是以 0 为中心的，这会降低权重更新的效率；

2. ReLU函数： $f(x)=max(0, x)$

   优点：1. 计算简单，只需比较输入是否大于零。2. 在正数区间上不存在梯度消失问题，能够加速模型的收敛。

   缺点：1. 在负数区间上输出为零，可能导致神经元死亡; 2. 函数输出不是以 0 为中心的，这会降低权重更新的效率；

3. Tanh函数：$f{x}=\frac{2}{1+e^{-2x}}-1$

​	优点：1. 输出以零为中心，有助于缓解梯度消失问题。2. 输出以零为中心

​	缺点：1. 仍然存在梯度消失问题，特别是在接近饱和区域时。

4. Softmax函数: $f(x)=\frac{e^{x_i}}{\sum_{i=1}^ne^{x_j}}$

​	优点： 适用于多类别分类问题，可以将网络输出转换为概率分布。

​	缺点： 求解过程中可能会受到数值稳定性的影响，特别是当输入较大或较小时。



### 神经网络的初始化方法

1. **零初始化（Zero Initialization）：** 将所有权重参数和偏置参数初始化为零。这种方法简单易行，但可能会导致网络对称性问题，并且不适用于深层网络。

2. **随机初始化（Random Initialization）：** 将权重参数和偏置参数初始化为随机的小值，通常服从某种均匀分布或高斯分布。常见的方法包括使用均匀分布 $[-\epsilon, \epsilon]$ 或者高斯分布 $N(0, \sigma^2)$，其中 $\epsilon$ 和 $\sigma$ 是根据网络结构和层数进行调整的超参数。

3. **Xavier初始化（Xavier Initialization）：** 也称为Glorot初始化，它根据每一层的输入和输出神经元数量来自适应地初始化权重参数。通常，Xavier初始化使用均匀分布，其方差计算如下：

   - 对于sigmoid激活函数：$Var(W) = \frac{2}{n_{in} + n_{out}}$
   - 对于tanh激活函数：$Var(W) = \frac{1}{n_{in} + n_{out}}$
   - 对于ReLU激活函数：$Var(W) = \frac{2}{n_{in}}$

   其中 $n_{in}$ 和 $n_{out}$ 分别是当前层的输入和输出神经元数量。

4. **He初始化（He Initialization）：** 与Xavier初始化类似，但是适用于ReLU激活函数。He初始化使用均匀分布，其方差计算如下：

   - 对于ReLU激活函数：$Var(W) = \frac{2}{n_{in}}$

   其中 $n_{in}$ 是当前层的输入神经元数量。

5. **预训练初始化（Pretrained Initialization）：** 在使用预训练模型进行微调时，可以使用已经训练好的模型的参数进行初始化，这种方法在迁移学习中非常常见。



### 正则化为什么可以增加模型泛化能力

**只要一个模型足够复杂，它是不是可以记住所有的训练集合样本之间的映射，代价就是模型复杂，带来的副作用就是没见过的只是略有不同的样本可能表现地就很差**。造成这种情况的问题就是学的太过，参数拟合的太好以致于超过了前面那个训练曲线的最低泛化误差临界点，究其根本原因是模型的表达能力足够强大到过拟合数据集。



### 分类任务目标函数

1. **交叉熵损失（Cross-Entropy Loss）**：

   - 交叉熵是一种用于衡量两个概率分布之间差异的指标。在分类任务中，交叉熵损失用于衡量模型对于每个类别的预测概率与真实标签之间的差异。
   - 交叉熵损失函数通常用于多分类任务，例如 softmax 分类器的输出与真实标签之间的交叉熵损失。
   - 公式：$L=-\sum_{i=1}^Ny_ilog(\hat y_i)$

2. **Hinge Loss**：

   - Hinge Loss 主要用于支持向量机（SVM）等模型的训练，通常用于二分类任务。它衡量了模型对于正确类别的预测分数是否大于错误类别的预测分数，如果差异大于一定阈值则不产生损失，否则产生损失。
   - 公式：Hinge Loss=max⁡(0,1−�⋅�^)Hinge Loss=max(0,1−*y*⋅*y*^) 其中 �*y* 是真实标签（1 或 -1），�^*y*^ 是模型的预测。

3. **Logistic Loss（Log Loss）**：

   - Logistic Loss 也称为对数损失，通常用于二分类任务中。它基于逻辑回归模型的概率预测，衡量了模型对于样本属于正类别的概率的预测与真实标签的差异。
   - 公式：$L=-ylog(\hat y)-(1-y)log(1-\hat y)$

   交叉熵损失更适用于多分类问题，因为它可以直接衡量模型对于每个类别的概率预测与真实分布之间的差异。Logistic 损失更适用于二分类问题，它与逻辑回归模型的概率预测密切相关，适用于输出为二元值的情况。

   

### 交叉熵的求解过程？N 分类任务;

```python
def cross_entropy(y_true, y_pred):
    """
    y_true: shape[m, N], m个样本，N个类别, one_hot
    y_pred: shape[m, N], m个样本，N个类别, 概率值
    """
    epsilon = 1e-5
    loss = -np.mean(y_true*np.log(y_pred+epsilon))
    return loss
```



### 常见的注意力机制

1. SE模块

```python
class SELayer(nn.Module):
    def __init__(self, channel, reduction=16):
        super(SELayer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=False),
            nn.Sigmoid()
        )
 
    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)
```



![img](https://img-blog.csdnimg.cn/e68deb8b324841d09aaced4ce9cd3a8e.png)

2. CA模块

```python
import torch
from torch import nn
class CA_Block(nn.Module):
    def __init__(self, channel, h, w, reduction=16):
        super(CA_Block, self).__init__()
        self.h = h
        self.w = w
        self.avg_pool_x = nn.AdaptiveAvgPool2d((h, 1))
        self.avg_pool_y = nn.AdaptiveAvgPool2d((1, w))
        self.conv_1x1 = nn.Conv2d(in_channels=channel, out_channels=channel//reduction, kernel_size=1, stride=1, bias=False)
        self.relu = nn.ReLU()
        self.bn = nn.BatchNorm2d(channel//reduction)
        self.F_h = nn.Conv2d(in_channels=channel//reduction, out_channels=channel, kernel_size=1, stride=1, bias=False)
        self.F_w = nn.Conv2d(in_channels=channel//reduction, out_channels=channel, kernel_size=1, stride=1, bias=False)
        self.sigmoid_h = nn.Sigmoid()
        self.sigmoid_w = nn.Sigmoid()
    def forward(self, x):
        x_h = self.avg_pool_x(x).permute(0, 1, 3, 2)
        x_w = self.avg_pool_y(x)
        x_cat_conv_relu = self.relu(self.conv_1x1(torch.cat((x_h, x_w), 3)))
        x_cat_conv_split_h, x_cat_conv_split_w = x_cat_conv_relu.split([self.h, self.w], 3)
        s_h = self.sigmoid_h(self.F_h(x_cat_conv_split_h.permute(0, 1, 3, 2)))
        s_w = self.sigmoid_w(self.F_w(x_cat_conv_split_w))
        out = x * s_h.expand_as(x) * s_w.expand_as(x)
        return out
```

![img](https://img-blog.csdnimg.cn/232b3dc1ed854fcfb97607d2f71ad7cd.png)

3. 自注意力机制：用于处理序列数据，如文本、语音等。它能够根据序列中各个元素之间的相互关系动态地计算每个元素的权重。

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert embed_dim % num_heads == 0, "Embedding dimension must be divisible by the number of heads."
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # 初始化线性变换矩阵
        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)

        # 输出线性变换矩阵
        self.W_o = nn.Linear(embed_dim, embed_dim)

    def forward(self, query, key, value, mask=None):
        batch_size = query.shape[0]

        # 线性变换
        Q = self.W_q(query)  # 形状: (batch_size, seq_len, embed_dim)
        K = self.W_k(key)    # 形状: (batch_size, seq_len, embed_dim)
        V = self.W_v(value)  # 形状: (batch_size, seq_len, embed_dim)

        # 重塑为多头
        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # 形状: (batch_size, num_heads, seq_len, head_dim)
        K = K.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # 形状: (batch_size, num_heads, seq_len, head_dim)
        V = V.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # 形状: (batch_size, num_heads, seq_len, head_dim)

        # 计算注意力分数
        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))
        if mask is not None:
            scores.masked_fill_(mask == 0, float('-inf'))

        # 使用 softmax 激活函数计算注意力权重
        attn_probs = F.softmax(scores, dim=-1)  # 形状: (batch_size, num_heads, seq_len, seq_len)

        # 使用注意力权重对值进行加权求和
        attn_output = torch.matmul(attn_probs, V)  # 形状: (batch_size, num_heads, seq_len, head_dim)

        # 拼接多头并投影回原始嵌入维度
        attn_output = attn_output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.embed_dim)  # 形状: (batch_size, seq_len, embed_dim)

        # 使用线性变换得到最终的注意力输出
        attn_output = self.W_o(attn_output)  # 形状: (batch_size, seq_len, embed_dim)

        return attn_output
```



3. 交叉注意力机制：用于处理具有不同输入的模型，如图像与文本之间的关系。它允许模型在处理不同输入时动态地计算各个输入之间的关联性，从而更好地整合不同输入的信息。



### 为什么Transformer要使用LayerNorm

归一化的公式：
$$
Norm(x)=\gamma\frac{x-\mu}{\sqrt{\sigma^2+\epsilon}}+\beta
$$
其中，$\gamma$和$\beta$是可学习的缩放和平移参数。

BatchNorm中：
$$
\mu=\frac{1}{m}\sum_{i=1}^mx^i
$$

$$
\sigma^2=\frac{1}{m}\sum_{i=1}^m(x^i-\mu)^2
$$

其中，m是批量样本的大小，$x^i$是第$i$个样本的输出；把一个batch中同一个通道的所有特征是为一个分布（有几个通道就有几个分布），并将其标准化，这意味着：不同图片的的同一通道的相对关系是保留的，即不同图片的同一通达的特征是可以比较的；同一图片的不同通道的特征则是失去了可比性；

LayerNorm中：
$$
\mu=\frac{1}{n}\sum_{i=1}^nx_i
$$

$$
\sigma^2=\frac{1}{n}\sum_{i=1}^n(x_i-\mu)^2
$$

其中，$n$ 是特征的维度，$x_i$是第$i$ 个特征的值。把一个句子的所有词向量视为一个分布（有几个句子就有几个分布），并将其归一化。这意味着：同一个句子中的词向量的相对大小是保留的；不同句子的词向量失去了可比性。



任何norm的意义都是为了让使用norm的网络的输入的数据分布变得更好，也就是转换为标准正态分布，数值进入敏感度区间，以减缓梯度消失，从而更容易训练。当然，这也意味着舍弃了除此维度之外其他维度的其他信息。首先要明确，如果在一个维度内进行normalization，那么在这个维度内，相对大小有意义的，是可以比较的；但是在normalization后的不同的维度之间，相对大小这是没有意义的。

相比于稳定前向输入分布，反向传播时mean和variance计算引入的梯度更有用，可以稳定反向传播时loss对输入的梯度。LN特别适合处理变长数据，因为是对channel维度做操作(这里指NLP中的hidden维度)，和句子长度和batch大小无关。

自己的理解：不同句子之间的词向量不需要有可比较的关系；



### 为什么self-attention可以堆叠多层，有什么作用

Self-attention（自注意力）能够捕捉输入序列中的长距离依赖关系，通过堆叠多层self-attention，模型可以学习序列中更深层次的模式和依赖关系。多层self-attention就像神经网络中的多个隐藏层一样，使模型能够学习和表示更复杂的函数。



### Transformer与CNN的优缺点

**Transformer 的优点：**

1. **适用于序列数据：** Transformer 主要应用于处理序列数据，如自然语言处理（NLP）任务中的文本序列。由于其自注意力机制的引入，Transformer 能够捕获序列中任意两个位置之间的依赖关系，从而更好地建模长距离依赖。
2. **并行计算：** Transformer 的自注意力机制允许每个位置的输入直接和所有其他位置的输入进行交互，使得模型在处理长序列时具有较好的并行性，能够高效地利用硬件资源。
3. **位置编码：** Transformer 使用位置编码来表示输入序列中的位置信息，这使得模型能够区分不同位置的词或符号，有助于模型理解序列的顺序信息。
4. **可解释性：** Transformer 模型的自注意力机制使得模型在生成预测时能够关注输入序列中的不同部分，从而具有一定的可解释性，可以分析模型在做出决策时的注意力分布情况。

**Transformer 的缺点：**

1. **计算复杂度高：** Transformer 模型通常需要较大的参数量，且在处理长序列时，由于自注意力机制的全连接特性，计算复杂度较高，可能导致训练和推理速度较慢。
2. **数据量要求高：** Transformer 模型通常需要大量的数据来进行训练，特别是在没有预训练模型的情况下，需要更多的数据来学习到有效的表示。

**CNN 的优点：**

1. **适用于图像数据：** CNN 主要应用于处理图像数据，具有良好的特征提取能力，能够捕获图像中的局部特征和层次结构。
2. **参数共享和稀疏连接：** CNN 中的卷积层具有参数共享和稀疏连接的特性，这使得模型具有较少的参数量和更高的参数效率，适合处理大规模图像数据。
3. **平移不变性：** CNN 中的卷积操作具有平移不变性，即对图像的平移操作具有一定的鲁棒性，能够在一定程度上保持对图像中相同特征的识别能力。

**CNN 的缺点：**

1. **局限于固定大小的输入：** CNN 在处理图像时通常需要固定大小的输入，因此对于尺寸不一致的图像数据需要进行预处理或者裁剪，可能会丢失一部分信息。
2. **不适用于序列数据：** CNN 不适用于处理序列数据，因为其卷积操作是基于固定大小的局部感受野进行的，无法捕获序列中不同位置之间的依赖关系。

### Transformer 中的位置编码在哪实现的？是固定的，还是可以学习？

位置编码通常被加到输入嵌入之后，以将位置信息与词嵌入结合起来，从而使模型能够区分不同位置的词。

一种常用的位置编码方法是使用正弦和余弦函数的组合：
$$
PE(pos, 2i)=sin(pos/10000^{2i/d_{model}})
$$

$$
PE(pos,2i+1)=cos(pos/10000^{2i/d_{model}})
$$



### 多头注意力机制和单头相比有什么优势

多头注意力机制可以同时关注输入的不同部分，并在不同的表示空间中学习到不同的表示。这使得模型能够更好地捕捉输入序列的局部和全局信息，提高了表示的丰富性和多样性；多头的本质是多个独立的attention计算，作为一个集成的作用，防止过拟合，因为每个头都可以学习到不同的关注点，从而减少了对单个注意力头的过度依赖；



### 多头注意力机制的计算复杂度

矩阵乘法的复杂度：假设A是$n×m$的矩阵，$B$是$m×p$的矩阵，那么$A×B$的复杂度为`O(mnp)`

1. 对于$n×d$和$d×n$的矩阵$Q$,$K^T$，矩阵相乘的复杂度为$O(n^2d)$​
2. softmax函数：$O(n^2)$
3. 加权平均：矩阵相乘，$O(n^2d)$

所以自注意力机制的计算复杂度为$O(n^2d)$

多头注意力机制的计算复杂度为$O(mn^2h)=O(n^2d)$



### Transformer中的head为什么要降维？

就是希望每个注意力头，只关注最终输出序列中一个子空间，互相**独立**。其核心思想在于，抽取到更加丰富的**特征信息**。在**不增加时间复杂度**的情况下，同时，借鉴**[CNN](https://www.zhihu.com/search?q=CNN&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A1718672303})多核**的思想，在**更低的维度**，在**多个独立的[特征空间](https://www.zhihu.com/search?q=特征空间&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A1718672303})**，**更容易**学习到更丰富的[特征信息](https://www.zhihu.com/search?q=特征信息&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A1718672303})。



### ViT 结构



### DETR中匈牙利匹配算法的具体流程

DETR不需要NMS，采用的是集合预测损失。在DETR中固定会输出N个预测框，如何将预测框与GT对应起来？

二分匹配问题：找到一组边集合，这组边集合没有公共的顶点。

例子：N个工人，M个任务，每个工人对应不同任务的价钱不同，如何以最小的代价完成任务。

![image-20240319161646416](C:\Users\scutbci\AppData\Roaming\Typora\typora-user-images\image-20240319161646416.png)

第一步，构建匹配代价$L_{match}$​​来进行匈牙利算法的最优分配，其中考虑了Ground truth与预测框之间的类别预测以及距离的代价；第二步，对前一步中匹配的所有配对的匈牙利损失，这个损失函数为类预测的负对数似然和边界框损失的线性组合。

### 实例的匹配问题

```python
    def pair_coordinates(setA, setB, radius):
        """Use the Munkres or Kuhn-Munkres algorithm to find the most optimal
        unique pairing (largest possible match) when pairing points in set B
        against points in set A, using distance as cost function.
        Args:
            setA, setB: np.array (float32) of size Nx2 contains the of XY coordinate
                        of N different points
            radius: valid area around a point in setA to consider
                    a given coordinate in setB a candidate for match
        Return:
            pairing: pairing is an array of indices
            where point at index pairing[0] in set A paired with point
            in set B at index pairing[1]
            unparedA, unpairedB: remaining poitn in set A and set B unpaired
        """
        # * Euclidean distance as the cost matrix
        pair_distance = scipy.spatial.distance.cdist(setA, setB, metric='euclidean')

        # * Munkres pairing with scipy library
        # the algorithm return (row indices, matched column indices)
        # if there is multiple same cost in a row, index of first occurence
        # is return, thus the unique pairing is ensured
        indicesA, paired_indicesB = linear_sum_assignment(pair_distance)

        # extract the paired cost and remove instances
        # outside of designated radius
        pair_cost = pair_distance[indicesA, paired_indicesB]

        pairedA = indicesA[pair_cost <= radius]
        pairedB = paired_indicesB[pair_cost <= radius]

        pairing = np.concatenate([pairedA[:, None], pairedB[:, None]], axis=-1)
        unpairedA = np.delete(np.arange(setA.shape[0]), pairedA)
        unpairedB = np.delete(np.arange(setB.shape[0]), pairedB)
        return pairing, unpairedA, unpairedB
```



### 匈牙利匹配算法

```C++
// 最大二分匹配
int graph[n][m]; // 矩阵
int match[m];      // 记录每个右边顶点匹配到的左边顶点
bool visited[n];   // 记录每个左边顶点是否已经被访问过
int n, m; // n为左边顶点的数量，m为右边顶点的数量
// 匈牙利算法的DFS函数
bool dfs(int u) {
    for (int v = 1; v <= m; ++v) {
        if (graph[u][v] && !visited[v]) { // 如果u与v之间有边，且v未被访问
            visited[v] = true; // 标记v已被访问
            if (match[v] == 0 || dfs(match[v])) { // 如果v未被匹配或者可以找到增广路径
                match[v] = u; // 更新匹配关系
                return true; // 找到增广路径，返回true
            }
        }
    }
    return false; // 无法找到增广路径，返回false
}

// 匈牙利算法主函数
int hungarian() {
    int cnt = 0; // 记录匹配的数量
    memset(match, 0, sizeof(match)); // 初始化匹配数组为0

    // 遍历每一个左边顶点，尝试进行匹配
    for (int i = 1; i <= n; ++i) {
        memset(visited, false, sizeof(visited)); // 初始化visited数组为false
        if (dfs(i)) { // 如果可以找到增广路径
            cnt++; // 匹配数量加1
        }
    }

    return cnt; // 返回匹配数量
}
```





### TensorRT优化流程

1. **网络定义和训练：** 首先，你需要定义一个深度学习模型并进行训练，通常使用常见的深度学习框架如TensorFlow、PyTorch或Caffe等。这个模型可以是用于分类、目标检测、语义分割等各种任务的模型。
2. **模型转换：** 接下来，你需要将训练好的模型转换为TensorRT的可优化格式。TensorRT支持从常见的深度学习框架（如TensorFlow、PyTorch等）导入模型，并将其转换为TensorRT的网络结构格式。这个过程通常被称为模型优化。
3. **网络优化：** 一旦模型被转换为TensorRT的格式，TensorRT可以对网络进行各种优化，以提高推理性能。这些优化包括结构优化、精度混合、内存优化、层融合、图剪枝等。
4. **引擎构建：** 在网络优化完成后，你需要使用TensorRT API构建推理引擎。推理引擎是TensorRT中的一个重要概念，它是一个针对特定硬件配置和推理需求进行了优化的二进制文件。构建引擎的过程会将优化后的网络结构映射到特定的硬件上，以实现最佳的推理性能。
5. **推理过程：** 最后，你可以使用TensorRT推理引擎对输入数据进行推理。推理引擎会利用之前优化的网络结构和硬件特性，高效地执行推理操作，并生成模型对输入数据的预测结果。



### 介绍YOLO v3~YOLO v8



### 为什么业内yolov5用的比较多而不是yolov8

### focal loss如何计算

Focal Loss（焦点损失）是一种用于解决类别不平衡问题的损失函数。Focal Loss在计算损失时对易分样本的权重进行了降低，这样可以减少易分样本对训练的影响，从而更加关注那些难分的样本，提高模型在类别不平衡情况下的性能。
$$
FL(p_t)=-\alpha(1-p_t)^\gamma log(p_t)
$$


### Iou loss 比 smooth l1 loss好在哪

可以反映预测框与真实框的接近程度；具有很好的**尺度不变性**，也就是尺度不敏感。

Smooth L1 loss 求出4个点（x，y，w，h）Loss后，再相加得到最终的Loss，这种做法的假设是4个点事相互独立的，但实际上是有一定相关性的。

在目标检测时，实际是用IOU来作为评价目标框的指标，而上述Loss函数与IOU是不等价的，**多个目标框可能有相同大小的Smooth L1 Loss，但是他们的IOU可能差异很大**。



### 计算IOU

```python
def calculate_iou(box_1, box_2):
    """
    calculate iou
    :param box_1: (x0, y0, x1, y1)
    :param box_2: (x0, y0, x1, y1)
    :return: value of iou
    """
    # calculate area of each box
    area_1 = (box_1[2] - box_1[0]) * (box_1[3] - box_1[1])
    area_2 = (box_2[2] - box_2[0]) * (box_1[3] - box_1[1])
 
    # find the edge of intersect box
    top = max(box_1[0], box_2[0])
    left = max(box_1[1], box_2[1])
    bottom = min(box_1[3], box_2[3])
    right = min(box_1[2], box_2[2])
 
    # if there is an intersect area
    if left >= right or top >= bottom:
        return 0
 
    # calculate the intersect area
    area_intersection = (right - left) * (bottom - top)
 
    # calculate the union area
    area_union = area_1 + area_2 - area_intersection
 
    iou = float(area_intersection) / area_union
 
    return iou
```





### 谈一下 faster rcnn 和 yolo

单阶段的正负样本太不均衡，两阶段的会划分正负样本均匀化。



### YOLO v8和YOLO v5的区别

### 目标检测中，单双阶段分别目的是什么？

单阶段检测器旨在通过单个神经网络模型直接检测图像中的目标，而不需要额外的区域提议生成步骤。因此，它们的主要目的是简化检测流程，提高检测速度。

双阶段检测器的主要目的是提高检测的准确性，即在保持较高准确率的同时，可能牺牲一些检测速度。它们通常包含两个主要阶段：生成候选区域和对候选区域进行分类与回归。



### RPN（Region Proposal Network）网络

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201124183306197.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2l0bGlseWVy,size_16,color_FFFFFF,t_70#pic_center)

1. 生成Anchor： 特征图中的每一个点都是一个anchor，每个anchor中有9中不同大小和宽高比的预设anchor box；
2. FPN利用一个3*3的卷积核得到一个256维度的向量，分别通过全连接层降维成2×9与4×9维，通过softmax进行二分类已经边框回归来使得anchor box更接近ground truth；



### Faster RCNN 中检测的网络如何适应不同的框的大小？

1. 采用了一种称为金字塔特征金字塔（Feature Pyramid Network，FPN）的技术。FPN 是一种多尺度特征图提取方法，可以在不同层级上生成具有不同分辨率的特征图，使模型能够在多个尺度下进行检测。
2. 预设了3中不同大小宽高的anchor box；

### 谈谈DETR, DETR 中的 query 怎么来的？



### 为什么UNet在医学图像上表现这么好

1. 医学图像语义较为简单、结构较为固定。所以高级语义信息和低级特征都显得很重要(UNet的skip connection和[U型结构](https://www.zhihu.com/search?q=U型结构&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A586501606})就派上了用场)。
2. .数据量少。医学影像的数据获取相对难一些，很多比赛只提供不到100例数据。所以我们设计的模型不宜多大，参数过多，很容易导致[过拟合](https://www.zhihu.com/search?q=过拟合&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A586501606})。



### 增强数据集的方法



### 密集行人检测的遮挡问题怎么解决

1. 损失函数Repulsion loss：
   $$
   L_{Rep}=L_{Attr}+\alpha*L_{RepGT}+\beta*L_{RepBox}
   $$
   其中 $$L_{Attr} $$ 是吸引项，需要预测框靠近其指定目标；$L_RepGT$ 和 $L_RepBox$ 是排斥项，分别需要预测框远离周遭其他的 groundtruth 物体和其他指定目标不同的预测框。



### 自动驾驶检测模型如何针对corner case优化

Corner cases(CC)是指不经常出现或一些极端的场景数据，也是一种长尾问题的表现形式。

**置信度(Confidence score)：**通过神经网络估计不确定coner case的置信的，确定是否是异常情况。

**重建(Reconstructive):**对异常对象进行重建，或者仿真模拟得出异常对象数据，加入模型训练。

**生成coner case（Generative):**对异常对象和数据进行扩充，利用GAN等技术生成扩充的coner case案例。

**特征抽取coner case(Feature Extraction):**利用聚类或者svm等分类算法，对coner case案例进行快速的定位。

**预测补充coner case(Prediction):**利用前后帧之间运动物体不会凭空消失或突变等，进行异常情况的校正回归。



### 介绍NMS及其变体

1. Soft-NMS：通过降低重叠边界框的置信度来保留更多信息，将其得分进行惩罚衰减

### NMS的缺点及其改进工作

```python
# 很多检测框都是检测同一个目标，但最终每个目标只需要一个检测框，NMS选择那个得分最高的检测框
import numpy as np

def calculate_iou(box1, box2):
    """
    计算两个边界框的交并比（IoU）
    """
    # 提取边界框的坐标
    x1_tl, y1_tl, x1_br, y1_br = box1
    x2_tl, y2_tl, x2_br, y2_br = box2

    # 计算交集的坐标
    x_intersection = max(0, min(x1_br, x2_br) - max(x1_tl, x2_tl))
    y_intersection = max(0, min(y1_br, y2_br) - max(y1_tl, y2_tl))
    intersection = x_intersection * y_intersection

    # 计算并集的面积
    area_box1 = (x1_br - x1_tl) * (y1_br - y1_tl)
    area_box2 = (x2_br - x2_tl) * (y2_br - y2_tl)
    union = area_box1 + area_box2 - intersection

    # 计算交并比
    iou = intersection / union
    return iou

def non_max_suppression(boxes, scores, threshold):
    """
    非极大值抑制算法
    """
    # 按照置信度排序
    sorted_indices = np.argsort(scores)[::-1]
    selected_indices = []

    while len(sorted_indices) > 0:
        # 选择置信度最高的边界框
        best_index = sorted_indices[0]
        selected_indices.append(best_index)

        # 计算当前边界框与其余边界框的IoU
        ious = [calculate_iou(boxes[best_index], boxes[idx]) for idx in sorted_indices[1:]]

        # 移除与当前边界框IoU大于阈值的边界框
        remove_indices = np.where(np.array(ious) > threshold)[0] + 1
        sorted_indices = np.delete(sorted_indices, remove_indices)

    return selected_indices
```

##### 缺点：

1. 参数敏感：NMS的性能受到参数的影响，不同的数据集和场景可能需要不同的参数设置。
2. 计算开销： 在大规模数据集上运行NMS可能会消耗大量的计算资源
3. 抑制不完全：NMS可能会错误地移除真实目标边界框，特别是当目标之间有部分遮挡或重叠时。
4. 处理不规则目标困难：对于不规则形状或大小不一的目标，NMS的效果可能不理想。
5. 将相邻检测框的分数均强制归零

改进方法：

1. Soft-NMS：通过降低重叠边界框的置信度来保留更多信息，将其得分进行惩罚衰减



### 多卡的BN如何实现同步

1. **跨GPU批量归一化（Cross-GPU Batch Normalization）：** 在这种方法中，每个GPU都有自己的批量归一化层。在每个mini-batch的处理过程中，每个GPU分别计算出均值和方差，并进行归一化处理。然后，通过在所有GPU上汇总并取平均，来计算整个mini-batch的全局均值和方差。最后，所有GPU上的数据都使用这些全局均值和方差进行归一化。这种方法在训练时可以实现较好的批量归一化效果，但需要在推理时仔细处理，因为此时可能不再有多个GPU可用。
2. **同步批量归一化（Synchronized Batch Normalization）：** 这种方法在训练时和推理时都可以有效地实现批量归一化。在同步批量归一化中，所有GPU上的数据都收集到一个单一的批量上。然后，通过在所有GPU上计算均值和方差，并进行同步操作来得到全局均值和方差。最后，所有GPU上的数据都使用这些全局均值和方差进行归一化。这种方法保证了所有GPU上的数据都使用相同的均值和方差进行归一化，从而确保了批量归一化的一致性。



### 如何检测到未知目标

**开放集识别：**模型应该拒绝未知类，而不是以高置信度将其辨认为已知类。

1. OpenMax:
2. 模型集成：
3. 置信度估计：



### 模型训练数据并行，训练优化

### 1. 数据并行（Data Parallelism，DP）：

数据并行是指将模型的副本分布在多个设备上，每个副本处理输入数据的不同批次（batch）。训练的每一步，每个副本都会计算损失和梯度，并使用梯度平均或者其他方法来更新模型参数。数据并行可以有效地利用多个GPU来加速训练过程。

**原理步骤**：

1. 将模型复制到多个设备（通常是多个GPU）上。
2. 将输入数据分成多个批次，并分配给不同的设备。
3. 每个设备上的模型副本计算损失和梯度。
4. 将梯度从每个设备上收集，并进行平均或其他合并操作。
5. 使用平均梯度来更新模型参数。

PyTorch提供了`torch.nn.DataParallel`模块来实现数据并行训练。使用该模块时，PyTorch会自动在多个GPU上复制模型，并在计算过程中处理梯度的合并和参数的更新。

### 2. 分布式数据并行（Distributed Data Parallelism，DDP）：

分布式数据并行是将数据和模型同时分布在多个设备和多个处理节点上进行训练。每个处理节点上可能包含多个设备，可以是GPU或者CPU。分布式训练通常用于更大规模的模型和数据集，并可以跨多个机器进行训练。

**原理步骤**：

1. 将模型和数据划分为多个部分，并分配给不同的处理节点和设备。
2. 每个处理节点上的模型副本计算损失和梯度。
3. 使用通信操作将梯度从不同的处理节点上收集，并进行合并，All reduce。
4. 使用合并后的梯度来更新模型参数。

PyTorch提供了`torch.nn.parallel.DistributedDataParallel`模块来实现分布式数据并行训练。该模块可以与PyTorch的分布式后端一起使用，如`torch.distributed`，以实现在多个处理节点上的并行训练。



### pytorch里面两种浮点类型怎么样混合计算的

利用单精度浮点数（float32）和半精度浮点数（float16）的混合来提高训练速度和减少内存占用。

在计算过程中，PyTorch会自动将梯度和参数转换为合适的精度。通常情况下，梯度计算会使用float32进行累积和计算，而参数更新则可以使用float16来提高计算速度。

由于float16的动态范围较小，可能会导致梯度消失或爆炸的问题。为了解决这个问题，混合精度通常会对损失进行缩放，使其适应float16的范围。这可以通过调整损失值的放大因子来实现。



### 为什么MIL的梯度传递不到CNN

1. **高分辨率与海量补丁**：WSIs具有极高的分辨率，通常包含成千上万甚至几十万个补丁（patch）。每个补丁在特征提取阶段需要经过多层卷积网络（如ResNet50）处理，生成中间特征图（activation maps）。这些特征图在反向传播时是梯度计算所必需的，用于更新网络权重。
2. **内存限制与存储需求**：为了进行梯度更新，所有补丁在前向传播过程中产生的中间特征图必须同时存储在GPU内存中。然而，每个补丁的中间特征图占用的内存相当大（如文中给出的ResNet50处理一个256x256补丁需约98.25MB），对于成千上万的补丁，总内存需求会迅速飙升至数百GB，远超出普通计算平台的GPU内存容量。
3. **内存瓶颈与训练障碍**：由于上述巨大的内存需求，直接端到端地优化整个MIL模型（包括特征提取器和MIL聚合器）会导致严重的内存溢出，无法在实际硬件条件下完成反向传播和梯度更新。因此，梯度无法有效地从MIL聚合器回传到特征提取器，阻碍了特征提取器的学习与优化。



### BCL中的贝叶斯体现在哪里

在这篇文章中，贝叶斯定理体现在对整个协作学习框架的建模过程中，尤其是在目标全片图像（Whole-Slide Image, WSI）分类器与辅助补丁分类器协作学习的统一贝叶斯概率框架中。具体而言，贝叶斯定理的应用体现在以下几个关键点：

1. **最大似然估计（Maximum Likelihood Estimation, MLE）问题的设定**：从贝叶斯概率视角出发，文章将协作学习任务表述为一个最大似然估计问题，即寻找模型参数 \( \theta \) 以最大化训练数据集中所有样本 \( (X_i, Y_i) \) 的联合概率。公式(3)表达了这一目标，其中 \( P(Y_i|X_i;\theta) \) 是在给定 \( X_i \) 时观察到 \( Y_i \) 的条件概率。通过最大化这个概率的对数乘积，模型试图找到最能解释数据生成过程的参数集。

2. **引入隐变量**：为了便于后续使用EM算法进行优化，引入了隐变量 \( Z_i = (z_{i1}, ..., z_{iN_i}) \)，其中 \( z_{in} \) 表示 \( X_i \) 中第 \( n \) 个补丁 \( x_{in} \) 的类别标签。这一步体现了贝叶斯统计中利用潜在变量来描述复杂数据生成机制的特点。

3. **联合概率分布**：定义了联合概率 \( P(Y_i, Z_i|X_i;\theta) \)，它反映了给定 \( X_i \) 时，WSI标签 \( Y_i \) 和所有补丁标签 \( Z_i \) 同时出现的概率。通过引入隐变量 \( Z_i \)，模型可以处理WSI分类中涉及的多个实例（补丁）之间的关系，而不仅仅依赖于单个实例的标签。

4. **使用EM算法求解**：EM算法（Expectation-Maximization）是一种迭代优化方法，专门用于含有隐变量的模型参数估计。在这种情况下，它被用来解决由贝叶斯概率框架定义的最大似然估计问题。EM算法通过交替执行E步（期望步）和M步（最大化步），逐步逼近模型参数的最大似然估计值。

   - **E步**：计算在当前模型参数下，给定观测数据和WSI标签时，所有可能的补丁标签配置 \( Z_i \) 的概率 \( $P(Z_i|X_i,Y_i;\theta_t)$ \)。E步中，利用贝叶斯规则将后验概率分解为似然和先验的乘积，这里通过对每个补丁进行伪标签分配来实现。

   - **M步**：基于E步得到的隐变量分布，更新模型参数以最大化包含所有样本的总目标函数。在M步中，模型参数的更新通过最大化 \( $\sum_{i=1}^{|D|} J_i(\theta,\theta_t)$ \) 来实现，其中 \( J_i \) 是根据E步计算出的每个样本的目标函数。这里的参数更新遵循了贝叶斯框架下极大后验概率估计（Maximum A Posteriori, MAP）的精神，虽然没有显式写出后验概率，但通过对似然函数的极大化，间接实现了参数的MAP估计。

综上所述，文章中的贝叶斯定理体现在构建了一个基于贝叶斯概率的协作学习框架，该框架通过引入隐变量描述WSI分类问题中补丁级别的标签信息，并利用EM算法来解决含有隐变量的最大似然估计问题。EM算法正是在这种贝叶斯背景下的一种有效优化工具，它利用贝叶斯定理将复杂的联合优化问题分解成两个相对简单的步骤（E步计算期望，M步最大化目标函数），从而有效地估计模型参数。这种结合贝叶斯概率建模与EM算法的方法旨在解决WSI分类中的内存瓶颈问题，同时确保特征编码器与MIL聚合器能够协同学习，提高分类性能。



### CAM算法的原理





### 介绍SAM模型



### negative prompt怎么做的



### Stable Diffusion的结构和原理



### 大语言模型的微调方法



### Bert结构



### GPT和Bert的区别



### Bert 预训练学习目标是什么



### 介绍chatGPT的训练流程



### 介绍RLHF完整训练过程



### 怎么做提示工程

