<h1 style="text-align:center;">面试内容
</h1>
<h2 style="text-align:center;"> 计算机基础知识</h2>

### 如何将RGB图像转成灰度图像

常见的转换方法是将RGB三个通道的值按照一定的权重进行加权平均，得到对应的灰度值。这些权重通常是根据人眼对不同颜色的敏感程度来确定的，例如常用的权重系数是：**灰度值 = 0.2989 * R + 0.5870 * G + 0.1140 * B**

将灰度图转成RGB转换过程中会丢失颜色信息。一种简单的方法是将灰度值作为RGB的三个通道的值。另一种常见的方法是使用伪彩色映射（Pseudocolor mapping），即将灰度图的灰度值映射到一个伪彩色表中，将不同灰度值映射到不同的颜色上。

### 你了解设计模式吗？在python中如何实现单例模式？

- 单例模式（Singleton Pattern）：确保一个类只有一个实例，并提供一个全局访问点。
- 工厂模式（Factory Pattern）：定义一个用于创建对象的接口，但将实际的创建工作推迟到子类中。
- 抽象工厂模式（Abstract Factory Pattern）：提供一个创建一系列相关或相互依赖对象的接口，而无需指定其具体类。
- 建造者模式（Builder Pattern）：将一个复杂对象的构建与其表示分离，使得同样的构建过程可以创建不同的表示。

#### 如何实现单例模式

1. 在定义类前采用修饰器@singleton

2. 使用了类方法 `__new__`，在对象创建之前进行检查，确保只有一个实例被创建，并且保证了线程安全性。

   ```python
   class Singleton:
       _instance = None
   
       def __new__(cls, *args, **kwargs):
           if cls._instance is None:
               cls._instance = super().__new__(cls, *args, **kwargs)
           return cls._instance
   
   class MyClass(Singleton):
       def __init__(self, name):
           self.name = name
   
   # 创建实例
   obj1 = MyClass("instance1")
   obj2 = MyClass("instance2")
   
   print(obj1.name)  # 输出：instance1
   print(obj2.name)  # 输出：instance1（因为它是同一个实例）
   ```

   #### 单例模式的优点：

   1. **全局唯一实例：** 保证一个类只有一个实例，对于频繁使用的对象，可以节省系统资源。
   2. **延迟实例化：** 可以延迟实例的创建，直到需要时再进行初始化。
   3. **避免竞态条件：** 在多线程环境中，单例模式可以避免竞态条件，确保只有一个实例被创建。

   #### 单例模式的缺点：

   1. **全局状态：** 单例模式引入了全局状态，可能导致程序的复杂性增加。
   2. **隐藏依赖：** 单例模式可能隐藏了类之间的依赖关系，使得代码难以理解和维护。

   #### 适用场景：

   1. 当一个系统只需要一个实例来协调行为时。

   2. 当实例需要被频繁访问，而不希望通过参数传递来获取实例。

   3. 当一个类的实例需要被全局访问，并且这个实例负责协调操作。

   #### 如何实现工厂模式

   ```pyth
   class Product:
       def operation(self):
           pass
   
   class ConcreteProduct1(Product):
       def operation(self):
           return "Operation from ConcreteProduct1"
   
   class ConcreteProduct2(Product):
       def operation(self):
           return "Operation from ConcreteProduct2"
   
   class SimpleFactory:
       @staticmethod
       def create_product(product_type):
           if product_type == "product1":
               return ConcreteProduct1()
           elif product_type == "product2":
               return ConcreteProduct2()
           else:
               raise ValueError("Invalid product type")
   
   # 使用工厂创建对象
   factory = SimpleFactory()
   product1 = factory.create_product("product1")
   product2 = factory.create_product("product2")
   
   print(product1.operation())  # 输出：Operation from ConcreteProduct1
   print(product2.operation())  # 输出：Operation from ConcreteProduct2
   
   ```

   #### 工厂模式的优点：

   1. **封装对象创建逻辑：** 工厂模式将对象的创建过程封装在工厂类中，客户端无需了解对象的创建过程，只需通过工厂类获取对象。
   2. **可扩展性：** 新增产品类时，只需要扩展工厂类而不需要修改客户端代码，符合开闭原则。
   3. **解耦：** 将具体产品类和客户端代码解耦，使得系统更易于维护和扩展。

   #### 工厂模式的缺点：

   1. **类数量增多：** 随着产品类的增多，可能导致工厂类的数量也增多，使得代码结构复杂。
   2. **不符合开闭原则的变体：** 在简单工厂模式中，每次新增产品都需要修改工厂类，不符合开闭原则。

   #### 工厂模式适用情景：

   1. 当一个系统需要独立于其创建、组合和表示时。

   2. 当一个系统需要多个产品系列中的一种，并且客户端不关心这些对象的创建细节。

   3. 当一个系统需要动态配置对象。

   

### C++有什么特性？

1. **面向对象编程（Object-Oriented Programming, OOP）**：C++ 支持面向对象编程，包括类、对象、继承、多态等概念，使得代码结构更加模块化和可重用。
2. **泛型编程（Generic Programming）**：C++ 支持泛型编程，通过模板（template）实现通用的算法和数据结构，提高了代码的灵活性和复用性。
3. **多重继承（Multiple Inheritance）**：C++ 允许一个类继承自多个基类，这使得在设计复杂系统时能够更好地表达对象之间的关系。
4. **内存管理**：C++ 具有灵活的内存管理能力，包括手动内存管理（使用 new 和 delete 操作符）和智能指针（如 std::shared_ptr、std::unique_ptr），可以有效地管理内存资源。
5. **运算符重载（Operator Overloading）**：C++ 允许用户重载运算符，使得用户定义的类型可以像内置类型一样使用运算符进行操作。
6. **异常处理（Exception Handling）**：C++ 提供了异常处理机制，通过 try-catch 块捕获和处理异常，使得程序能够更加健壮和可靠。
7. **标准模板库（Standard Template Library, STL）**：STL 是 C++ 标准库的一部分，包含了丰富的通用数据结构和算法，如容器（vector、list、map 等）和算法（排序、查找、遍历等），提高了编程效率和代码质量。
8. **高效的性能**：C++ 是一种高性能的语言，具有接近于底层的控制和优化能力，适用于开发对性能要求较高的系统和应用程序。
9. **平台独立性**：C++ 是一种跨平台的语言，可以在多种操作系统上编译和运行，如 Windows、Linux、macOS 等。
10. **友元函数和友元类（Friend Function and Friend Class）**：C++ 允许将函数或类声明为另一个类的友元，使得这些函数或类能够访问该类的私有成员。
11. **支持低级操作**：C++ 允许直接操作内存，包括指针、引用、位操作等，使得开发者能够更加灵活地进行系统级别的编程。

#### 面向对象编程的特点和核心概念：

1. **类与对象**：
   - 类（Class）是一种用户自定义的数据类型，用于描述一类对象的共同属性和行为。类可以看作是对象的模板或蓝图。
   - 对象（Object）是类的实例，具体化了类的属性和行为。对象是程序中的实体，具有状态、行为和标识。
2. **封装（Encapsulation）**：
   - 封装是指将数据和操作封装在类的内部，隐藏了对象的内部实现细节，只提供公共的接口供外部访问。
   - 封装可以提高代码的安全性和可维护性，减少了对象之间的耦合度。
3. **继承（Inheritance）**：
   - 继承是一种机制，允许一个类（子类）继承另一个类（父类）的属性和方法，并且可以在此基础上进行扩展或修改。
   - 继承可以提高代码的重用性，减少了重复编写代码的工作量。
4. **多态（Polymorphism）**：
   - 多态是指同一个操作作用于不同的对象上时，可以产生不同的行为。
   - 多态通过函数重载、运算符重载和虚函数等机制实现，使得程序更加灵活和可扩展。

面向对象编程的主要目标是提高代码的**重用性、可扩展性和可维护性**，通过封装、继承和多态等特性，使得程序结构更加清晰和易于理解。

#### 标准模板库

STL 主要包含以下三个组件：

1. **容器（Containers）**： 容器是用来存储数据的数据结构，STL 提供了多种类型的容器，包括顺序容器和关联容器。
   - 顺序容器（Sequential Containers）：如 vector、list、deque、array 等，用于按顺序存储和访问元素。
   - 关联容器（Associative Containers）：如 set、map、multiset、multimap 等，用于按键值进行快速查找和访问元素。
2. **算法（Algorithms）**： 算法是对容器中的数据进行操作和处理的函数，STL 提供了一系列的通用算法，如排序、查找、合并、计算等。 这些算法通过迭代器（Iterators）来访问容器中的元素，使得算法与数据结构解耦合，提高了算法的复用性和灵活性。
3. **迭代器（Iterators）**： 迭代器是一种类似于指针的对象，用于在容器中遍历和访问元素，STL 提供了多种类型的迭代器，如输入迭代器、输出迭代器、正向迭代器、随机访问迭代器等。 迭代器提供了统一的接口，使得算法可以在不同类型的容器上进行操作，而无需关心容器的具体实现细节。

#### 虚函数的概念

通过在基类中声明虚函数，派生类可以覆盖（override）基类中的虚函数，从而在运行时确定调用的是哪个版本的函数。这种机制称为动态绑定（dynamic binding）或运行时多态（runtime polymorphism）。

**1. 在基类中声明虚函数：** 在基类中将函数声明为虚函数，使用 `virtual` 关键字进行修饰。这样，派生类可以选择性地覆盖基类中的虚函数。

```C++
class Base {
public:
    virtual void show() {
        cout << "Base class function" << endl;
    }
};
```

**2. 在派生类中覆盖虚函数：** 派生类可以重新定义基类中的虚函数，实现自己特定的行为。派生类中的函数声明必须与基类中的虚函数声明相匹配。

```C++
class Derived : public Base {
public:
    void show() override {
        cout << "Derived class function" << endl;
    }
};
```

3. **动态绑定：** 当基类指针（或引用）指向派生类对象时，通过虚函数的动态绑定机制，在运行时确定调用的是派生类中的版本还是基类中的版本。这样可以根据对象的实际类型来调用对应的函数。

```C++
int main() {
    Base* ptr = new Derived();
    ptr->show(); // 调用的是Derived类中的show函数
    delete ptr;
    return 0;
}
```



### Python与C++的区别

1. **语法和风格：**
   - Python 的语法简洁清晰，采用缩进来表示代码块，没有大括号；而 C++ 的语法更加严格，采用分号和大括号来表示代码块。
   - Python 是一种解释型语言，代码无需编译即可执行；而 C++ 是一种编译型语言，需要先编译成机器码再执行。
2. **类型系统：**
   - Python 是一种动态类型语言，变量的类型在运行时确定，无需显式声明；而 C++ 是一种静态类型语言，变量的类型在编译时确定，需要显式声明。
   - Python 的变量可以指向任意类型的对象；而 C++ 的变量必须指定特定类型，并且类型在编译时是固定的。
3. **内存管理：**
   - Python 使用自动内存管理机制（垃圾回收），通过引用计数和循环垃圾收集来管理内存；而 C++ 需要程序员手动管理内存，包括分配和释放内存。
   - Python 的内存管理机制简化了程序开发，但可能导致性能损失和内存泄漏；而 C++ 的手动内存管理更加灵活，但需要程序员更加小心地处理内存。
4. **面向对象编程：**
   - Python 和 C++ 都支持面向对象编程，但其实现方式有所不同。
   - Python 的面向对象特性更加简单和灵活，支持动态继承、动态绑定等特性；而 C++ 的面向对象特性更加丰富，包括多重继承、虚函数、纯虚函数等。
5. **标准库和生态系统：**
   - Python 拥有丰富的标准库和第三方库，涵盖了几乎所有领域的应用开发，如科学计算、网络编程、Web 开发等；而 C++ 的标准库和第三方库相对较少，需要程序员自己去选择和引入。
6. **适用场景：**
   - Python 适用于快速开发、原型设计、数据分析、科学计算等领域，尤其擅长处理复杂的业务逻辑和高层次的任务。
   - C++ 适用于系统编程、游戏开发、嵌入式系统、性能优化等领域，尤其擅长处理底层的系统级别的任务和对性能要求较高的场景。

### 在项目中使用过多线程吗，是如何实现的？

1. **线程创建：** 程序员可以使用操作系统提供的线程库（如 POSIX 线程库 pthreads）或语言级别的多线程库（如 Java 中的 `Thread` 类）来创建线程。线程创建时需要指定线程函数或线程对象，并传递相应的参数。
2. **线程调度：** 操作系统负责线程的调度和管理，根据线程的优先级、状态和调度策略来决定哪个线程可以获得 CPU 时间片执行任务。常见的调度策略包括先来先服务（FCFS）、最短作业优先（SJF）、轮转调度（Round Robin）等。
3. **线程同步：** 多线程程序中，可能会存在共享资源的竞争和冲突，需要通过同步机制来确保多个线程之间的数据访问安全。常见的线程同步机制包括互斥锁（Mutex）、信号量（Semaphore）、条件变量（Condition Variable）等。
4. **线程通信：** 多线程程序中的线程之间可能需要进行通信和协作，以实现共同的任务目标。线程通信机制可以通过共享内存、消息队列、管道等方式来实现。
5. **线程销毁：** 线程执行完成后，需要及时释放线程所占用的资源，以防止资源泄漏和系统资源浪费。操作系统负责线程的销毁和资源回收。

#### 1. 互斥锁（Mutex）：

**概念：** 互斥锁是一种用于保护临界区的同步机制，用于确保在同一时刻只有一个线程可以访问共享资源。当一个线程进入临界区时，它会尝试获得互斥锁，如果互斥锁已被其他线程占用，则该线程会被阻塞，直到互斥锁被释放。

**特点：**

- 互斥锁是一种二进制锁，只有两种状态：锁定和解锁。
- 只有成功获得互斥锁的线程才能进入临界区，其他线程被阻塞。
- 互斥锁是一种独占锁，只能由获得锁的线程释放。

#### 2. 信号量（Semaphore）：

**概念：** 信号量是一种计数器，用于控制同时访问共享资源的线程数量。当一个线程进入临界区时，它会尝试获取信号量，如果信号量的值大于 0，则表示资源可用，线程可以继续执行；如果信号量的值等于 0，则表示资源不可用，线程会被阻塞，直到资源可用。

**特点：**

- 信号量是一种整数型变量，可以有多种取值。
- 信号量可以用于解决生产者-消费者问题、限流等场景。
- 信号量可以是二进制信号量（取值为 0 或 1）或计数信号量（取值大于等于 0）。

#### 3. 条件变量（Condition Variable）：

**概念：** 条件变量是一种线程间的通信机制，用于在多线程环境中实现线程的等待和唤醒操作。条件变量通常与互斥锁配合使用，等待线程在条件不满足时会阻塞，并释放互斥锁；当条件满足时，唤醒线程并重新获取互斥锁，继续执行。

**特点：**

- 条件变量用于解决线程间的同步和通信问题，允许线程在特定条件下等待或唤醒。
- 条件变量通常与互斥锁一起使用，互斥锁用于保护条件变量的访问和修改。



### TCP与UDP有什么区别，各有什么优劣？

TCP（传输控制协议）和UDP（用户数据报协议）是两种不同的传输层协议，用于在计算机网络中传输数据。它们之间的主要差异包括：

1. **连接性**：
   - TCP 是面向连接的协议，它在通信之前需要建立连接，然后在通信结束时释放连接。这种连接方式保证了数据的可靠性和顺序性。
   - UDP 是无连接的协议，通信双方在发送数据之前不需要建立连接，也不需要维护连接状态。因此，UDP 的通信速度通常比 TCP 快，但不具备 TCP 那种可靠性和顺序性。
2. **可靠性**：
   - TCP 提供可靠的数据传输。它通过序号、确认和重传机制来确保数据的可靠性，即使在网络出现丢包或者出现延迟的情况下，也能够保证数据的完整性和顺序性。
   - UDP 不提供可靠的数据传输保证。它仅仅提供了数据的最基本的传输功能，不对数据进行确认和重传，因此在网络不可靠或者对实时性要求较高的情况下使用。
3. **头部开销**：
   - TCP 的头部开销相对较大，因为它需要包含序号、确认号、窗口大小等控制信息，以及可选的选项字段。
   - UDP 的头部开销较小，只包含了源端口、目标端口、长度和校验和等基本信息，因此 UDP 的头部开销比 TCP 小。
4. **适用场景**：
   - TCP 适用于对数据传输可靠性要求较高的场景，如文件传输、网页浏览、电子邮件等。
   - UDP 适用于实时性要求较高，且对数据可靠性要求不高的场景，如音视频流媒体、在线游戏等。

**TCP优点：**

- 可靠性高，适用于对数据传输可靠性要求较高的场景，如文件传输、网页浏览等。

**TCP缺点：**

- 建立连接和断开连接的开销较大，影响了数据传输的实时性。
- 数据传输的实时性较差，不适用于对实时性要求较高的场景，如实时视频传输、游戏等。

**UDP优点：**

- 建立连接和断开连接的开销小，适用于对数据传输实时性要求较高的场景，如实时音频传输、视频会议等。
- 数据传输的实时性好，不受连接建立和断开的影响。

**UDP缺点：**

- 不可靠性高，数据传输可能会丢失或乱序，需要应用层进行数据重传和确认。
- 不提供流量控制机制，容易导致网络拥塞，影响数据传输的稳定性。

#### TCP为什么要经历三次握手

1. **确认双方能够正常收发数据：** 第一次握手中，客户端向服务器发送连接请求报文段，服务器收到后进行确认，并进入 SYN-RCVD 状态；第二次握手中，服务器向客户端发送连接确认报文段，客户端收到后也进入 ESTABLISHED 状态；第三次握手中，客户端向服务器发送确认报文段，服务器收到后也进入 ESTABLISHED 状态。通过这个过程，确保了双方都能够正常收发数据。
2. **防止已失效的连接请求报文段被服务端误认为是新的连接请求：** 如果只进行两次握手，那么在某些情况下，客户端发送的连接请求报文段可能会在网络中被延迟，导致服务端收到后进行确认，但客户端并未收到确认，此时客户端会认为连接未建立成功，并重新发送连接请求。而服务端可能会收到之前已经确认过的连接请求报文段，并错误地认为是新的连接请求。因此，通过第三次握手，可以避免这种情况发生，确保已失效的连接请求报文段不会被误认为是新的连接请求。
3. **确保连接的可靠性：** 三次握手过程中，客户端和服务端都有机会确认对方的身份和能力，并且在第三次握手中，客户端和服务端都对连接进行了确认，从而确保了连接的可靠性和稳定性。



### TCP的四次挥手过程

终止一个已建立的 TCP 连接。这个过程涉及到两端（客户端和服务器端）互相发送控制信息以关闭连接。

1. 第一次挥手：客户端向服务器端发送一个 TCP 报文段，设置 FIN 标志位，表示客户端已经没有数据要发送了，但仍然可以接收数据。客户端进入 FIN_WAIT_1 状态，等待服务器端的确认。
2. 第二次挥手：服务器端收到客户端发送的 FIN 报文段后，会发送一个 ACK 报文段作为确认。此时服务器端进入 CLOSE_WAIT 状态，表示已经收到了客户端的关闭请求，但服务器端还有数据需要发送给客户端。
3. 第三次挥手：当服务器端确定数据都发送完毕后，会向客户端发送一个 FIN 报文段，表示服务器端已经没有数据要发送了。服务器端进入 LAST_ACK 状态。
4. 第四次挥手：客户端收到服务器端发送的 FIN 报文段后，客户端会发送一个 ACK 报文段作为确认。客户端进入 TIME_WAIT 状态，等待可能出现的延迟报文。



### 介绍虚继承

虚继承的主要作用是解决由多重继承导致的菱形继承（Diamond Inheritance）问题。在多重继承中，如果一个类同时继承自两个或多个具有共同基类的类，而这些共同基类又有一个共同的派生类，就会形成菱形继承的结构。这样的继承结构可能会导致问题，例如数据成员在派生类中出现多次，导致冗余或不一致性。**虚继承通过在派生类对共同基类进行虚继承来解决这个问题。使用虚继承后，共同基类在继承体系中只会被派生类保留一份**

```C++
class Base {
public:
    int data;
};

class Derived1 : virtual public Base {
    // Derived1 可以通过 Base 继承的 data
};

class Derived2 : virtual public Base {
    // Derived2 可以通过 Base 继承的 data
};

class FinalDerived : public Derived1, public Derived2 {
    // 可以直接访问 Base 继承的 data，而不会有二义性
};
```

### C++的结构体内存对齐

尽管内存是以字节为单位，但是大部分处理器并不是按字节块来存取内存的.它一般会以双字节,四字节,8字节,16字节甚至32字节为单位来存取内存，我们将上述这些存取单位称为内存存取粒度。32位系统处理器只能从地址为4的倍数的内存开始读取数据。内存对齐的目的是为了提高内存访问的效率，减少内存访问的次数，以及避免因为非对齐内存访问而引起的性能损失。

C++ 中结构体的内存对齐规则如下：

1. 结构体的每个成员变量的起始地址必须是其自身大小的整数倍。
2. 结构体的大小必须是其最大成员大小的整数倍。

### Linux常用命令

`ls`: 列出当前目录的内容。

`ls -l`: 以长格式列出当前目录的内容，包括文件权限、所有者、大小等信息。

`ls -a`: 列出当前目录的所有内容，包括隐藏文件。

`pwd`: 显示当前工作目录的完整路径。

`rm -f`: 强制删除，不提示确认。

`rm -r`: 递归删除目录及其内容。

`rmdir`: 删除一个空目录。

`cp -r`: 递归复制目录及其内容。

`cat`: 查看文件内容。

`more` 或 `less`: 分页查看文件内容，可以滚动查看大文件。

`nano` 或 `vi` 或 `vim`: 编辑文件的内容。

`chmod`: 改变文件或目录的权限。

```Ba
-R：递归地修改目录及其下属文件和子目录的权限。
符号：+ 表示添加权限，- 表示删除权限，= 表示设置权限。
权限：r 表示读权限，w 表示写权限，x 表示执行权限。
用户类型：u 表示所有者，g 表示所属组，o 表示其他用户，a 表示所有用户。
chmod u+x file.txt：给文件的所有者添加执行权限。
chmod a=rwx directory：设置目录的所有用户都具有读、写、执行权限。
```

`find`: 在指定目录中搜索文件。

```bash
find 路径 [选项] [表达式]
-name 模式：按照文件名模式进行搜索，模式可以使用通配符。
-type 类型：指定要搜索的文件类型，如 f 表示普通文件，d 表示目录。
-size 大小：按照文件大小进行搜索，可以使用 + 表示大于，- 表示小于，以及 c 表示字节、k 表示千字节、M 表示兆字节等单位。
-mtime +n/-n：按照修改时间进行搜索，+n 表示修改时间在 n 天之前，-n 表示修改时间在 n 天之内。
find /path/to/search -type d -mtime -7 -exec ls -ld {} \;：搜索指定目录下最近 7 天内修改过的目录，并输出详细信息。
```

### 深度优先遍历和广度优先遍历

### 广度优先搜索（BFS）：

- **原理**：BFS从图中的某个起始节点开始，首先遍历起始节点的所有邻居节点，然后再依次遍历邻居节点的邻居节点，以此类推，直到遍历完整个图为止。它按层级的顺序逐层遍历，先访问距离起始节点最近的节点。
- **数据结构**：通常使用队列（Queue）来实现BFS，起始节点入队，然后出队时将其邻居节点入队，依次进行。
- **应用**：BFS常用于寻找最短路径、查找图的连通性、生成迷宫等。

### 深度优先搜索（DFS）：

- **原理**：DFS从图中的某个起始节点开始，首先遍历起始节点的一个邻居节点，然后再以同样的方式遍历该邻居节点的一个邻居节点，依次进行直到到达最深的节点，然后再回溯到前面的节点，以同样的方式遍历其他未被访问过的邻居节点，直到所有节点都被访问过。
- **数据结构**：通常使用递归或者栈（Stack）来实现DFS。递归方式下，函数自己调用自己，栈方式下，手动维护一个栈来保存节点。
- **应用**：DFS常用于图的遍历、拓扑排序、寻找连通分量、解决迷宫问题等。

### 区别：

1. **访问顺序**：BFS按层级顺序遍历，先访问离起始节点最近的节点；DFS先遍历当前节点的一个分支，直到末端再回溯到前面的节点继续搜索其他分支。
2. **数据结构**：BFS通常使用队列，而DFS通常使用递归或者栈。
3. **空间复杂度**：BFS在存储未访问节点时需要较大的空间，因为它需要将所有的当前层节点都存储起来，而DFS则不需要这样做，因此在空间上DFS比BFS更节省。
4. **时间复杂度**：在相同的条件下，BFS和DFS的时间复杂度一样，都是 O*(*V*+*E)，其中V是节点数，E 是边数。
5. **应用场景**：具体应用场景不同。BFS适用于解决最短路径等问题，而DFS适用于拓扑排序、连通分量等问题。



<h1 style="text-align:center;">手撕算法
</h1>

### 背包问题总结

#### 背包递推公式：

1. 问是否能装满背包，或者最多能装多少：

```C++
dp[j]=max(dp[j], dp[j-nums[i]]+nums[i]);
```

2. 问装满背包有几种方法：

```C++
dp[0] = 1;
dp[j] += dp[j-nums[i]];
```

3. 问装满背包所有物品的最小个数：

```C++
vector<int> dp(size+1, INT_MAX);
dp[0] = 0;
dp[j] = min(dp[j-coins[i]]+1, dp[j]);
```

#### 遍历顺序

1. 01背包，物品只有一个，选或者不选

先遍历物品再遍历背包，且第二层for是从大到小遍历

2. 完全背包，物品有无数个，不选或者选几个

完全背包第二层for循环从小到大遍历

**求组合数：**外层for遍历物品，内层for遍历背包

**求排列数：**外层for遍历背包，内层for遍历物品

**求最小数：**两层for循环的先后顺序无所谓



### 链表反转

```C++
// 给定单链表的头节点 head ，请反转链表，并返回反转后的链表的头节点。
class Solution {
public:
    ListNode* reverseList(ListNode* head) {
        ListNode* pre = nullptr;
        ListNode* cur = head;
        while(cur!=nullptr) {
            ListNode* tmp = cur->next;
            cur->next = pre;
            pre = cur;
            cur = tmp;
        }
        return pre;
    }
};
```

### 如何寻找二次曲线的最小值

```py
# y=a*x^2+b*x+c
def gradient(x, a, b):
    return 2*a*x+b
def gradient_descent(lr, iterations, x, a, b):
    for _ in range(iterations):
        grad = gradient(x, a, b)
        x -= lr*grad
    return x
```

### 用两个栈实现队列

```C++
class MyQueue {
public:
    stack<int> in_st;
    stack<int> out_st;
    MyQueue() {

    }
    
    void push(int x) {
        while (!out_st.empty()) {
            in_st.push(out_st.top());
            out_st.pop();
        }
        in_st.push(x);
    }
    
    int pop() {
        if (empty()) return -1;
        int val = peek();
        in_st.pop();
        return val;
    }
    
    int peek() {
        if (empty()) return -1;
        if (in_st.size() > 0) {
            while(in_st.size()>1) {
            out_st.push(in_st.top());
            in_st.pop();
            }
        }
        else {
            in_st.push(out_st.top());
            out_st.pop();
        }
        return in_st.top();
    }
    
    bool empty() {
        return in_st.empty()&&out_st.empty();
    }
};
```



### 判断链表中是否有环

```C++
class Solution {
public:
    ListNode *detectCycle(ListNode *head) {
        ListNode* slow = head, *fast = head;
        while(fast!=nullptr) {
            if (fast->next==nullptr) return nullptr;
            fast = fast->next->next;
            slow = slow->next;
            if (fast==slow) {
                ListNode* slow = head;
                while(slow!=fast) {
                    slow=slow->next;
                    fast=fast->next;
                }
                return slow;
            }
        }
        return nullptr;
    }
};
```



### 合并两个有序数组

```C++
// 给你两个按 非递减顺序 排列的整数数组 nums1 和 nums2，另有两个整数 m 和 n ，分别表示 nums1 和 nums2 中的元素数目。
// 请你 合并 nums2 到 nums1 中，使合并后的数组同样按非递减顺序排列。
// 直接合并后排序
class Solution {
public:
    void merge(vector<int>& nums1, int m, vector<int>& nums2, int n) {
        for (int i = 0; i != n; ++i) {
            nums1[m + i] = nums2[i];
        }
        sort(nums1.begin(), nums1.end());
    }
};
// 双指针
class Solution {
public:
    void merge(vector<int>& nums1, int m, vector<int>& nums2, int n) {
        int p1 = 0, p2 = 0;
        int sorted[m + n];
        int cur;
        while (p1 < m || p2 < n) {
            if (p1 == m) {
                cur = nums2[p2++];
            } else if (p2 == n) {
                cur = nums1[p1++];
            } else if (nums1[p1] < nums2[p2]) {
                cur = nums1[p1++];
            } else {
                cur = nums2[p2++];
            }
            sorted[p1 + p2 - 1] = cur;
        }
        for (int i = 0; i != m + n; ++i) {
            nums1[i] = sorted[i];
        }
    }
};
```

### 链表中的节点每k个组翻转

```C++
class Solution {
public:
    ListNode* reverseKGroup(ListNode* head, int k) {
        stack<ListNode*> st;
        ListNode* dummpyHead = new ListNode(0);
        ListNode* cur = head, * pre = dummpyHead;
        while (cur!=nullptr) {
            if (st.size()==k) {
                while(!st.empty()) {
                    pre->next = st.top();
                    st.pop();
                    pre = pre->next;
                }
            }
            else {
                st.push(cur);
                cur=cur->next;
            }
        }
        if (st.size()==k) {
            while(!st.empty()) {
                pre->next = st.top();
                st.pop();
                pre = pre->next;
                cout << pre->val;
            }
            pre->next = nullptr;
        }
        else {
            while(!st.empty()) {
                pre->next = st.top();
                st.pop();
            }
        }
        return dummpyHead->next;
    }
};
```



### 超级丑数



### 查找和最小的k对数字



### 有序矩阵中的第k个最小数组和









<h1 style="text-align:center;">AI算法题
</h1>

### 常见的注意力机制

1. SE模块

```python
class SELayer(nn.Module):
    def __init__(self, channel, reduction=16):
        super(SELayer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=False),
            nn.Sigmoid()
        )
 
    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)
```



![img](https://img-blog.csdnimg.cn/e68deb8b324841d09aaced4ce9cd3a8e.png)

2. CA模块

```python
import torch
from torch import nn
class CA_Block(nn.Module):
    def __init__(self, channel, h, w, reduction=16):
        super(CA_Block, self).__init__()
        self.h = h
        self.w = w
        self.avg_pool_x = nn.AdaptiveAvgPool2d((h, 1))
        self.avg_pool_y = nn.AdaptiveAvgPool2d((1, w))
        self.conv_1x1 = nn.Conv2d(in_channels=channel, out_channels=channel//reduction, kernel_size=1, stride=1, bias=False)
        self.relu = nn.ReLU()
        self.bn = nn.BatchNorm2d(channel//reduction)
        self.F_h = nn.Conv2d(in_channels=channel//reduction, out_channels=channel, kernel_size=1, stride=1, bias=False)
        self.F_w = nn.Conv2d(in_channels=channel//reduction, out_channels=channel, kernel_size=1, stride=1, bias=False)
        self.sigmoid_h = nn.Sigmoid()
        self.sigmoid_w = nn.Sigmoid()
    def forward(self, x):
        x_h = self.avg_pool_x(x).permute(0, 1, 3, 2)
        x_w = self.avg_pool_y(x)
        x_cat_conv_relu = self.relu(self.conv_1x1(torch.cat((x_h, x_w), 3)))
        x_cat_conv_split_h, x_cat_conv_split_w = x_cat_conv_relu.split([self.h, self.w], 3)
        s_h = self.sigmoid_h(self.F_h(x_cat_conv_split_h.permute(0, 1, 3, 2)))
        s_w = self.sigmoid_w(self.F_w(x_cat_conv_split_w))
        out = x * s_h.expand_as(x) * s_w.expand_as(x)
        return out
```

![img](https://img-blog.csdnimg.cn/232b3dc1ed854fcfb97607d2f71ad7cd.png)

3. 自注意力机制：用于处理序列数据，如文本、语音等。它能够根据序列中各个元素之间的相互关系动态地计算每个元素的权重。

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert embed_dim % num_heads == 0, "Embedding dimension must be divisible by the number of heads."
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # 初始化线性变换矩阵
        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)

        # 输出线性变换矩阵
        self.W_o = nn.Linear(embed_dim, embed_dim)

    def forward(self, query, key, value, mask=None):
        batch_size = query.shape[0]

        # 线性变换
        Q = self.W_q(query)  # 形状: (batch_size, seq_len, embed_dim)
        K = self.W_k(key)    # 形状: (batch_size, seq_len, embed_dim)
        V = self.W_v(value)  # 形状: (batch_size, seq_len, embed_dim)

        # 重塑为多头
        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # 形状: (batch_size, num_heads, seq_len, head_dim)
        K = K.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # 形状: (batch_size, num_heads, seq_len, head_dim)
        V = V.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # 形状: (batch_size, num_heads, seq_len, head_dim)

        # 计算注意力分数
        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))
        if mask is not None:
            scores.masked_fill_(mask == 0, float('-inf'))

        # 使用 softmax 激活函数计算注意力权重
        attn_probs = F.softmax(scores, dim=-1)  # 形状: (batch_size, num_heads, seq_len, seq_len)

        # 使用注意力权重对值进行加权求和
        attn_output = torch.matmul(attn_probs, V)  # 形状: (batch_size, num_heads, seq_len, head_dim)

        # 拼接多头并投影回原始嵌入维度
        attn_output = attn_output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.embed_dim)  # 形状: (batch_size, seq_len, embed_dim)

        # 使用线性变换得到最终的注意力输出
        attn_output = self.W_o(attn_output)  # 形状: (batch_size, seq_len, embed_dim)

        return attn_output
```



3. 交叉注意力机制：用于处理具有不同输入的模型，如图像与文本之间的关系。它允许模型在处理不同输入时动态地计算各个输入之间的关联性，从而更好地整合不同输入的信息。

### TensorRT优化流程

1. **网络定义和训练：** 首先，你需要定义一个深度学习模型并进行训练，通常使用常见的深度学习框架如TensorFlow、PyTorch或Caffe等。这个模型可以是用于分类、目标检测、语义分割等各种任务的模型。
2. **模型转换：** 接下来，你需要将训练好的模型转换为TensorRT的可优化格式。TensorRT支持从常见的深度学习框架（如TensorFlow、PyTorch等）导入模型，并将其转换为TensorRT的网络结构格式。这个过程通常被称为模型优化。
3. **网络优化：** 一旦模型被转换为TensorRT的格式，TensorRT可以对网络进行各种优化，以提高推理性能。这些优化包括结构优化、精度混合、内存优化、层融合、图剪枝等。
4. **引擎构建：** 在网络优化完成后，你需要使用TensorRT API构建推理引擎。推理引擎是TensorRT中的一个重要概念，它是一个针对特定硬件配置和推理需求进行了优化的二进制文件。构建引擎的过程会将优化后的网络结构映射到特定的硬件上，以实现最佳的推理性能。
5. **推理过程：** 最后，你可以使用TensorRT推理引擎对输入数据进行推理。推理引擎会利用之前优化的网络结构和硬件特性，高效地执行推理操作，并生成模型对输入数据的预测结果。

### 介绍YOLO v3~YOLO v8



### 为什么UNet在医学图像上表现这么好

1. 医学图像语义较为简单、结构较为固定。所以高级语义信息和低级特征都显得很重要(UNet的skip connection和[U型结构](https://www.zhihu.com/search?q=U型结构&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A586501606})就派上了用场)。
2. .数据量少。医学影像的数据获取相对难一些，很多比赛只提供不到100例数据。所以我们设计的模型不宜多大，参数过多，很容易导致[过拟合](https://www.zhihu.com/search?q=过拟合&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A586501606})。

### 训练网络时怎么判断有没有过拟合，欠拟合

#### 过拟合：

1. **训练集和验证集损失对比**：监控训练集和验证集的损失。如果训练集的损失持续下降，而验证集的损失却开始上升，则可能出现了过拟合。
2. **训练集和验证集性能对比**：除了损失外，还可以比较训练集和验证集的性能指标，如准确率、精确率、召回率等。如果训练集的性能指标较高，而验证集的性能指标较低，则可能存在过拟合。
3. **可视化训练过程**：可视化训练过程中的损失和性能曲线，观察它们的趋势变化。如果训练集和验证集的曲线出现了分歧，可能出现了过拟合。

#### 欠拟合：

1. **监控训练集和验证集的损失**：如果训练集和验证集的损失都很高，说明模型可能存在欠拟合。这意味着模型无法捕捉数据中的足够信息。
2. **观察模型性能**：监控模型在训练集和验证集上的性能指标。如果性能指标较低，可能是因为模型过于简单，无法很好地拟合数据。
3. **增加模型容量**：如果发现模型出现了欠拟合，可以尝试增加模型的容量，例如增加网络的层数、神经元数量等。
4. **尝试其他模型**：如果增加模型容量仍然无法解决欠拟合问题，可以尝试使用其他更复杂的模型，或者调整模型的架构以更好地适应数据。

### 密集行人检测的遮挡问题怎么解决

1. 损失函数Repulsion loss：
   $$
   L_{Rep}=L_{Attr}+\alpha*L_{RepGT}+\beta*L_{RepBox}
   $$
   其中 $$L_{Attr} $$ 是吸引项，需要预测框靠近其指定目标；$L_RepGT$ 和 $L_RepBox$ 是排斥项，分别需要预测框远离周遭其他的 groundtruth 物体和其他指定目标不同的预测框。



### 自动驾驶检测模型如何针对corner case优化



### 常见的损失函数

1. 均方误差：适用于回归任务，用于衡量预测值与真实值之间的平均差异。

   优点：在训练过程中，优化过程较为简单，且可微分，有利于梯度下降。

   缺点：对异常值敏感，可能导致模型过度关注异常值。

2. 交叉熵损失：交叉熵损失在分类任务中表现良好，它将概率分布的差异转化为损失值，使得模型更加关注于正确类别的预测。

   优点：对于分类任务，尤其是二分类或多分类任务，交叉熵损失通常是首选的损失函数。

   缺点：不适用于回归任务，而且可能出现梯度消失问题。

3. 平均绝对误差：MAE 对异常值相对较为鲁棒，因为它是误差的绝对值的平均值。

   优点：MAE 对异常值不那么敏感，能够更好地反映数据的实际分布情况。

   缺点：不易优化，因为它在零点附近不是光滑可微的。

4. Hinge损失：Hinge损失适用于支持向量机等分类器，对于大间隔分类有较好的性能。

   优点：对于分类任务，能够产生稀疏的解，适用于处理大规模数据。

   缺点：不适用于概率输出的模型，不能直接用于多类别分类问题。

5. Dice损失：主要用于图像分割任务，能够衡量预测分割与真实分割之间的重叠程度。

   优点：适用于处理像素级别的分割任务，尤其在医学图像分割领域有较好的表现。

   缺点：可能无法直接推广到其他类型的任务。

### 神经网络优化器有哪些

1. **随机梯度下降 (SGD)：** 是最基本的优化算法之一，每次迭代随机选择一批样本进行梯度计算和参数更新。虽然简单，但在实践中可能会受到局部极小值、学习率调整等问题的影响。
2. **动量优化器（Momentum）：** 通过引入动量项来加速收敛过程，可以在参数更新时考虑历史梯度的加权和。常见的动量优化器包括标准动量（SGD with Momentum）、Nesterov 动量等。
3. **AdaGrad：** 自适应学习率的方法之一，通过对每个参数的学习率进行适应性调整，使得稀疏梯度对应的学习率较大，而频繁出现的梯度对应的学习率较小。
4. **RMSProp：** 根据梯度的历史信息来调整学习率，对 AdaGrad 进行了改进，通过指数加权移动平均的方式对历史梯度进行衰减。
5. **Adam：** 结合了动量优化器和 RMSProp 的优点，在计算动量和学习率时考虑了梯度的一阶矩估计（均值）和二阶矩估计（方差），并使用偏差修正来提高稳定性。

### GBDT和Adaboost的区别



### 神经网络的初始化方法

1. **零初始化（Zero Initialization）：** 将所有权重参数和偏置参数初始化为零。这种方法简单易行，但可能会导致网络对称性问题，并且不适用于深层网络。

2. **随机初始化（Random Initialization）：** 将权重参数和偏置参数初始化为随机的小值，通常服从某种均匀分布或高斯分布。常见的方法包括使用均匀分布 $[-\epsilon, \epsilon]$ 或者高斯分布 $N(0, \sigma^2)$，其中 $\epsilon$ 和 $\sigma$ 是根据网络结构和层数进行调整的超参数。

3. **Xavier初始化（Xavier Initialization）：** 也称为Glorot初始化，它根据每一层的输入和输出神经元数量来自适应地初始化权重参数。通常，Xavier初始化使用均匀分布，其方差计算如下：

   - 对于sigmoid激活函数：$Var(W) = \frac{2}{n_{in} + n_{out}}$
   - 对于tanh激活函数：$Var(W) = \frac{1}{n_{in} + n_{out}}$
   - 对于ReLU激活函数：$Var(W) = \frac{2}{n_{in}}$

   其中 $n_{in}$ 和 $n_{out}$ 分别是当前层的输入和输出神经元数量。

4. **He初始化（He Initialization）：** 与Xavier初始化类似，但是适用于ReLU激活函数。He初始化使用均匀分布，其方差计算如下：

   - 对于ReLU激活函数：$Var(W) = \frac{2}{n_{in}}$

   其中 $n_{in}$ 是当前层的输入神经元数量。

7. **预训练初始化（Pretrained Initialization）：** 在使用预训练模型进行微调时，可以使用已经训练好的模型的参数进行初始化，这种方法在迁移学习中非常常见。

### DETR中匈牙利匹配算法的具体流程

DETR不需要NMS，采用的是集合预测损失。在DETR中固定会输出N个预测框，如何将预测框与GT对应起来？

二分匹配问题：找到一组边集合，这组边集合没有公共的顶点。

例子：N个工人，M个任务，每个工人对应不同任务的价钱不同，如何以最小的代价完成任务。

![image-20240319161646416](C:\Users\scutbci\AppData\Roaming\Typora\typora-user-images\image-20240319161646416.png)

第一步，构建匹配代价$L_{match}$来进行匈牙利算法的最优分配，其中考虑了Ground truth与预测框之间的类别预测以及距离的代价；第二步，对前一步中匹配的所有配对的匈牙利损失，这个损失函数为类预测的负对数似然和边界框损失的线性组合。

### 为什么Transformer要使用LayerNorm

归一化的公式：
$$
Norm(x)=\gamma\frac{x-\mu}{\sqrt{\sigma^2+\epsilon}}+\beta
$$
其中，$\gamma$和$\beta$是可学习的缩放和平移参数。

BatchNorm中：
$$
\mu=\frac{1}{m}\sum_{i=1}^mx^i
$$

$$
\sigma^2=\frac{1}{m}\sum_{i=1}^m(x^i-\mu)^2
$$

其中，m是批量样本的大小，$x^i$是第$i$个样本的输出；把一个batch中同一个通道的所有特征是为一个分布（有几个通道就有几个分布），并将其标准化，这意味着：不同图片的的同一通道的相对关系是保留的，即不同图片的同一通达的特征是可以比较的；同一图片的不同通道的特征则是失去了可比性；

LayerNorm中：
$$
\mu=\frac{1}{n}\sum_{i=1}^nx_i
$$

$$
\sigma^2=\frac{1}{n}\sum_{i=1}^n(x_i-\mu)^2
$$

其中，$n$ 是特征的维度，$x_i$是第$i$ 个特征的值。把一个句子的所有词向量视为一个分布（有几个句子就有几个分布），并将其归一化。这意味着：同一个句子中的词向量的相对大小是保留的；不同句子的词向量失去了可比性。



任何norm的意义都是为了让使用norm的网络的输入的数据分布变得更好，也就是转换为标准正态分布，数值进入敏感度区间，以减缓梯度消失，从而更容易训练。当然，这也意味着舍弃了除此维度之外其他维度的其他信息。首先要明确，如果在一个维度内进行normalization，那么在这个维度内，相对大小有意义的，是可以比较的；但是在normalization后的不同的维度之间，相对大小这是没有意义的。

相比于稳定前向输入分布，反向传播时mean和variance计算引入的梯度更有用，可以稳定反向传播时loss对输入的梯度。LN特别适合处理变长数据，因为是对channel维度做操作(这里指NLP中的hidden维度)，和句子长度和batch大小无关。

自己的理解：不同句子之间的词向量不需要有可比较的关系；



### 为什么self-attention可以堆叠多层，有什么作用

Self-attention（自注意力）能够捕捉输入序列中的长距离依赖关系，通过堆叠多层self-attention，模型可以学习序列中更深层次的模式和依赖关系。多层self-attention就像神经网络中的多个隐藏层一样，使模型能够学习和表示更复杂的函数。

### 介绍KMeans

聚类属于非监督学习，K均值聚类是最基础常用的聚类算法。它的基本思想是，通过迭代寻找K个簇（Cluster）的一种划分方案，使得聚类结果对应的损失函数最小。其中，损失函数可以定义为各个样本距离所属簇中心点的误差平方和：
$$
J(c,u)=\sum_1^n||x_i-\mu_{c_i}||^2
$$
其中$x_i$代表第i个样本，$c_i$是$x_i$所属的簇，$u_{c_i}$是对应簇的中心点。

具体步骤：1.数据预处理，标准化，过滤异常点；2.随机选取K个中心；3。定义损失函数；4.对每个样本，将其分配到距离最近的中心$c_i^t<-argmin_k||x_i-u_k^t||^2$，重新计算类中心$u_k^{t+1}<-argmin_u\sum_{i_i^t=k}^b||x_i-\mu||^2$

优点：1. 高效可伸缩，计算复杂度 为$O(NKt)$接近于线性（N是数据量，K是聚类总数，t是迭代轮数）；2.收敛速度快，原理相对通俗易懂，可解释性强。

缺点：1. 受初始值和异常点影响，聚类结果可能不是全局最优而是局部最优。2. K是超参数，一般需要按经验选择；3. 样本点只能划分到单一的类中

### EM算法

EM（Expectation-Maximum）算法即期望最大化算法，是最常见的隐变量估计方法。EM算法是一种迭代优化策略，每一次迭代都分为两步：期望步（E）、极大步（M）。**EM算法的提出最初是为了解决数据缺失情况下的参数估计问题**，基本思想是首先根据已有的观测数据，通过极大似然估计估计出模型的参数；再根据上一步估计出的参数值估计缺失数据的值；最后根据估计出的缺失数据和原有的观测数据重新对参数值进行估计，反复迭代直到收敛。

**结论：EM算法可以保证收敛到一个稳定点，即EM算法是一定收敛的。**

### 介绍NMS及其变体

1. Soft-NMS：通过降低重叠边界框的置信度来保留更多信息，将其得分进行惩罚衰减，



### 多卡的BN如何实现同步

1. **跨GPU批量归一化（Cross-GPU Batch Normalization）：** 在这种方法中，每个GPU都有自己的批量归一化层。在每个mini-batch的处理过程中，每个GPU分别计算出均值和方差，并进行归一化处理。然后，通过在所有GPU上汇总并取平均，来计算整个mini-batch的全局均值和方差。最后，所有GPU上的数据都使用这些全局均值和方差进行归一化。这种方法在训练时可以实现较好的批量归一化效果，但需要在推理时仔细处理，因为此时可能不再有多个GPU可用。
2. **同步批量归一化（Synchronized Batch Normalization）：** 这种方法在训练时和推理时都可以有效地实现批量归一化。在同步批量归一化中，所有GPU上的数据都收集到一个单一的批量上。然后，通过在所有GPU上计算均值和方差，并进行同步操作来得到全局均值和方差。最后，所有GPU上的数据都使用这些全局均值和方差进行归一化。这种方法保证了所有GPU上的数据都使用相同的均值和方差进行归一化，从而确保了批量归一化的一致性。

### 如何检测到未知目标

**开放集识别：**模型应该拒绝未知类，而不是以高置信度将其辨认为已知类。

1. OpenMax:
2. 模型集成
3. 置信度估计



### 图像边缘检测算子有哪些？

1. Sobel算子：一种基于一阶梯度的边缘检测算子，用于寻找图像中的水平和垂直边缘。Sobel算子包含两个卷积核：一个用于计算水平方向的梯度，另一个用于计算垂直方向的梯度。

$$
G_x=\begin{matrix}
-1&0&1\\
-2&0&2\\
-1&0&1\\
\end{matrix}
$$

$$
G_y=\begin{matrix}
-1&-2&-1\\
0&0&0\\
1&2&1\\
\end{matrix}
$$



2. Prewitt算子：类似于Sobel算子，也是一种基于一阶梯度的边缘检测算子，用于寻找图像中的水平和垂直边缘。

$$
G_x=\begin{matrix}
-1&0&1\\
-1&0&1\\
-1&0&1\\
\end{matrix}
$$

$$
G_y=\begin{matrix}
1&1&1\\
0&0&0\\
1&1&1\\
\end{matrix}
$$



3. Laplacian算子：
   $$
   f'(x,y)=-4f(x,y)+f(x-1,y)+f(x+1,y)+f(x,y-1)+f(x,y+1)
   $$
   

### Transformer与CNN的优缺点

**Transformer 的优点：**

1. **适用于序列数据：** Transformer 主要应用于处理序列数据，如自然语言处理（NLP）任务中的文本序列。由于其自注意力机制的引入，Transformer 能够捕获序列中任意两个位置之间的依赖关系，从而更好地建模长距离依赖。
2. **并行计算：** Transformer 的自注意力机制允许每个位置的输入直接和所有其他位置的输入进行交互，使得模型在处理长序列时具有较好的并行性，能够高效地利用硬件资源。
3. **位置编码：** Transformer 使用位置编码来表示输入序列中的位置信息，这使得模型能够区分不同位置的词或符号，有助于模型理解序列的顺序信息。
4. **可解释性：** Transformer 模型的自注意力机制使得模型在生成预测时能够关注输入序列中的不同部分，从而具有一定的可解释性，可以分析模型在做出决策时的注意力分布情况。

**Transformer 的缺点：**

1. **计算复杂度高：** Transformer 模型通常需要较大的参数量，且在处理长序列时，由于自注意力机制的全连接特性，计算复杂度较高，可能导致训练和推理速度较慢。
2. **数据量要求高：** Transformer 模型通常需要大量的数据来进行训练，特别是在没有预训练模型的情况下，需要更多的数据来学习到有效的表示。

**CNN 的优点：**

1. **适用于图像数据：** CNN 主要应用于处理图像数据，具有良好的特征提取能力，能够捕获图像中的局部特征和层次结构。
2. **参数共享和稀疏连接：** CNN 中的卷积层具有参数共享和稀疏连接的特性，这使得模型具有较少的参数量和更高的参数效率，适合处理大规模图像数据。
3. **平移不变性：** CNN 中的卷积操作具有平移不变性，即对图像的平移操作具有一定的鲁棒性，能够在一定程度上保持对图像中相同特征的识别能力。

**CNN 的缺点：**

1. **局限于固定大小的输入：** CNN 在处理图像时通常需要固定大小的输入，因此对于尺寸不一致的图像数据需要进行预处理或者裁剪，可能会丢失一部分信息。
2. **不适用于序列数据：** CNN 不适用于处理序列数据，因为其卷积操作是基于固定大小的局部感受野进行的，无法捕获序列中不同位置之间的依赖关系。



### 介绍SVM和逻辑回归

#### SVM

支持向量机（Support Vector Machine，SVM）是一种经典的监督学习算法，主要用于分类和回归分析。SVM 的目标是找到一个最优的超平面，使得正负样本中距离超平面最近的数据点到超平面的距离（即支持向量）尽可能远，这个距离称为间隔（margin）。线性可分的情况并不总是存在，因此引入软间隔分类器。软间隔分类器允许一些数据点不满足线性可分的条件，但是仍然试图最大化间隔，并引入了惩罚项来限制分类错误的数量。SVM 可以通过核技巧处理非线性可分的情况。核技巧允许在高维空间中计算点之间的内积，而不需要显式地计算高维空间的坐标。这使得 SVM 在处理非线性问题时非常有效，常用的核函数包括线性核、多项式核、高斯核等。SVM 的目标是求解一个凸优化问题，通过最小化损失函数来找到最优的超平面参数。常见的损失函数包括合页损失函数（hinge loss）。

#### 逻辑回归

逻辑回归使用一个称为“逻辑函数”或“Sigmoid函数”的函数作为假设函数，Sigmoid函数的公式如下：
$$
h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}
$$
其中$h_\theta(x)$是预测概率，$x$是特征向量，$\theta$是模型参数向量。

对数损失函数：$J(\theta)=-[ylog(h_\theta(x))+(1-y)log(1-h_\theta(x))]$

逻辑回归是一种简单而有效的分类算法，特别适用于二分类问题，并且具有良好的解释性。

#### 二者的区别：

1. 目标函数：SVM的目标是找到一个最大间隔超平面，即使得两个类别的样本距离超平面的间隔最大化。其损失函数基于间隔和支持向量的概念。逻辑回归的目标是最小化对数损失函数，使得模型的预测概率与实际标签尽可能一致。
2. 输出：SVM是一种判别模型，其输出是一个样本属于某个类别的决策边界。逻辑回归输出的是样本属于某个类别的概率。
3. 鲁棒性:SVM对异常值相对较为敏感，因为它的目标是最大化间隔，异常值可能会对决策边界产生较大影响。逻辑回归对异常值的影响相对较小。
4. 计算复杂度：逻辑回归的计算复杂度通常较低，因为它的损失函数是凸函数，可以直接使用梯度下降等优化算法。SVM在大规模数据集上的计算复杂度较高，尤其是在使用核技巧时。
5. 模型可解释性：逻辑回归提供了直接的系数解释，可以解释每个特征对结果的影响。SVM在高维空间中操作，因此其模型的解释性通常较差。

### Canny算子的流程

1. 高斯滤波：滤波的主要目的是降噪，一般的图像处理算法都需要先进行降噪。

2. 计算梯度值和梯度方向：通过点乘一个sobel或其他算子得到不同方向的梯度值$g_x(m,n)$,$g_y(m,n)$。综合梯度通过以下公式计算梯度值和梯度方向：
   $$
   G(m,n)=\sqrt{g_x(m,n)^2+g_y(m,n)^2}
   $$

   $$
   \theta=arctan \frac{g_y(m,n)}{g_x(m,n)}
   $$

3. 非极大值抑制：使边缘的宽度尽可能为1个像素点，如果一个像素点属于边缘， 那么这个像素点在梯度方向上的梯度值是最大的，否则不是边缘，将灰度值设置为0.

4. 使用上下阈值来检测边缘：设置两个threshold，分别为maxVal和minVal。其中大于maxVal的都被检测为边缘，而低于minval的都被检测为非边缘。对于中间的像素点，如果与确定为边缘的像素点邻接，则判定为边缘；否则为非边缘。

### NMS的缺点及其改进工作

```python
# 很多检测框都是检测同一个目标，但最终每个目标只需要一个检测框，NMS选择那个得分最高的检测框
import numpy as np

def calculate_iou(box1, box2):
    """
    计算两个边界框的交并比（IoU）
    """
    # 提取边界框的坐标
    x1_tl, y1_tl, x1_br, y1_br = box1
    x2_tl, y2_tl, x2_br, y2_br = box2

    # 计算交集的坐标
    x_intersection = max(0, min(x1_br, x2_br) - max(x1_tl, x2_tl))
    y_intersection = max(0, min(y1_br, y2_br) - max(y1_tl, y2_tl))
    intersection = x_intersection * y_intersection

    # 计算并集的面积
    area_box1 = (x1_br - x1_tl) * (y1_br - y1_tl)
    area_box2 = (x2_br - x2_tl) * (y2_br - y2_tl)
    union = area_box1 + area_box2 - intersection

    # 计算交并比
    iou = intersection / union
    return iou

def non_max_suppression(boxes, scores, threshold):
    """
    非极大值抑制算法
    """
    # 按照置信度排序
    sorted_indices = np.argsort(scores)[::-1]
    selected_indices = []

    while len(sorted_indices) > 0:
        # 选择置信度最高的边界框
        best_index = sorted_indices[0]
        selected_indices.append(best_index)

        # 计算当前边界框与其余边界框的IoU
        ious = [calculate_iou(boxes[best_index], boxes[idx]) for idx in sorted_indices[1:]]

        # 移除与当前边界框IoU大于阈值的边界框
        remove_indices = np.where(np.array(ious) > threshold)[0] + 1
        sorted_indices = np.delete(sorted_indices, remove_indices)

    return selected_indices
```

##### 缺点：

1. 参数敏感：NMS的性能受到参数的影响，不同的数据集和场景可能需要不同的参数设置。
2. 计算开销： 在大规模数据集上运行NMS可能会消耗大量的计算资源
3. 抑制不完全：NMS可能会错误地移除真实目标边界框，特别是当目标之间有部分遮挡或重叠时。
4. 处理不规则目标困难：对于不规则形状或大小不一的目标，NMS的效果可能不理想。
5. 将相邻检测框的分数均强制归零

改进方法：

1. Soft-NMS：通过降低重叠边界框的置信度来保留更多信息，将其得分进行惩罚衰减

### 怎么判断连通域是否相邻

```python
# 检查他们是否共享边界或者像素
def are_connected(region1, region2):
    # 检查region1中的每个像素是否与region2中的像素相邻
    for pixel in region1:
        # 检查上下左右四个方向
        for dx, dy in [(0, 1), (0, -1), (1, 0), (-1, 0)]:
            neighbor = (pixel[0] + dx, pixel[1] + dy)
            # 如果邻居在region2中，则region1与region2相邻
            if neighbor in region2:
                return True
    return False
```

### 手写卷积

```python
# 滑动窗口
import numpy as np

def conv2d(image, kernel, padding=(0, 0), stride=(1, 1)):
    # 图像大小
    H, W, C = image.shape
    # 卷积核大小
    K_h, K_w, C = kernel.shape
    # 填充大小
    pad_h, pad_w = padding
    # 步长
    stride_h, stride_w = stride

    # 计算输出图像大小
    out_h = (H + 2 * pad_h - K_h) // stride_h + 1
    out_w = (W + 2 * pad_w - K_w) // stride_w + 1

    # 填充输入图像
    padded_image = np.pad(image, ((pad_h, pad_h), (pad_w, pad_w), (0, 0)), mode='constant')

    # 初始化输出图像
    output = np.zeros((out_h, out_w))

    # 进行卷积运算
    for y in range(0, out_h):
        for x in range(0, out_w):
            # 定义卷积区域
            region = padded_image[y * stride_h:y * stride_h + K_h, x * stride_w:x * stride_w + K_w, :]
            # 计算卷积结果
            output[y, x] = np.sum(region * kernel)

    return output
```



### 介绍SAM模型



### YOLO v8和YOLO v5的区别



### negative prompt怎么做的



### Stable Diffusion的结构和原理



### 大语言模型的微调方法

