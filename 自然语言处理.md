# 词表示与语言模型

将词转换成机器能够理解的形式， **word similarity**， **word relation**

one-hot编码（词与词正交），字典（同义词，上位词，但是会存在一词多义的情况），使用context words表达这个词（词表大的时候需要很多存储），word embedding(Word2Vec)

### N-gram Model

collect statistics about how frequent different n-grams are,and use these to predict next word.

考虑前几个词的，预测下一个词。

Markov assumption
$$
P(w_1,w_2,...,w_n)=\prod_{i}P(w_i|w_{i-k},...,w_{i-1})
$$
问题：N-gram考虑的上下文较少，如果N越大统计结果越稀疏，没有办法捕获词之间的相似度



### Word2Vec

Using shallow neural networks that associate words to distributed representations.It can capture many linguistic regularities.

**Continuous bag-of-words(CBOW)**

对于句子，Never too late to learn, 假定窗口大小是5， $$P(late|never,too,to,learn)$$.

**Continuous skip-gram**



**when the vocabulary size is very large**: Negative sampling，采样一小部分的词表进行采样，按照词频的概率进行采样

**Other Tips for Word Enbeddings**

1. sub-sampleing: Rare words can be more likely to carry distinct information, according to which, sub-sampling discards words 𝑤 with probability:  $$1-\sqrt{t/f(w)}$$
2. Soft sliding window: Sliding window should assign less weight to more distant words



### RNNs

**Key concept for RNNs:** Sequential memory  during processing sequence data

![image-20240412211328262](.\assets\image-20240412211328262.png)

RNN Cell

![image-20240412211417829](.\assets\image-20240412211417829.png)

当前的状态是由过去的状态获得到的

**优点:**Can process any length input, Model size does not increase for longer inpu, Weights are shared across timesteps, Computation for step 𝑖 can (in theory) use information from many steps back

**缺点：**Recurrent computation is slow , it’s difficult to access information from many steps back

![image-20240412211942958](.\assets\image-20240412211942958.png)



### Gated Recurrent Unit(GRU)

update gate, reset gate 用于权衡过去的信息与当前信息的比重问题

![image-20240412212231067](.\assets\image-20240412212231067.png)



### Long Short-Term Memory Network (LSTM)

![image-20240412212529205](.\assets\image-20240412212529205.png)

**Fortget gate:** decide what information to throw away from the cell state

![image-20240412212727409](.\assets\image-20240412212727409.png)

**input gate: **decide what information to store in the cell state

![image-20240412212738749](.\assets\image-20240412212738749.png)

**update the old cell state $$C_{t-1}$$**

![image-20240412212902843](.\assets\image-20240412212902843.png)

**output gate:** decide what information to output  

![image-20240412212936030](.\assets\image-20240412212936030.png)

**优点：**

1. Powerful especially when stacked and made even deeper   

2. Very useful if you have plenty of data  



### Bidirectional RNNs

in many applications, we want to have an output $$y_t$$depending on the **whole input sequence**

![image-20240412213132724](.\assets\image-20240412213132724.png)



### Attention

**Seq2seq: The bottleneck problem**

![image-20240412213506574](.\assets\image-20240412213506574.png)

1. The single vector of source sentence encoding needs to capture  all information about the source sentence.
2. The single vector limits the representation capacity of the encoder: the information bottleneck

**eos 最后一个向量的容量限制了Encoder的表达**

解决方法--> Attention

1. Attention provides a solution to the bottleneck problem  
2. Core idea: at each step of the decoder, focus on a particular part of the source sequence



### Seq2Seq with Attention

![image-20240412213948217](.\assets\image-20240412213948217.png)

![image-20240412214149238](.\assets\image-20240412214149238.png)

#### Attention Mechanism

1. Encoder hidden states: $$h_1, h_2, ...,h_N$$
2. Decoder hidden state at time step t: $$s_t$$
3. attention score $$e^t=[s_1^Th_1, ..., h_t^Th_N]$$
4. Use softmax to get the attention distrubution $$\alpha^t=softmax(e^t)$$
5. attetion output $$o_t=\sum_{i=1}^N \alpha_i^th_i$$
6. Concatenate the attention ouput and the decoder hidden state to predict the word $$[o_t; s_t]$$



#### definition of attention:  

Given a query vector and a set of value  vectors, the attention technique computes a **weighted sum of the  values  according to the query**.

The weighted sum is  a selective summary of the values.

We can obtain a fixed-size representation of **an arbitrary set of representations** via the attention mechanism.



### Insights of Attention

1. 解决信息瓶颈， The decoder could directly look at source  
2. 防止梯度消失， By providing shortcuts to long-distance states  
3. 提供可解释性

![image-20240412215202689](.\assets\image-20240412215202689.png)



### Transformer

**Motivations:**

1. Sequential computation in RNNs prevents parallelization  
2. Despite using GRU or LSTM, RNNs still need attention mechanism which provides access to any state
3. Maybe we do not need RNNs?  



#### Byte Pair Encoding (BPE)

使用空格进行词语的切分有时候并不好

计算语料库中每一个byte gram 出现的数量，将频度最高的Btye gram抽象成一个词加入到词表中。

1. Start with a vocabulary of characters
2. Turn the most frequent n-gram to a new n-gram  

**Motivation:**

1. Solve the OOV  **(out of vocabulary) problem** by encoding rare and unknown words as sequences of subword units
2. In the example above, the OOV word "lowest" would be segmented into **"low est”**



### Positional Encoding (PE)

**Motivation:**

1. The Transformer block is not sensitive to **the same words  with different positions**
2. The positional encoding is added so that the same words at different locations have different representations.



### Transformer Encoder Block

![image-20240412220837101](.\assets\image-20240412220837101.png)

**Two sublayers**: Multi-Head Attention, Feed-Forward Network(2-layer MLP)

**Two tricks**: Residual connection, Lyaer normalization(changes input to have mean 0 and variance 1)

![image-20240412221116899](.\assets\image-20240412221116899.png)



**Scaled Dot-Product Attention:** As $$d_k$$ get large, the variance of $$q^Tk$$​ increase, the softmax gets very peaked, gradient gets smaller, 一个位置的数值为1，其他位置的为0，导致梯度越来越小，scaled的目的是使得输出的向量方差接近1



### Transformer Decoder Block

1. Masked self-attention: The word can only look at **previous** words

![image-20240412222257278](.\assets\image-20240412222257278.png)

2. Encoder-decoder attention: Quries come from the decoder while keys and values come from the encoder

![image-20240412222127449](.\assets\image-20240412222127449.png)



**优点：**

1. 具有很强的表示能力
2. attention， FFN适合并行计算
3. 可解释性

**缺点：**

1. 难以优化，对超参数以及优化器等选择敏感
2. `O(n^2)`的时间复杂度，文本长度有限制 



### Pretrain Language Model (PLM)

word2vec, GPT, BERT, …  

PLMs: language models having powerful  transferability for other NLP tasks

**Feature-based methods:** word2vec. Use the outputs of PLMs as the inputs of our downstream models.

**Fine-tuning methods:** BERT,GPT. The language models will also be the downstream models and their parameters will be updated.



### 语言模型评价指标Perplexity

PPL是用在自然语言处理领域（NLP）中，衡量语言模型好坏的指标。它主要是根据每个词来估计一句话出现的概率，并用句子长度作normalize，公式为：
$$
PP(S)=^N\sqrt{\prod_{i-1}^N\frac{1}{P(w_i|w_1,w2,...,w_{i-1})}}
$$
S代表sentence，N是句子长度，$$P(w_i)$$是第i个词的概率。第一个词就是 $p(w_1|w_0)$，而$w_0$是START，表示句子的起始，是个占位符。

PPL越小，$p(w_i)$则越大，一句我们期望的sentence出现的概率就越高,模型越好。



### GPT

![image-20240413114503914](.\assets\image-20240413114503914.png)

GPT表现出了强大的zero-shot能力，通过我们提问的方式，GPT可以通过自回归的方式来生成对话结果。



### BERT

Change the paradigm of NLP significantly

**Problem:** 

Language models only use left context or right context, but language understanding is  **bidirectional**.

**Why are LMs unidirectional?  **

1. Directionality is needed to generate a wellformed probability distribution ， 自然地把长文本拆解成小部分
2. Words can “see themselves” in a bidirectional encoder 信息泄露

![image-20240413150050491](.\assets\image-20240413150050491.png)

#### **Solution：**Mask  out k% of the input words, and then predict the masked words.

**15%是一个综合的考量**

1. Too little masking: too expensive to train  
2. Too much masking: not enough context  



**Problem：[mask] token never seen at fine-tune, [mask] token在微调的时候是不可见的，所以会导致微调的性能下降**

**Solution：**

1. 80%的可能保留[mask] token
2. 10%的可能替换成一个随机的词，然后希望将该词进行一个纠错，让模型关注不是[mask]的词，但是这样可能导致模型认为所有的词都是错的
3. 10%的可能保持原本的词



### BERT：Next Sentence Prediction

**To learn relationships  between sentences, predict whether Sentence B is the actual sentence that proceeds Sentence A, or just a random sentence**

![image-20240413151332905](.\assets\image-20240413151332905.png)



### BERT: Input Repressentation

1. **token embedding:** Use 30,000 WordPiece vocabulary on input , token embedding 层是要将各个词转换成固定维度的向量。在BERT中，每个词会被转换成768维的向量表示
2. **Segment Embeddings:** Segement Embeddings 层有两种向量表示，前一个向量是把 0 赋值给第一个句子的各个 Token，后一个向量是把1赋值给各个 Token，问答系统等任务要预测下一句，因此输入是有关联的句子。而文本分类只有一个句子，那么 Segement embeddings 就全部是 0。
3. **Position Embeddings:** BERT能够处理最长512个token的输入序列。通过让BERT在各个位置上学习一个向量表示来讲序列顺序的信息编码进来。
4. **[CLS]的作用：** BERT在第一句前会加一个[CLS]标志，最后一层该位对应向量可以作为整句话的语义表示，从而用于下游的分类任务等。

![image-20240413151525938](.\assets\image-20240413151525938.png)



### Summary

1. Feature-based 的方法给下游任务提供一个有上下文的word embedding，固定了输入的feature，效果比较有限
2. Fine-tuning的方法的参数在整个下游任务中得到调整，性能更高
3. BERT的预训练和下游任务存在较大的gap，训练的效率比较低，因为只有15%的单词被预测，训练的窗口较小，只有512



### GPT-3

**Excellent few-shot/in-context learning ability  **

![image-20240413153852645](.\assets\image-20240413153852645.png)

![image-20240413153900258](.\assets\image-20240413153900258.png)

### T5 (Encoder-Decoder)

Reframe all NLP tasks into a unified text-to-text-format where the input and output are always text strings .

采用Seq2Seq的方式去生成一个句子的答案。



### Large Model with MoE （Miture of Experts）

采用MoE的方式去增大模型的参数，训练更大规模的预训练语言模型。

**Key idea：** 把模型的参数分成一块一块的模块，每次的模型输入只调用其中部分子模块来参与计算

![image-20240413154236349](.\assets\image-20240413154236349.png)





# Prompt

### T5

采用Seq2Seq的方式去训练模型，给模型一个句子让他输出我们希望的结果

1. Encoder-decoder with 11 billion parameters  
2. Cast tasks to seq2seq manner with simple demonstrations  
3. A decoder is trained to output the desired tokens  

### GPT3

1. Huge model with 175 billion parameters  
2. No parameters are updated at all  
3. Descriptions (Prompts) + Few-shot examples to generate tokens  







































































