# è¯è¡¨ç¤ºä¸è¯­è¨€æ¨¡å‹

å°†è¯è½¬æ¢æˆæœºå™¨èƒ½å¤Ÿç†è§£çš„å½¢å¼ï¼Œ **word similarity**ï¼Œ **word relation**

one-hotç¼–ç ï¼ˆè¯ä¸è¯æ­£äº¤ï¼‰ï¼Œå­—å…¸ï¼ˆåŒä¹‰è¯ï¼Œä¸Šä½è¯ï¼Œä½†æ˜¯ä¼šå­˜åœ¨ä¸€è¯å¤šä¹‰çš„æƒ…å†µï¼‰ï¼Œä½¿ç”¨context wordsè¡¨è¾¾è¿™ä¸ªè¯ï¼ˆè¯è¡¨å¤§çš„æ—¶å€™éœ€è¦å¾ˆå¤šå­˜å‚¨ï¼‰ï¼Œword embedding(Word2Vec)

### N-gram Model

collect statistics about how frequent different n-grams are,and use these to predict next word.

è€ƒè™‘å‰å‡ ä¸ªè¯çš„ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚

Markov assumption
$$
P(w_1,w_2,...,w_n)=\prod_{i}P(w_i|w_{i-k},...,w_{i-1})
$$
é—®é¢˜ï¼šN-gramè€ƒè™‘çš„ä¸Šä¸‹æ–‡è¾ƒå°‘ï¼Œå¦‚æœNè¶Šå¤§ç»Ÿè®¡ç»“æœè¶Šç¨€ç–ï¼Œæ²¡æœ‰åŠæ³•æ•è·è¯ä¹‹é—´çš„ç›¸ä¼¼åº¦



### Word2Vec

Using shallow neural networks that associate words to distributed representations.It can capture many linguistic regularities.

**Continuous bag-of-words(CBOW)**

å¯¹äºå¥å­ï¼ŒNever too late to learn, å‡å®šçª—å£å¤§å°æ˜¯5ï¼Œ $$P(late|never,too,to,learn)$$.

**Continuous skip-gram**



**when the vocabulary size is very large**: Negative samplingï¼Œé‡‡æ ·ä¸€å°éƒ¨åˆ†çš„è¯è¡¨è¿›è¡Œé‡‡æ ·ï¼ŒæŒ‰ç…§è¯é¢‘çš„æ¦‚ç‡è¿›è¡Œé‡‡æ ·

**Other Tips for Word Enbeddings**

1. sub-sampleing: Rare words can be more likely to carry distinct information, according to which, sub-sampling discards words ğ‘¤ with probability:  $$1-\sqrt{t/f(w)}$$
2. Soft sliding window: Sliding window should assign less weight to more distant words



### RNNs

**Key concept for RNNs:** Sequential memory  during processing sequence data

![image-20240412211328262](.\assets\image-20240412211328262.png)

RNN Cell

![image-20240412211417829](.\assets\image-20240412211417829.png)

å½“å‰çš„çŠ¶æ€æ˜¯ç”±è¿‡å»çš„çŠ¶æ€è·å¾—åˆ°çš„

**ä¼˜ç‚¹:**Can process any length input, Model size does not increase for longer inpu, Weights are shared across timesteps, Computation for step ğ‘– can (in theory) use information from many steps back

**ç¼ºç‚¹ï¼š**Recurrent computation is slow , itâ€™s difficult to access information from many steps back

![image-20240412211942958](.\assets\image-20240412211942958.png)



### Gated Recurrent Unit(GRU)

update gate, reset gate ç”¨äºæƒè¡¡è¿‡å»çš„ä¿¡æ¯ä¸å½“å‰ä¿¡æ¯çš„æ¯”é‡é—®é¢˜

![image-20240412212231067](.\assets\image-20240412212231067.png)



### Long Short-Term Memory Network (LSTM)

![image-20240412212529205](.\assets\image-20240412212529205.png)

**Fortget gate:** decide what information to throw away from the cell state

![image-20240412212727409](.\assets\image-20240412212727409.png)

**input gate: **decide what information to store in the cell state

![image-20240412212738749](.\assets\image-20240412212738749.png)

**update the old cell state $$C_{t-1}$$**

![image-20240412212902843](.\assets\image-20240412212902843.png)

**output gate:** decide what information to output  

![image-20240412212936030](.\assets\image-20240412212936030.png)

**ä¼˜ç‚¹ï¼š**

1. Powerful especially when stacked and made even deeper   

2. Very useful if you have plenty of data  



### Bidirectional RNNs

in many applications, we want to have an output $$y_t$$depending on the **whole input sequence**

![image-20240412213132724](.\assets\image-20240412213132724.png)



### Attention

**Seq2seq: The bottleneck problem**

![image-20240412213506574](.\assets\image-20240412213506574.png)

1. The single vector of source sentence encoding needs to capture  all information about the source sentence.
2. The single vector limits the representation capacity of the encoder: the information bottleneck

**eos æœ€åä¸€ä¸ªå‘é‡çš„å®¹é‡é™åˆ¶äº†Encoderçš„è¡¨è¾¾**

è§£å†³æ–¹æ³•--> Attention

1. Attention provides a solution to the bottleneck problem  
2. Core idea: at each step of the decoder, focus on a particular part of the source sequence



### Seq2Seq with Attention

![image-20240412213948217](.\assets\image-20240412213948217.png)

![image-20240412214149238](.\assets\image-20240412214149238.png)

#### Attention Mechanism

1. Encoder hidden states: $$h_1, h_2, ...,h_N$$
2. Decoder hidden state at time step t: $$s_t$$
3. attention score $$e^t=[s_1^Th_1, ..., h_t^Th_N]$$
4. Use softmax to get the attention distrubution $$\alpha^t=softmax(e^t)$$
5. attetion output $$o_t=\sum_{i=1}^N \alpha_i^th_i$$
6. Concatenate the attention ouput and the decoder hidden state to predict the word $$[o_t; s_t]$$



#### definition of attention:  

Given a query vector and a set of value  vectors, the attention technique computes a **weighted sum of the  values  according to the query**.

The weighted sum is  a selective summary of the values.

We can obtain a fixed-size representation of **an arbitrary set of representations** via the attention mechanism.



### Insights of Attention

1. è§£å†³ä¿¡æ¯ç“¶é¢ˆï¼Œ The decoder could directly look at source  
2. é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼Œ By providing shortcuts to long-distance states  
3. æä¾›å¯è§£é‡Šæ€§

![image-20240412215202689](.\assets\image-20240412215202689.png)



### Transformer

**Motivations:**

1. Sequential computation in RNNs prevents parallelization  
2. Despite using GRU or LSTM, RNNs still need attention mechanism which provides access to any state
3. Maybe we do not need RNNs?  



#### Byte Pair Encoding (BPE)

ä½¿ç”¨ç©ºæ ¼è¿›è¡Œè¯è¯­çš„åˆ‡åˆ†æœ‰æ—¶å€™å¹¶ä¸å¥½

è®¡ç®—è¯­æ–™åº“ä¸­æ¯ä¸€ä¸ªbyte gram å‡ºç°çš„æ•°é‡ï¼Œå°†é¢‘åº¦æœ€é«˜çš„Btye gramæŠ½è±¡æˆä¸€ä¸ªè¯åŠ å…¥åˆ°è¯è¡¨ä¸­ã€‚

1. Start with a vocabulary of characters
2. Turn the most frequent n-gram to a new n-gram  

**Motivation:**

1. Solve the OOV  **(out of vocabulary) problem** by encoding rare and unknown words as sequences of subword units
2. In the example above, the OOV word "lowest" would be segmented into **"low estâ€**



### Positional Encoding (PE)

**Motivation:**

1. The Transformer block is not sensitive to **the same words  with different positions**
2. The positional encoding is added so that the same words at different locations have different representations.



### Transformer Encoder Block

![image-20240412220837101](.\assets\image-20240412220837101.png)

**Two sublayers**: Multi-Head Attention, Feed-Forward Network(2-layer MLP)

**Two tricks**: Residual connection, Lyaer normalization(changes input to have mean 0 and variance 1)

![image-20240412221116899](.\assets\image-20240412221116899.png)



**Scaled Dot-Product Attention:** As $$d_k$$ get large, the variance of $$q^Tk$$â€‹ increase, the softmax gets very peaked, gradient gets smaller, ä¸€ä¸ªä½ç½®çš„æ•°å€¼ä¸º1ï¼Œå…¶ä»–ä½ç½®çš„ä¸º0ï¼Œå¯¼è‡´æ¢¯åº¦è¶Šæ¥è¶Šå°ï¼Œscaledçš„ç›®çš„æ˜¯ä½¿å¾—è¾“å‡ºçš„å‘é‡æ–¹å·®æ¥è¿‘1



### Transformer Decoder Block

1. Masked self-attention: The word can only look at **previous** words

![image-20240412222257278](.\assets\image-20240412222257278.png)

2. Encoder-decoder attention: Quries come from the decoder while keys and values come from the encoder

![image-20240412222127449](.\assets\image-20240412222127449.png)



**ä¼˜ç‚¹ï¼š**

1. å…·æœ‰å¾ˆå¼ºçš„è¡¨ç¤ºèƒ½åŠ›
2. attentionï¼Œ FFNé€‚åˆå¹¶è¡Œè®¡ç®—
3. å¯è§£é‡Šæ€§

**ç¼ºç‚¹ï¼š**

1. éš¾ä»¥ä¼˜åŒ–ï¼Œå¯¹è¶…å‚æ•°ä»¥åŠä¼˜åŒ–å™¨ç­‰é€‰æ‹©æ•æ„Ÿ
2. `O(n^2)`çš„æ—¶é—´å¤æ‚åº¦ï¼Œæ–‡æœ¬é•¿åº¦æœ‰é™åˆ¶ 



### Pretrain Language Model (PLM)

word2vec, GPT, BERT, â€¦  

PLMs: language models having powerful  transferability for other NLP tasks

**Feature-based methods:** word2vec. Use the outputs of PLMs as the inputs of our downstream models.

**Fine-tuning methods:** BERT,GPT. The language models will also be the downstream models and their parameters will be updated.



### è¯­è¨€æ¨¡å‹è¯„ä»·æŒ‡æ ‡Perplexity

PPLæ˜¯ç”¨åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼ˆNLPï¼‰ä¸­ï¼Œè¡¡é‡è¯­è¨€æ¨¡å‹å¥½åçš„æŒ‡æ ‡ã€‚å®ƒä¸»è¦æ˜¯æ ¹æ®æ¯ä¸ªè¯æ¥ä¼°è®¡ä¸€å¥è¯å‡ºç°çš„æ¦‚ç‡ï¼Œå¹¶ç”¨å¥å­é•¿åº¦ä½œnormalizeï¼Œå…¬å¼ä¸ºï¼š
$$
PP(S)=^N\sqrt{\prod_{i-1}^N\frac{1}{P(w_i|w_1,w2,...,w_{i-1})}}
$$
Sä»£è¡¨sentenceï¼ŒNæ˜¯å¥å­é•¿åº¦ï¼Œ$$P(w_i)$$æ˜¯ç¬¬iä¸ªè¯çš„æ¦‚ç‡ã€‚ç¬¬ä¸€ä¸ªè¯å°±æ˜¯ $p(w_1|w_0)$ï¼Œè€Œ$w_0$æ˜¯STARTï¼Œè¡¨ç¤ºå¥å­çš„èµ·å§‹ï¼Œæ˜¯ä¸ªå ä½ç¬¦ã€‚

PPLè¶Šå°ï¼Œ$p(w_i)$åˆ™è¶Šå¤§ï¼Œä¸€å¥æˆ‘ä»¬æœŸæœ›çš„sentenceå‡ºç°çš„æ¦‚ç‡å°±è¶Šé«˜,æ¨¡å‹è¶Šå¥½ã€‚



### GPT

![image-20240413114503914](.\assets\image-20240413114503914.png)

GPTè¡¨ç°å‡ºäº†å¼ºå¤§çš„zero-shotèƒ½åŠ›ï¼Œé€šè¿‡æˆ‘ä»¬æé—®çš„æ–¹å¼ï¼ŒGPTå¯ä»¥é€šè¿‡è‡ªå›å½’çš„æ–¹å¼æ¥ç”Ÿæˆå¯¹è¯ç»“æœã€‚



### BERT

Change the paradigm of NLP significantly

**Problem:** 

Language models only use left context or right context, but language understanding is  **bidirectional**.

**Why are LMs unidirectional?  **

1. Directionality is needed to generate a wellformed probability distribution ï¼Œ è‡ªç„¶åœ°æŠŠé•¿æ–‡æœ¬æ‹†è§£æˆå°éƒ¨åˆ†
2. Words can â€œsee themselvesâ€ in a bidirectional encoder ä¿¡æ¯æ³„éœ²

![image-20240413150050491](.\assets\image-20240413150050491.png)

#### **Solutionï¼š**Mask  out k% of the input words, and then predict the masked words.

**15%æ˜¯ä¸€ä¸ªç»¼åˆçš„è€ƒé‡**

1. Too little masking: too expensive to train  
2. Too much masking: not enough context  



**Problemï¼š[mask] token never seen at fine-tune, [mask] tokenåœ¨å¾®è°ƒçš„æ—¶å€™æ˜¯ä¸å¯è§çš„ï¼Œæ‰€ä»¥ä¼šå¯¼è‡´å¾®è°ƒçš„æ€§èƒ½ä¸‹é™**

**Solutionï¼š**

1. 80%çš„å¯èƒ½ä¿ç•™[mask] token
2. 10%çš„å¯èƒ½æ›¿æ¢æˆä¸€ä¸ªéšæœºçš„è¯ï¼Œç„¶åå¸Œæœ›å°†è¯¥è¯è¿›è¡Œä¸€ä¸ªçº é”™ï¼Œè®©æ¨¡å‹å…³æ³¨ä¸æ˜¯[mask]çš„è¯ï¼Œä½†æ˜¯è¿™æ ·å¯èƒ½å¯¼è‡´æ¨¡å‹è®¤ä¸ºæ‰€æœ‰çš„è¯éƒ½æ˜¯é”™çš„
3. 10%çš„å¯èƒ½ä¿æŒåŸæœ¬çš„è¯



### BERTï¼šNext Sentence Prediction

**To learn relationships  between sentences, predict whether Sentence B is the actual sentence that proceeds Sentence A, or just a random sentence**

![image-20240413151332905](.\assets\image-20240413151332905.png)



### BERT: Input Repressentation

1. **token embedding:** Use 30,000 WordPiece vocabulary on input , token embedding å±‚æ˜¯è¦å°†å„ä¸ªè¯è½¬æ¢æˆå›ºå®šç»´åº¦çš„å‘é‡ã€‚åœ¨BERTä¸­ï¼Œæ¯ä¸ªè¯ä¼šè¢«è½¬æ¢æˆ768ç»´çš„å‘é‡è¡¨ç¤º
2. **Segment Embeddings:** Segement Embeddings å±‚æœ‰ä¸¤ç§å‘é‡è¡¨ç¤ºï¼Œå‰ä¸€ä¸ªå‘é‡æ˜¯æŠŠ 0 èµ‹å€¼ç»™ç¬¬ä¸€ä¸ªå¥å­çš„å„ä¸ª Tokenï¼Œåä¸€ä¸ªå‘é‡æ˜¯æŠŠ1èµ‹å€¼ç»™å„ä¸ª Tokenï¼Œé—®ç­”ç³»ç»Ÿç­‰ä»»åŠ¡è¦é¢„æµ‹ä¸‹ä¸€å¥ï¼Œå› æ­¤è¾“å…¥æ˜¯æœ‰å…³è”çš„å¥å­ã€‚è€Œæ–‡æœ¬åˆ†ç±»åªæœ‰ä¸€ä¸ªå¥å­ï¼Œé‚£ä¹ˆ Segement embeddings å°±å…¨éƒ¨æ˜¯ 0ã€‚
3. **Position Embeddings:** BERTèƒ½å¤Ÿå¤„ç†æœ€é•¿512ä¸ªtokençš„è¾“å…¥åºåˆ—ã€‚é€šè¿‡è®©BERTåœ¨å„ä¸ªä½ç½®ä¸Šå­¦ä¹ ä¸€ä¸ªå‘é‡è¡¨ç¤ºæ¥è®²åºåˆ—é¡ºåºçš„ä¿¡æ¯ç¼–ç è¿›æ¥ã€‚
4. **[CLS]çš„ä½œç”¨ï¼š** BERTåœ¨ç¬¬ä¸€å¥å‰ä¼šåŠ ä¸€ä¸ª[CLS]æ ‡å¿—ï¼Œæœ€åä¸€å±‚è¯¥ä½å¯¹åº”å‘é‡å¯ä»¥ä½œä¸ºæ•´å¥è¯çš„è¯­ä¹‰è¡¨ç¤ºï¼Œä»è€Œç”¨äºä¸‹æ¸¸çš„åˆ†ç±»ä»»åŠ¡ç­‰ã€‚

![image-20240413151525938](.\assets\image-20240413151525938.png)



### Summary

1. Feature-based çš„æ–¹æ³•ç»™ä¸‹æ¸¸ä»»åŠ¡æä¾›ä¸€ä¸ªæœ‰ä¸Šä¸‹æ–‡çš„word embeddingï¼Œå›ºå®šäº†è¾“å…¥çš„featureï¼Œæ•ˆæœæ¯”è¾ƒæœ‰é™
2. Fine-tuningçš„æ–¹æ³•çš„å‚æ•°åœ¨æ•´ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­å¾—åˆ°è°ƒæ•´ï¼Œæ€§èƒ½æ›´é«˜
3. BERTçš„é¢„è®­ç»ƒå’Œä¸‹æ¸¸ä»»åŠ¡å­˜åœ¨è¾ƒå¤§çš„gapï¼Œè®­ç»ƒçš„æ•ˆç‡æ¯”è¾ƒä½ï¼Œå› ä¸ºåªæœ‰15%çš„å•è¯è¢«é¢„æµ‹ï¼Œè®­ç»ƒçš„çª—å£è¾ƒå°ï¼Œåªæœ‰512



### GPT-3

**Excellent few-shot/in-context learning ability  **

![image-20240413153852645](.\assets\image-20240413153852645.png)

![image-20240413153900258](.\assets\image-20240413153900258.png)

### T5 (Encoder-Decoder)

Reframe all NLP tasks into a unified text-to-text-format where the input and output are always text strings .

é‡‡ç”¨Seq2Seqçš„æ–¹å¼å»ç”Ÿæˆä¸€ä¸ªå¥å­çš„ç­”æ¡ˆã€‚



### Large Model with MoE ï¼ˆMiture of Expertsï¼‰

é‡‡ç”¨MoEçš„æ–¹å¼å»å¢å¤§æ¨¡å‹çš„å‚æ•°ï¼Œè®­ç»ƒæ›´å¤§è§„æ¨¡çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚

**Key ideaï¼š** æŠŠæ¨¡å‹çš„å‚æ•°åˆ†æˆä¸€å—ä¸€å—çš„æ¨¡å—ï¼Œæ¯æ¬¡çš„æ¨¡å‹è¾“å…¥åªè°ƒç”¨å…¶ä¸­éƒ¨åˆ†å­æ¨¡å—æ¥å‚ä¸è®¡ç®—

![image-20240413154236349](.\assets\image-20240413154236349.png)





# Prompt

### T5

é‡‡ç”¨Seq2Seqçš„æ–¹å¼å»è®­ç»ƒæ¨¡å‹ï¼Œç»™æ¨¡å‹ä¸€ä¸ªå¥å­è®©ä»–è¾“å‡ºæˆ‘ä»¬å¸Œæœ›çš„ç»“æœ

1. Encoder-decoder with 11 billion parameters  
2. Cast tasks to seq2seq manner with simple demonstrations  
3. A decoder is trained to output the desired tokens  

### GPT3

1. Huge model with 175 billion parameters  
2. No parameters are updated at all  
3. Descriptions (Prompts) + Few-shot examples to generate tokens  







































































