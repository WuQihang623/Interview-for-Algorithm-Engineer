# è¯è¡¨ç¤ºä¸è¯­è¨€æ¨¡å‹

å°†è¯è½¬æ¢æˆæœºå™¨èƒ½å¤Ÿç†è§£çš„å½¢å¼ï¼Œ **word similarity**ï¼Œ **word relation**

one-hotç¼–ç ï¼ˆè¯ä¸è¯æ­£äº¤ï¼‰ï¼Œå­—å…¸ï¼ˆåŒä¹‰è¯ï¼Œä¸Šä½è¯ï¼Œä½†æ˜¯ä¼šå­˜åœ¨ä¸€è¯å¤šä¹‰çš„æƒ…å†µï¼‰ï¼Œä½¿ç”¨context wordsè¡¨è¾¾è¿™ä¸ªè¯ï¼ˆè¯è¡¨å¤§çš„æ—¶å€™éœ€è¦å¾ˆå¤šå­˜å‚¨ï¼‰ï¼Œword embedding(Word2Vec)

### N-gram Model

collect statistics about how frequent different n-grams are,and use these to predict next word.

è€ƒè™‘å‰å‡ ä¸ªè¯çš„ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚

Markov assumption
$$
P(w_1,w_2,...,w_n)=\prod_{i}P(w_i|w_{i-k},...,w_{i-1})
$$
é—®é¢˜ï¼šN-gramè€ƒè™‘çš„ä¸Šä¸‹æ–‡è¾ƒå°‘ï¼Œå¦‚æœNè¶Šå¤§ç»Ÿè®¡ç»“æœè¶Šç¨€ç–ï¼Œæ²¡æœ‰åŠæ³•æ•è·è¯ä¹‹é—´çš„ç›¸ä¼¼åº¦



### Word2Vec

Using shallow neural networks that associate words to distributed representations.It can capture many linguistic regularities.

**Continuous bag-of-words(CBOW)**

å¯¹äºå¥å­ï¼ŒNever too late to learn, å‡å®šçª—å£å¤§å°æ˜¯5ï¼Œ $$P(late|never,too,to,learn)$$.

**Continuous skip-gram**



**when the vocabulary size is very large**: Negative samplingï¼Œé‡‡æ ·ä¸€å°éƒ¨åˆ†çš„è¯è¡¨è¿›è¡Œé‡‡æ ·ï¼ŒæŒ‰ç…§è¯é¢‘çš„æ¦‚ç‡è¿›è¡Œé‡‡æ ·

**Other Tips for Word Enbeddings**

1. sub-sampleing: Rare words can be more likely to carry distinct information, according to which, sub-sampling discards words ğ‘¤ with probability:  $$1-\sqrt{t/f(w)}$$
2. Soft sliding window: Sliding window should assign less weight to more distant words



### RNNs

**Key concept for RNNs:** Sequential memory  during processing sequence data

![image-20240412211328262](.\assets\image-20240412211328262.png)

RNN Cell

![image-20240412211417829](.\assets\image-20240412211417829.png)

å½“å‰çš„çŠ¶æ€æ˜¯ç”±è¿‡å»çš„çŠ¶æ€è·å¾—åˆ°çš„

**ä¼˜ç‚¹:**Can process any length input, Model size does not increase for longer inpu, Weights are shared across timesteps, Computation for step ğ‘– can (in theory) use information from many steps back

**ç¼ºç‚¹ï¼š**Recurrent computation is slow , itâ€™s difficult to access information from many steps back

![image-20240412211942958](.\assets\image-20240412211942958.png)



### Gated Recurrent Unit(GRU)

update gate, reset gate ç”¨äºæƒè¡¡è¿‡å»çš„ä¿¡æ¯ä¸å½“å‰ä¿¡æ¯çš„æ¯”é‡é—®é¢˜

![image-20240412212231067](.\assets\image-20240412212231067.png)



### Long Short-Term Memory Network (LSTM)

![image-20240412212529205](.\assets\image-20240412212529205.png)

**Fortget gate:** decide what information to throw away from the cell state

![image-20240412212727409](.\assets\image-20240412212727409.png)

**input gate: **decide what information to store in the cell state

![image-20240412212738749](.\assets\image-20240412212738749.png)

**update the old cell state $$C_{t-1}$$**

![image-20240412212902843](.\assets\image-20240412212902843.png)

**output gate:** decide what information to output  

![image-20240412212936030](.\assets\image-20240412212936030.png)

**ä¼˜ç‚¹ï¼š**

1. Powerful especially when stacked and made even deeper   

2. Very useful if you have plenty of data  



### Bidirectional RNNs

in many applications, we want to have an output $$y_t$$depending on the **whole input sequence**

![image-20240412213132724](.\assets\image-20240412213132724.png)



### Attention

**Seq2seq: The bottleneck problem**

![image-20240412213506574](.\assets\image-20240412213506574.png)

1. The single vector of source sentence encoding needs to capture  all information about the source sentence.
2. The single vector limits the representation capacity of the encoder: the information bottleneck

**eos æœ€åä¸€ä¸ªå‘é‡çš„å®¹é‡é™åˆ¶äº†Encoderçš„è¡¨è¾¾**

è§£å†³æ–¹æ³•--> Attention

1. Attention provides a solution to the bottleneck problem  
2. Core idea: at each step of the decoder, focus on a particular part of the source sequence



### Seq2Seq with Attention

![image-20240412213948217](.\assets\image-20240412213948217.png)

![image-20240412214149238](.\assets\image-20240412214149238.png)

#### Attention Mechanism

1. Encoder hidden states: $$h_1, h_2, ...,h_N$$
2. Decoder hidden state at time step t: $$s_t$$
3. attention score $$e^t=[s_1^Th_1, ..., h_t^Th_N]$$
4. Use softmax to get the attention distrubution $$\alpha^t=softmax(e^t)$$
5. attetion output $$o_t=\sum_{i=1}^N \alpha_i^th_i$$
6. Concatenate the attention ouput and the decoder hidden state to predict the word $$[o_t; s_t]$$



#### definition of attention:  

Given a query vector and a set of value  vectors, the attention technique computes a **weighted sum of the  values  according to the query**.

The weighted sum is  a selective summary of the values.

We can obtain a fixed-size representation of **an arbitrary set of representations** via the attention mechanism.



### Insights of Attention

1. è§£å†³ä¿¡æ¯ç“¶é¢ˆï¼Œ The decoder could directly look at source  
2. é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼Œ By providing shortcuts to long-distance states  
3. æä¾›å¯è§£é‡Šæ€§

![image-20240412215202689](.\assets\image-20240412215202689.png)



### Transformer

**Motivations:**

1. Sequential computation in RNNs prevents parallelization  
2. Despite using GRU or LSTM, RNNs still need attention mechanism which provides access to any state
3. Maybe we do not need RNNs?  



#### Byte Pair Encoding (BPE)

ä½¿ç”¨ç©ºæ ¼è¿›è¡Œè¯è¯­çš„åˆ‡åˆ†æœ‰æ—¶å€™å¹¶ä¸å¥½

è®¡ç®—è¯­æ–™åº“ä¸­æ¯ä¸€ä¸ªbyte gram å‡ºç°çš„æ•°é‡ï¼Œå°†é¢‘åº¦æœ€é«˜çš„Btye gramæŠ½è±¡æˆä¸€ä¸ªè¯åŠ å…¥åˆ°è¯è¡¨ä¸­ã€‚

1. Start with a vocabulary of characters
2. Turn the most frequent n-gram to a new n-gram  

**Motivation:**

1. Solve the OOV  **(out of vocabulary) problem** by encoding rare and unknown words as sequences of subword units
2. In the example above, the OOV word "lowest" would be segmented into **"low estâ€**



### Positional Encoding (PE)

**Motivation:**

1. The Transformer block is not sensitive to **the same words  with different positions**
2. The positional encoding is added so that the same words at different locations have different representations.



### Transformer Encoder Block

![image-20240412220837101](.\assets\image-20240412220837101.png)

**Two sublayers**: Multi-Head Attention, Feed-Forward Network(2-layer MLP)

**Two tricks**: Residual connection, Lyaer normalization(changes input to have mean 0 and variance 1)

![image-20240412221116899](.\assets\image-20240412221116899.png)



**Scaled Dot-Product Attention:** As $$d_k$$ get large, the variance of $$q^Tk$$â€‹ increase, the softmax gets very peaked, gradient gets smaller, ä¸€ä¸ªä½ç½®çš„æ•°å€¼ä¸º1ï¼Œå…¶ä»–ä½ç½®çš„ä¸º0ï¼Œå¯¼è‡´æ¢¯åº¦è¶Šæ¥è¶Šå°ï¼Œscaledçš„ç›®çš„æ˜¯ä½¿å¾—è¾“å‡ºçš„å‘é‡æ–¹å·®æ¥è¿‘1



### Transformer Decoder Block

1. Masked self-attention: The word can only look at **previous** words

![image-20240412222257278](.\assets\image-20240412222257278.png)

2. Encoder-decoder attention: Quries come from the decoder while keys and values come from the encoder

![image-20240412222127449](.\assets\image-20240412222127449.png)



**ä¼˜ç‚¹ï¼š**

1. å…·æœ‰å¾ˆå¼ºçš„è¡¨ç¤ºèƒ½åŠ›
2. attentionï¼Œ FFNé€‚åˆå¹¶è¡Œè®¡ç®—
3. å¯è§£é‡Šæ€§

**ç¼ºç‚¹ï¼š**

1. éš¾ä»¥ä¼˜åŒ–ï¼Œå¯¹è¶…å‚æ•°ä»¥åŠä¼˜åŒ–å™¨ç­‰é€‰æ‹©æ•æ„Ÿ
2. `O(n^2)`çš„æ—¶é—´å¤æ‚åº¦ï¼Œæ–‡æœ¬é•¿åº¦æœ‰é™åˆ¶ 



### Pretrain Language Model (PLM)

word2vec, GPT, BERT, â€¦  

PLMs: language models having powerful  transferability for other NLP tasks

**Feature-based methods:** word2vec. Use the outputs of PLMs as the inputs of our downstream models.

**Fine-tuning methods:** BERT,GPT. The language models will also be the downstream models and their parameters will be updated.



### è¯­è¨€æ¨¡å‹è¯„ä»·æŒ‡æ ‡Perplexity

PPLæ˜¯ç”¨åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼ˆNLPï¼‰ä¸­ï¼Œè¡¡é‡è¯­è¨€æ¨¡å‹å¥½åçš„æŒ‡æ ‡ã€‚å®ƒä¸»è¦æ˜¯æ ¹æ®æ¯ä¸ªè¯æ¥ä¼°è®¡ä¸€å¥è¯å‡ºç°çš„æ¦‚ç‡ï¼Œå¹¶ç”¨å¥å­é•¿åº¦ä½œnormalizeï¼Œå…¬å¼ä¸ºï¼š
$$
PP(S)=^N\sqrt{\prod_{i-1}^N\frac{1}{P(w_i|w_1,w2,...,w_{i-1})}}
$$
Sä»£è¡¨sentenceï¼ŒNæ˜¯å¥å­é•¿åº¦ï¼Œ$$P(w_i)$$æ˜¯ç¬¬iä¸ªè¯çš„æ¦‚ç‡ã€‚ç¬¬ä¸€ä¸ªè¯å°±æ˜¯ $p(w_1|w_0)$ï¼Œè€Œ$w_0$æ˜¯STARTï¼Œè¡¨ç¤ºå¥å­çš„èµ·å§‹ï¼Œæ˜¯ä¸ªå ä½ç¬¦ã€‚

PPLè¶Šå°ï¼Œ$p(w_i)$åˆ™è¶Šå¤§ï¼Œä¸€å¥æˆ‘ä»¬æœŸæœ›çš„sentenceå‡ºç°çš„æ¦‚ç‡å°±è¶Šé«˜,æ¨¡å‹è¶Šå¥½ã€‚



### GPT

![image-20240413114503914](.\assets\image-20240413114503914.png)

GPTè¡¨ç°å‡ºäº†å¼ºå¤§çš„zero-shotèƒ½åŠ›ï¼Œé€šè¿‡æˆ‘ä»¬æé—®çš„æ–¹å¼ï¼ŒGPTå¯ä»¥é€šè¿‡è‡ªå›å½’çš„æ–¹å¼æ¥ç”Ÿæˆå¯¹è¯ç»“æœã€‚



### BERT

Change the paradigm of NLP significantly

**Problem:** 

Language models only use left context or right context, but language understanding is  **bidirectional**.

**Why are LMs unidirectional?  **

1. Directionality is needed to generate a wellformed probability distribution ï¼Œ è‡ªç„¶åœ°æŠŠé•¿æ–‡æœ¬æ‹†è§£æˆå°éƒ¨åˆ†
2. Words can â€œsee themselvesâ€ in a bidirectional encoder ä¿¡æ¯æ³„éœ²

![image-20240413150050491](.\assets\image-20240413150050491.png)

#### **Solutionï¼š**Mask  out k% of the input words, and then predict the masked words.

**15%æ˜¯ä¸€ä¸ªç»¼åˆçš„è€ƒé‡**

1. Too little masking: too expensive to train  
2. Too much masking: not enough context  



**Problemï¼š[mask] token never seen at fine-tune, [mask] tokenåœ¨å¾®è°ƒçš„æ—¶å€™æ˜¯ä¸å¯è§çš„ï¼Œæ‰€ä»¥ä¼šå¯¼è‡´å¾®è°ƒçš„æ€§èƒ½ä¸‹é™**

**Solutionï¼š**

1. 80%çš„å¯èƒ½ä¿ç•™[mask] token
2. 10%çš„å¯èƒ½æ›¿æ¢æˆä¸€ä¸ªéšæœºçš„è¯ï¼Œç„¶åå¸Œæœ›å°†è¯¥è¯è¿›è¡Œä¸€ä¸ªçº é”™ï¼Œè®©æ¨¡å‹å…³æ³¨ä¸æ˜¯[mask]çš„è¯ï¼Œä½†æ˜¯è¿™æ ·å¯èƒ½å¯¼è‡´æ¨¡å‹è®¤ä¸ºæ‰€æœ‰çš„è¯éƒ½æ˜¯é”™çš„
3. 10%çš„å¯èƒ½ä¿æŒåŸæœ¬çš„è¯



### BERTï¼šNext Sentence Prediction

**To learn relationships  between sentences, predict whether Sentence B is the actual sentence that proceeds Sentence A, or just a random sentence**

![image-20240413151332905](.\assets\image-20240413151332905.png)



### BERT: Input Repressentation

1. **token embedding:** Use 30,000 WordPiece vocabulary on input , token embedding å±‚æ˜¯è¦å°†å„ä¸ªè¯è½¬æ¢æˆå›ºå®šç»´åº¦çš„å‘é‡ã€‚åœ¨BERTä¸­ï¼Œæ¯ä¸ªè¯ä¼šè¢«è½¬æ¢æˆ768ç»´çš„å‘é‡è¡¨ç¤º
2. **Segment Embeddings:** Segement Embeddings å±‚æœ‰ä¸¤ç§å‘é‡è¡¨ç¤ºï¼Œå‰ä¸€ä¸ªå‘é‡æ˜¯æŠŠ 0 èµ‹å€¼ç»™ç¬¬ä¸€ä¸ªå¥å­çš„å„ä¸ª Tokenï¼Œåä¸€ä¸ªå‘é‡æ˜¯æŠŠ1èµ‹å€¼ç»™å„ä¸ª Tokenï¼Œé—®ç­”ç³»ç»Ÿç­‰ä»»åŠ¡è¦é¢„æµ‹ä¸‹ä¸€å¥ï¼Œå› æ­¤è¾“å…¥æ˜¯æœ‰å…³è”çš„å¥å­ã€‚è€Œæ–‡æœ¬åˆ†ç±»åªæœ‰ä¸€ä¸ªå¥å­ï¼Œé‚£ä¹ˆ Segement embeddings å°±å…¨éƒ¨æ˜¯ 0ã€‚
3. **Position Embeddings:** BERTèƒ½å¤Ÿå¤„ç†æœ€é•¿512ä¸ªtokençš„è¾“å…¥åºåˆ—ã€‚é€šè¿‡è®©BERTåœ¨å„ä¸ªä½ç½®ä¸Šå­¦ä¹ ä¸€ä¸ªå‘é‡è¡¨ç¤ºæ¥è®²åºåˆ—é¡ºåºçš„ä¿¡æ¯ç¼–ç è¿›æ¥ã€‚
4. **[CLS]çš„ä½œç”¨ï¼š** BERTåœ¨ç¬¬ä¸€å¥å‰ä¼šåŠ ä¸€ä¸ª[CLS]æ ‡å¿—ï¼Œæœ€åä¸€å±‚è¯¥ä½å¯¹åº”å‘é‡å¯ä»¥ä½œä¸ºæ•´å¥è¯çš„è¯­ä¹‰è¡¨ç¤ºï¼Œä»è€Œç”¨äºä¸‹æ¸¸çš„åˆ†ç±»ä»»åŠ¡ç­‰ã€‚

![image-20240413151525938](.\assets\image-20240413151525938.png)



### Summary

1. Feature-based çš„æ–¹æ³•ç»™ä¸‹æ¸¸ä»»åŠ¡æä¾›ä¸€ä¸ªæœ‰ä¸Šä¸‹æ–‡çš„word embeddingï¼Œå›ºå®šäº†è¾“å…¥çš„featureï¼Œæ•ˆæœæ¯”è¾ƒæœ‰é™
2. Fine-tuningçš„æ–¹æ³•çš„å‚æ•°åœ¨æ•´ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­å¾—åˆ°è°ƒæ•´ï¼Œæ€§èƒ½æ›´é«˜
3. BERTçš„é¢„è®­ç»ƒå’Œä¸‹æ¸¸ä»»åŠ¡å­˜åœ¨è¾ƒå¤§çš„gapï¼Œè®­ç»ƒçš„æ•ˆç‡æ¯”è¾ƒä½ï¼Œå› ä¸ºåªæœ‰15%çš„å•è¯è¢«é¢„æµ‹ï¼Œè®­ç»ƒçš„çª—å£è¾ƒå°ï¼Œåªæœ‰512



### GPT-3

**Excellent few-shot/in-context learning ability  **

![image-20240413153852645](.\assets\image-20240413153852645.png)

![image-20240413153900258](.\assets\image-20240413153900258.png)

### T5 (Encoder-Decoder)

Reframe all NLP tasks into a unified text-to-text-format where the input and output are always text strings .

é‡‡ç”¨Seq2Seqçš„æ–¹å¼å»ç”Ÿæˆä¸€ä¸ªå¥å­çš„ç­”æ¡ˆã€‚



### Large Model with MoE ï¼ˆMiture of Expertsï¼‰

é‡‡ç”¨MoEçš„æ–¹å¼å»å¢å¤§æ¨¡å‹çš„å‚æ•°ï¼Œè®­ç»ƒæ›´å¤§è§„æ¨¡çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚

**Key ideaï¼š** æŠŠæ¨¡å‹çš„å‚æ•°åˆ†æˆä¸€å—ä¸€å—çš„æ¨¡å—ï¼Œæ¯æ¬¡çš„æ¨¡å‹è¾“å…¥åªè°ƒç”¨å…¶ä¸­éƒ¨åˆ†å­æ¨¡å—æ¥å‚ä¸è®¡ç®—

![image-20240413154236349](.\assets\image-20240413154236349.png)





# Prompt

### T5

é‡‡ç”¨Seq2Seqçš„æ–¹å¼å»è®­ç»ƒæ¨¡å‹ï¼Œç»™æ¨¡å‹ä¸€ä¸ªå¥å­è®©ä»–è¾“å‡ºæˆ‘ä»¬å¸Œæœ›çš„ç»“æœ

1. Encoder-decoder with 11 billion parameters  
2. Cast tasks to seq2seq manner with simple demonstrations  
3. A decoder is trained to output the desired tokens  

### GPT3

1. Huge model with 175 billion parameters  
2. No parameters are updated at all  
3. Descriptions (Prompts) + Few-shot examples to generate tokens  



### How to Adapt Large-scale PLMs

1. å…¨å‚æ•°å¾®è°ƒå¤§æ¨¡å‹æ˜¯ä¸ç°å®çš„ï¼›
2. å¯¹æ¯ä¸€ä¸ªä»»åŠ¡éƒ½å¾®è°ƒä¸€ä¸ªæ¨¡å‹ï¼Œé‚£ä¹ˆå¦‚æœä¸‹æ¸¸ä»»åŠ¡å¾ˆå¤šçš„è¯å°±éœ€è¦å­˜å‚¨nä¸ªæ¨¡å‹å®ä¾‹ï¼›

å› æ­¤æˆ‘ä»¬è¦é«˜æ•ˆçš„å¾®è°ƒæ¨¡å‹ï¼Œä¾‹å¦‚**prompt learning**ï¼Œ **delta tuning**(åªå¾®è°ƒéƒ¨åˆ†å‚æ•°)

![image-20240414233932579](assets/image-20240414233932579.png)



### Prompt Learning

pretrain å’Œ fine-tuningä¹‹é—´å­˜åœ¨å¾ˆå¤šçš„GAP

![image-20240414234018377](assets/image-20240414234018377.png)

**Add additional context(template) with a [MASK] position**

å³ä½¿ä»»åŠ¡ä¸åŒï¼Œä½†æ˜¯æˆ‘ä»¬å¯ä»¥åˆ©ç”¨è¿™ç§èŒƒå¼å»ç»Ÿä¸€å¾®è°ƒçš„è¿‡ç¨‹ã€‚

![image-20240414234128261](assets/image-20240414234128261.png)

**Example:**

æƒ…æ„Ÿåˆ†ç±»

![image-20240414234538026](assets/image-20240414234538026.png)



### å¦‚ä½•é€‰æ‹©é¢„è®­ç»ƒçš„æ¨¡å‹

1. Auto-regressive (GPT, OPT)ï¼Œ å•å‘attentionï¼Œæ‰€ä»¥æœ‰æ›´å¥½çš„ç”Ÿæˆèƒ½åŠ›ï¼Œé€‚åˆè¶…å¤§è§„æ¨¡çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œé‡‡ç”¨å¦‚ä¸‹çš„[MASK]åœ¨æœ€åçš„prompt

![image-20240414235201191](assets/image-20240414235201191.png)

2. Maked Language Modeling (BERT), åŒå‘attensionï¼Œæ‰€ä»¥æœ‰æ›´å¥½çš„ç†è§£èƒ½åŠ›ï¼Œæ¨¡æ¿çš„[MASK]ä¸ç”¨è€ƒè™‘ä½ç½®

![image-20240414235313703](assets/image-20240414235313703.png)

3. Encoder-Decoder(T5, BART), Encoderé˜¶æ®µæ˜¯åŒå‘çš„attentionï¼ŒDecoderæ˜¯å•å‘çš„attetion

![image-20240414235418163](assets/image-20240414235418163.png)



### å¦‚ä½•é€‰æ‹©Template

1. äººå·¥çš„æ„é€ ï¼Œæ•ˆæœä¸€èˆ¬éƒ½ä¸é”™ï¼Œä½†æ˜¯è¦è€ƒè™‘ä»»åŠ¡çš„ç‰¹æ€§æ¥æ„é€ ä¸åŒçš„templateï¼Œéœ€è¦äººçš„å…ˆéªŒçŸ¥è¯†
2. è‡ªåŠ¨çš„ç”Ÿæˆï¼Œé‡‡ç”¨æœç´¢çš„æ–¹å¼å»ç”ŸæˆTemplateï¼Œæˆ–è€…å¯ä»¥ä¸ä¸€å®šæ˜¯æ–‡æœ¬ï¼Œé€šè¿‡è®­ç»ƒçš„æ–¹å¼

**Example1**

![image-20240414235738277](assets/image-20240414235738277.png)

**Example2: Extract World Knowledge**

1. Copy the entity in the Template
2. Predict fine-grained entity types
3. Extract world knowledge

![image-20240415000019523](assets/image-20240415000019523.png)

**Example3: Structured Template**

é‡‡ç”¨åŒä¸€ç§Templateï¼Œè®­ç»ƒä¸åŒçš„ä»»åŠ¡ã€‚

1. Key-value Pairs for all the prompts
2. Organize different tasks to a structured format

![image-20240415000503436](assets/image-20240415000503436.png)



**Example4ï¼š Automatic Search**

Gradient-based search of prompts based on existing words

æˆ‘ä»¬å‘ç°ï¼Œè®­ç»ƒå¾—åˆ°çš„templateå¯èƒ½è¯­æ³•ä¸åŒã€‚

å¯ç¤ºï¼šPromptçš„ç›®çš„æ˜¯è¦å»è§¦å‘æˆ‘ä»¬æƒ³è¦çš„tokenï¼Œå®é™…ä¸Šå¯¹äºæœºå™¨æ¥è¯´å¯èƒ½ä¸ä¸€å®šè¦ä½¿ç”¨å’Œäººä¸€æ ·çš„prompt

![image-20240415000738306](assets/image-20240415000738306.png)

**Example5ï¼š Using a encoder-decoder model to generate prompts**

![image-20240415001123812](assets/image-20240415001123812.png)

**Example6: Promptint with Continuous Templates P-tuning**

å°†æ¨¡å‹å†»ç»“ä½ï¼Œåªå»è®­ç»ƒé‚£äº›prompt embedding

1. Genetative models for NLU by optimizing continuous prompts
2. P-tuning v1: prompts to the input label (with Reparameterization)
3. P-tuning v2: prompts to ever layer (like prefix-tuning)

![image-20240415001301517](assets/image-20240415001301517.png)

**ç»“è®ºï¼š**

1. åœ¨Few-shotæ—¶å€™æ€§èƒ½ç‰¹åˆ«å¥½
2. ä¸åŒçš„æ¨¡æ¿å¯¹ç»“æœæœ‰å¾ˆå¤§çš„å½±å“



### å¦‚ä½•Verbalizer

1. äººå·¥è®¾è®¡
2. åˆ©ç”¨é¢å¤–çš„çŸ¥è¯†æ‰©å……

Verbalizerå…¶å®å°±æ˜¯å°†æ ‡ç­¾æ˜ å°„æˆæ ‡ç­¾è¯çš„è¿‡ç¨‹ï¼Œ Mapping: Answer->Unfixed Labelsï¼Œä¾‹å¦‚ï¼š

![image-20240415112231342](assets/image-20240415112231342.png)

ä¸Šå›¾ä¸­ï¼Œpositiveå¯ä»¥å¯¹åº”å¾ˆå¤šè¯ï¼Œå¯ä»¥å¯¹è¿™äº›è¯çš„æ¦‚ç‡åšå¹³å‡æˆ–è€…åŠ æƒå¹³å‡ï¼Œå¾—åˆ°Positiveè¿™ä¸ªç±»çš„æ¦‚ç‡ã€‚

**Verbalizerçš„å½¢å¼ï¼š**

1. Tokens: å¯ä»¥æ˜¯ä¸€ä¸ªè¯æˆ–è€…å¤šä¸ªè¯
2. Chunksï¼š å¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²
3. Sentence

### **Verbalizerçš„æ„é€ ï¼š**

1. æ‰‹å·¥è®¾è®¡ï¼šManually design with human prior knowledge  
2. ä»ä¸€ä¸ªlabel wordå¼€å§‹å»å¯»æ‰¾åŒä¹‰è¯ï¼šStart with an initial label word, paraphrase & expand . å½“è¿™ä¸ªè¯è¡¨è¶Šæ¥è¶Šå¤§æ—¶ä¼šä¸ºæ¨¡å‹æé«˜å®¹é”™ç‡ï¼Œä½†æ˜¯ä¹Ÿä¼šå¸¦æ¥æ›´å¤šçš„å™ªéŸ³
3. Start with an initial label word, use external knowledge & expand.
4. Decompose the label with multiple tokens.
5. Virtual token and optimize the label embedding  

Verbalizerçš„ç›®çš„æ€ä¹ˆåˆ©ç”¨æ¨¡å‹é¢„æµ‹çš„åˆ†å¸ƒ

**Example1: Knowledgeable Prompting**

![image-20240415113249010](assets/image-20240415113249010.png)

**Example2: Virtual Tokens as Label Words**

Project the hidden states of [MASK] tokens to the embedding space and learn **prototypes**

The learned prototypes constitute the verbalizer and map the PLM outputs to corresponding labels  

![image-20240415113327556](assets/image-20240415113327556.png)



### Prompt Tuning

**Example1ï¼š soft prompts**

ç»™æ¨¡å‹åŠ ä¸€äº›soft promptsï¼Œåªè®­ç»ƒsoft embeddingï¼Œæ¨¡å‹çš„ç»“æœåœ¨å¤§å‚æ•°ä¸Šè·å¾—çš„æ•ˆæœä¸å…¨å‚æ•°å¾®è°ƒç±»ä¼¼ã€‚

å®é™…ä¸Šï¼ŒPrompt tuningä¸prompt learningçš„æ€è·¯å¤§ç›¸å¾„åº­ï¼Œ**prompt tuning æ˜¯å¸Œæœ›é€šè¿‡æ¨¡å‹å¼ºå¤§çš„æ‹Ÿåˆèƒ½åŠ›æ¥é€‚åº”ä¸‹æ¸¸çš„æ•°æ®ï¼Œè€Œprompt learningå¸Œæœ›æŠŠé¢„è®­ç»ƒå’Œä¸‹æ¸¸ä»»åŠ¡ä¹‹é—´çš„gapå¼¥è¡¥èµ·æ¥ã€‚**



![image-20240415141156578](assets/image-20240415141156578.png)



**Example2ï¼š Multi-task pre-training with hand-crafted prompts (instruction-tuning)**

![image-20240415141721870](assets/image-20240415141721870.png)



### Summary Prompt-Learning

1. **A comprehensive framework** that considers PLMs, downstream tasks, and human prior knowledge
2. The design of **Template & Verbalizer** is crucial  
3. Prompt-learning has promising performance in low-data regime, and high variance with the select of templates
4. Prompt-learning has broad applications  



### Delta Tuning

Only updating **a small amount of parameters** of PLMs , Keeping the parameters of the PLM **fixed **

![image-20240415142357895](assets/image-20240415142357895.png)

**Why Parameter Efficient Work**

Pre-training can learn **Universal Knowledge **



**Delta Tuningçš„èŒƒå¼ï¼š**

1. **Addition-based  methods** introduce extra trainable neural modules or parameters that do not exist in the original model; å¢åŠ ä¸€äº›é¢å¤–çš„å¯å­¦ä¹ å‚æ•°
2. **Specification-based  methods** specify certain parameters in the original model or process become trainable, while others frozen; éƒ¨åˆ†å‚æ•°å¯å­¦ä¹ 
3. Reparameterization-based methods reparameterize existing parameters to a parameter-efficient form by transformation.

![image-20240415142801314](assets/image-20240415142801314.png)



### Addition-basedï¼šAdapters

1. Injecting small neural modules (adapters) into Transformer Layer  
2. **Only fine-tuning adapters** and keeping other parameters frozen  
3. Adapters are **down-projection and up-projection **

![image-20240415142934817](assets/image-20240415142934817.png)

**Example1: Move the Adapter Out of the Backbone**

Ladder Side Networkï¼Œä½¿å¾—æ¢¯åº¦åå‘ä¼ æ’­çš„æ—¶å€™ä¸éœ€è¦ä¼ é€’åˆ°backboneä¸­ï¼ŒèŠ‚çœäº†å¾ˆå¤šè®¡ç®—å’Œæ˜¾å­˜

![image-20240415143103868](assets/image-20240415143103868.png)

**Example2ï¼šPrefix-Tuning**

**Inject prefixes (soft prompts) to each layer of the Transformer** ï¼ŒOnly optimizing the prefixes of the model ã€‚åœ¨æ¯ä¸€å±‚çš„Transformerå‰é¢åŠ å…¥soft tokenï¼Œè¿™ä¸ªæ–¹æ³•å’ŒPrompt-Tuningæ¯”è¾ƒç›¸ä¼¼ã€‚



### Specification-based

**Example1ï¼š BitFit**

A simple strategy: only updating the bias terms ï¼Œ åªå»å¾®è°ƒæ¨¡å‹çš„biaså‚æ•°

![image-20240415143618145](assets/image-20240415143618145.png)

### Reparameterization-based   é‡å‚æ•°åŒ–æ–¹æ³•

**é¢„è®­ç»ƒæ¨¡å‹æ‹¥æœ‰æå°çš„å†…åœ¨ç»´åº¦(instrisic dimension)ï¼Œå³å­˜åœ¨ä¸€ä¸ªæä½ç»´åº¦çš„å‚æ•°ï¼Œå¾®è°ƒå®ƒå’Œåœ¨å…¨å‚æ•°ç©ºé—´ä¸­å¾®è°ƒèƒ½èµ·åˆ°ç›¸åŒçš„æ•ˆæœ**ã€‚

**Example1ï¼š Low-dimension space**

å‡è®¾ï¼šPLMä¼˜åŒ–çš„è¿‡ç¨‹å¯ä»¥åœ¨ä¸€ä¸ªä½ç»´ç©ºé—´å†…å®Œæˆï¼Œå°†nä¸ªä»»åŠ¡çš„ä¼˜åŒ–å‹ç¼©åˆ°ä¸€ä¸ªä½ç»´çš„å­ç©ºé—´é‡Œï¼Œåœ¨ä½ç»´çš„ç©ºé—´é‡Œé¢å»è®­ç»ƒå‚æ•°ï¼Œå†å°†å®ƒæ”¾ç¼©åˆ°åŸæ¥çš„ç»´åº¦é‡Œ

![image-20240415143727081](assets/image-20240415143727081.png)

**Example2: LoRA Low-Rank Adaptation**

Key idea: è¦ä¼˜åŒ–çš„çŸ©é˜µè™½ç„¶ä¸æ˜¯ä¸€ä¸ªä½ç§©çš„ï¼Œä½†æ˜¯æˆ‘ä»¬å¯ä»¥å¯¹å®ƒåšä¸€ä¸ªä½ç§©åˆ†è§£ï¼Œä¾‹å¦‚ï¼Œå¯¹ä¸€ä¸ª1000Ã—1000çš„çŸ©é˜µå¯ä»¥åˆ†è§£æˆä¸€ä¸ª1000Ã—2 å’Œ 2Ã—1000ï¼Œä»è€Œå‡å°‘è®¡ç®—é‡ã€‚

å¯¹äºé¢„è®­ç»ƒçš„æƒé‡çŸ©é˜µ$W_0 \in \mathbb{R}^{d \times k}$, æˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªä½ç§©åˆ†è§£æ¥è¡¨ç¤ºå‚æ•°æ›´æ–°$\Delta W$ï¼Œ å³ï¼š
$$
W_0=\Delta W = W_0 + BA,B \in \mathbb{R}^{d\times k} and A\in \mathbb{R}^{r\times k}
$$


Freeze the model weights , **Injects trainable rank-decomposition**  matrices to each Transformer layer

![image-20240415144242649](assets/image-20240415144242649.png)



**Example3: Connections**

å­˜åœ¨ä¸€ä¸ªä½ç»´ç©ºé—´å¯ä»¥è¡¨ç¤ºå¾®è°ƒå¤§é‡çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œå› æ­¤å¯ä»¥ç»„åˆå„ç§Delta-tuning

![image-20240415145359940](assets/image-20240415145359940.png)

1. **Left:** there exist a low-dimensional intrinsic subspace that could reparameterize one specific fine-tuning process (the left part).   
2. **Middle:**the change of weights during adaptation has a low intrinsic rank (the middle part)  
3. **Right: **there may exist **a common intrinsic space** that could handle the **fine-tuning for various NLP tasks** (the right part).  

![image-20240415145728133](assets/image-20240415145728133.png)

**Theoretical Analysisï¼š**

Low-dimensional representation in solution space  

Low dimensional representation in functional space  



**Automatically search the structure**

è‡ªåŠ¨åœ°ç»„åˆå„ç§Delta-Tuningï¼Œ ä»è€Œå¾—åˆ°ä¸€ä¸ªæ¯”è¾ƒç¨€ç–çš„è§£ï¼Œä½¿å¾—å³ä½¿å‚æ•°é‡æ›´å°‘çš„æ—¶å€™ä¹Ÿèƒ½å¤Ÿwork

![image-20240415150058600](assets/image-20240415150058600.png)



### Summaryï¼š Delta Tuning

1. é«˜æ•ˆçš„
2. å½“æ¨¡å‹çš„å‚æ•°å¾ˆå¤§çš„æ—¶å€™å¯ä»¥ä¸éœ€è¦å¤ªè¿‡è€ƒè™‘Adapterçš„ç»“æ„ã€‚



# å¤§æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†

### CPU vs. GPU

CPU: small number of large cores.

GPU: large number of small cores.  

![image-20240415152832206](assets/image-20240415152832206.png)

**CPU controls GPU**

![image-20240415152852009](assets/image-20240415152852009.png)

****



**Memory Components**

1. æ¨¡å‹çš„å‚æ•°
2. åä¼ çš„æ¢¯åº¦
3. æ¨¡å‹çš„ä¸­é—´ç»“æœ
4. ä¼˜åŒ–å™¨

![image-20240415153142355](assets/image-20240415153142355.png)

Adamä¼˜åŒ–å™¨çš„å‚æ•°ï¼š

![image-20240415153228574](assets/image-20240415153228574.png)



#### **Collective Communication**

**Broadcastï¼š**Send data from one GPU to other GPUs.  

![image-20240415153945774](assets/image-20240415153945774.png)

**Reduceï¼š** Reduce (Sum/Average) data of all GPUs, send to one GPU.  

![image-20240415154011452](assets/image-20240415154011452.png)

**All reduce:** Reduce (Sum/Average) data of all GPUs, send to all GPUs.  

![image-20240415154036966](assets/image-20240415154036966.png)

**Reduce Scatter:** Reduce (Sum/Average) data of all GPUs, send portions to all GPUs.  

![image-20240415154109357](assets/image-20240415154109357.png)

**All Gather:** Gather data of all GPUs, send all GPUs.  

![image-20240415154152039](assets/image-20240415154152039.png)



****



#### Data Parallel

1. éœ€è¦ä¸€ä¸ªå‚æ•°æœåŠ¡å™¨ï¼ˆå®é™…ä¸Šè¿™ä¸ªå‚æ•°æœåŠ¡å™¨å°±æ˜¯0å·æ˜¾å¡ï¼‰ï¼Œç”¨æ¥ä¿å­˜æ¨¡å‹çš„å‚æ•°ä»¥åŠå®Œæ•´çš„ä¸€æ‰¹æ•°æ®
2. Forwardï¼š

â€‹	å‚æ•°æœåŠ¡å™¨ä¸­çš„æ¨¡å‹å‚æ•°ä¼šè¢«å¤åˆ¶åˆ°æ‰€æœ‰çš„æ˜¾å¡ä¸­ï¼ŒæŠŠæ•°æ®åˆ‡åˆ†æˆä¸‰ä»½ï¼Œè¿›è¡Œå‰å‘ä¼ æ’­

3. Backwardï¼š

â€‹	æ¯ä¸ªæ˜¾å¡ä¸­çš„è®¡ç®—æ¢¯åº¦ï¼Œåè¿›è¡Œèšåˆï¼Œä¼ é€’åˆ°å‚æ•°æœåŠ¡å™¨ä¸­ã€‚åˆ©ç”¨ä¼˜åŒ–å™¨å¯¹å‚æ•°æœåŠ¡å™¨ä¸­çš„æ¨¡å‹è¿›è¡Œå‚æ•°æ›´æ–°ï¼Œå†é‡æ–°åˆ†å‘å‚æ•°ç»™æ˜¾å¡

![image-20240415153341517](assets/image-20240415153341517.png)



#### Distributed Data Parallel

ä¸éœ€è¦å‚æ•°æœåŠ¡å™¨ï¼Œæ¯å¼ æ˜¾å¡å„è‡ªçš„ä¿è¯è‡ªå·±çš„å‚æ•°æ›´æ–°

1. Forward:

â€‹	æ¯ä¸ªæ˜¾å¡å¾—åˆ°ä¸€éƒ¨åˆ†çš„è¾“å…¥

2. Backwardï¼š

â€‹	æ¯ä¸ªæ˜¾å¡å„è‡ªæ›´æ–°å‚æ•°ï¼Œä¸ºäº†è®©æ¯å¼ æ˜¾å¡å¾—åˆ°åŒæ ·çš„æ¢¯åº¦ï¼Œé‡‡ç”¨All Reduceç®—å­

![image-20240415154501011](assets/image-20240415154501011.png)



#### Model Parallel

åŸç†ï¼š å¯ä»¥å°†ä»¥ä¸‹çš„çŸ©é˜µä¹˜æ³•åˆ†å¾ˆå¤šä¸ªéƒ¨åˆ†ï¼Œå†è¿›è¡Œç»“æœçš„æ‹¼æ¥ï¼Œè¿™æ ·å°±å¯ä»¥æŠŠæ¨¡å‹çš„å‚æ•°åˆ†å‘åˆ°ä¸åŒçš„æ˜¾å¡ä¸­

![image-20240415155241994](assets/image-20240415155241994.png)

æ¯ä¸ªGPUçš„è¾“å…¥æ•°æ®æ˜¯ç›¸åŒçš„ï¼Œå°†æ¨¡å‹åˆ‡åˆ†å¾ˆå¤šä¸ªå°ä»½ï¼Œå°†è¾“å‡ºç»“æœè¿›è¡Œæ‹¼æ¥å¾—åˆ°æœ€ç»ˆçš„ç»“æœï¼Œæœ€åé‡‡ç”¨All Gather

![image-20240415155358621](assets/image-20240415155358621.png)



#### ZEROï¼ŒZero Redundancy Optimizer

å¯¹äºæ¨¡å‹å¹¶è¡Œæ¥è¯´ï¼Œæ¯ä¸ªGPUä¸Šçš„å‚æ•°æ›´æ–°éƒ½æ˜¯ä½¿ç”¨çš„åŒæ ·ä¸€æ‰¹çš„æ¢¯åº¦ï¼Œä»–ä»¬å„è‡ªå»è¿›è¡Œæ¨¡å‹çš„ä¼˜åŒ–å¸¦æ¥äº†è®¡ç®—ä¸Šçš„é‡å¤å’Œå†—ä½™ï¼Œä¸ºäº†æ¶ˆé™¤å†—ä½™ï¼Œæ¯å¼ æ˜¾å¡åªè®¡ç®—éƒ¨åˆ†çš„æ¢¯åº¦æ›´æ–°éƒ¨åˆ†çš„å‚æ•°ã€‚

æ¯å¼ GPUä¸Šä¿å­˜äº†å®Œæ•´çš„æ¨¡å‹å‚æ•°å’Œä¸€éƒ¨åˆ†çš„æ•°æ®ï¼Œæ¯å¼ GPUä¼šå¾—åˆ°ä¸åŒçš„æ¢¯åº¦ï¼Œé€šè¿‡Reduce Scatterä½¿å¾—æ¯ä¸ªGPUä¸Šæ¨¡å‹åªæ›´æ–°éƒ¨åˆ†çš„å‚æ•°ï¼Œæœ€åé‡‡ç”¨All Gatherå°†æ¨¡å‹çš„å‚æ•°è¿›è¡Œæ‹¼æ¥ã€‚

Zero2ï¼šåœ¨æ¢¯åº¦åå‘ä¼ æ’­çš„è¿‡ç¨‹ä¸­è®¡ç®—Gradient*åŒæ—¶ä¸æ–­åœ°é‡Šæ”¾Gradientçš„å‚æ•°

Zero3ï¼š æ¯å¼ æ˜¾å¡ä¸Šå¯ä»¥åªä¿å­˜å®ƒæ‰€è´Ÿè´£çš„å‚æ•°æ›´æ–°çš„é‚£éƒ¨åˆ†å‚æ•°ï¼Œå› æ­¤åœ¨å‰å‘ä¼ æ’­çš„è¿‡ç¨‹ä¸­é€šè¿‡All Gatherçš„æ–¹å¼æ¥è·å¾—æ¨¡å‹çš„å®Œæ•´å‚æ•°ï¼Œä½¿ç”¨å®Œåå³å¯åˆ é™¤

![image-20240415155854472](assets/image-20240415155854472.png)

![image-20240415161110869](assets/image-20240415161110869.png)



#### Pipeline Parallel

å°†æ¨¡å‹çš„ä¸åŒå±‚å‘æ”¾ç»™ä¸åŒçš„GPUï¼Œå› æ­¤æ¯å¼ æ˜¾å¡ä¸­åªéœ€è¦ä¿ç•™éƒ¨åˆ†çš„å‚æ•°ï¼Œéƒ¨åˆ†çš„æ¢¯åº¦ï¼Œéƒ¨åˆ†çš„ä¼˜åŒ–å™¨ï¼Œéƒ¨åˆ†çš„ä¸­é—´ç»“æœã€‚

å¼Šç«¯ï¼š åœ¨ä¸€ä¸ªæ˜¾å¡è®¡ç®—çš„æ—¶å€™å…¶ä»–çš„æ˜¾å¡å¤„äºç©ºé—²çŠ¶æ€

![image-20240415161358744](assets/image-20240415161358744.png)



#### æ··åˆç²¾åº¦è®­ç»ƒ

é‡‡ç”¨FP16å‚æ•°å’Œæ¢¯åº¦è¿›è¡Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼Œåˆ©ç”¨FP32çš„å‚æ•°ç”¨äºæ›´æ–°ä¼˜åŒ–å™¨

FP16çš„ä¼˜ç‚¹ï¼š

1. è®¡ç®—é€Ÿåº¦æ›´å¿«
2. éœ€è¦æ›´å°çš„ç©ºé—´
3. åŒæ—¶å‚æ•°çš„ç©ºé—´èŒƒå›´è¶³å¤Ÿå¤§äº†

FP16çš„ç¼ºç‚¹ï¼š

1. å‚æ•°çš„æ›´æ–°$Weight update = gradient*lr$ï¼Œ FP16çš„æœ€å°çš„èŒƒå›´åœ¨$6.1e-5$ï¼Œé‚£ä¹ˆåœ¨å‚æ•°æ›´æ–°çš„è¿‡ç¨‹ä¸­å¯èƒ½ä¼šå‡ºç°ä¸‹æº¢çš„é—®é¢˜ï¼Œä½¿å¾—å‚æ•°çš„æ›´æ–°ä¸º0

![image-20240415162910957](assets/image-20240415162910957.png)



#### Offloading

å¯¹äºAdamæ¥è¯´ï¼Œä¼˜åŒ–å™¨çš„å‚æ•°ä¼šæ˜¯æ¨¡å‹å‚æ•°çš„ä¸¤å€ä»¥ä¸Šï¼Œå› æ­¤å¦‚æœæŠŠä¼˜åŒ–å™¨æ”¾åœ¨æ˜¾å¡ä¸Šä¼šå æ®å¤§é‡çš„æ˜¾å­˜ã€‚

å› æ­¤æŠŠä¼˜åŒ–å™¨çš„å‚æ•°æ”¾åˆ°CPUä¸­ï¼Œå‚æ•°æ›´æ–°æ—¶éœ€è¦æŠŠæ¨¡å‹çš„å‚æ•°ä»GPUä¼ é€’åˆ°CPUï¼Œåœ¨CPUä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œå†å°†ä¼˜åŒ–çš„ç»“æœé‡æ–°ä¼ ä¼šGPU

![image-20240415163203134](assets/image-20240415163203134.png)



#### Overlapping

åœ¨GPUä¸­memoryçš„æ“ä½œä¸€èˆ¬æ˜¯å¼‚æ­¥çš„ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆç»™memoryå‘é€ä¸€ä¸ªè¯·æ±‚ï¼Œç„¶åå»è¿›è¡Œå…¶ä»–çš„è®¡ç®—ï¼Œè®¡ç®—å®Œæˆä¹‹åå¯¹memoryçš„è¯·æ±‚è¿›è¡Œæ¥æ”¶

ä¾‹å¦‚ä¸‹é¢çš„æ“ä½œï¼Œæˆ‘ä»¬åœ¨æ‰§è¡Œlayer1çš„è®¡ç®—ä¹‹å‰å…ˆgather layer2çš„å‚æ•°ï¼Œåœ¨æ‰§è¡Œlayer2çš„è®¡ç®—ä¹‹å‰gather layer3çš„å‚æ•°

![image-20240415163611895](assets/image-20240415163611895.png)



#### checkpoint

ç”¨äºå‡å°‘ä¸­é—´ç»“æœçš„ä¿å­˜ï¼Œä¾‹å¦‚åªä¿å­˜æ¯ä¸ªblockçš„è¾“å…¥å’Œè¾“å‡ºï¼Œè¿™æ ·åœ¨åå‘ä¼ æ’­çš„æ—¶å€™å¯¹è¾“å…¥è¿›è¡Œä¸€ä¸ªé‡è®¡ç®—å°±å¯ä»¥å¾—åˆ°æ¢¯åº¦ä¿¡æ¯ã€‚

![image-20240415164015284](assets/image-20240415164015284.png)



#### çŸ¥è¯†è’¸é¦

**what is knowledge:** æ¨¡å‹çš„å‚æ•°çš„è¿æ¥å’Œç»„åˆï¼Œæ›´æŠ½è±¡æ¥è¯´å°±æ˜¯è¾“å‡ºå’Œè¾“å‡ºçš„å‡½æ•°çš„æ˜ å°„å…³ç³»ã€‚

å› æ­¤å­¦ç”Ÿæ¨¡å‹çš„ç›®æ ‡æ˜¯å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„ä¸€ä¸ªå­ç©ºé—´çš„æ˜ å°„ï¼Œå› ä¸ºå°çš„æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›å°äºæ•™å¸ˆæ¨¡å‹ã€‚

å¯¹äºç›‘ç£ä¿¡å·æ¥è¯´æ˜¯ä¸€ä¸ªå€¼ï¼Œè€Œå¯¹äºæ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºæ¥è¯´æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œè¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒæœ‰æ—¶å€™èƒ½å¤Ÿæä¾›æ›´å¤šçš„ç›‘ç£ä¿¡æ¯ã€‚

**PKDï¼š**Learn from multiple intermediate layers of the teacher model  

![image-20240415164936927](assets/image-20240415164936927.png)



**TinyBERT**

ä¸ä»…å¯¹è¾“å…¥è¾“å‡ºçš„ç‰¹å¾è¿›è¡Œçº¦æŸï¼Œè¿˜å¯¹attention matrixç›¸ä¼¼åº¦è¿›è¡Œåˆ»ç”»ã€‚

![image-20240415165337001](assets/image-20240415165337001.png)



#### Model Pruning

åœ¨å·ç§¯ç¥ç»ç½‘ç»œä¸­ï¼Œå·ç§¯æ ¸æ˜¯å¾ˆç¨€ç–çš„ï¼Œæ‰€ä»¥èƒ½ä¸èƒ½å«ä¸º0çš„å‚æ•°ç»™ä¸¢å¼ƒæ‰ï¼›æˆ–è€…é€šè¿‡æ¢¯åº¦çš„å¤§å°æ¥è¡¡é‡å‚æ•°çš„é‡è¦æ€§ã€‚

ç»“æ„åŒ–å‰ªæï¼šä¸€æ¬¡æ€§å°†çŸ©é˜µä¸­çš„ä¸€è¡Œæˆ–è€…ä¸€å—åˆ é™¤ï¼Œä½¿å¾—GPUæ›´å¥½åœ°åŠ é€Ÿ

**Example1ï¼š**  å°†Attention çš„æŸä¸ª headä¸¢å¼ƒæ‰

**Example2ï¼š** å°†æ¨¡å‹çš„æŸå‡ å±‚ç»™ä¸¢æ‰



#### Model Quantization

Reduce the number of bits used to represent a value  

**Example1ï¼š BinaryBERT**

1. Initialize a binary model with the ternary model by weight splitting  
2. Fine-tune the binary model  

![image-20240415170823016](assets/image-20240415170823016.png)





### Tokenizer

```python
from tokenizers import Tokenizer
from tokenizers.models import BPE
tokenizer = Tokenizer(BPE(unk_token="[UNK]")) # å®šä¹‰æœªçŸ¥çš„ç¬¦å·

# è®­ç»ƒè‡ªå·±çš„åˆ†è¯å™¨ï¼Œå…¶ä¸­special_tokensä¼šè¢«æ˜ å°„åˆ°0,1,2,3...
from tokenizers.trainers import BpeTrainer
trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

# ä½¿ç”¨é¢„æ ‡è®°å™¨å°†ç©ºæ ¼åŒºåˆ†å¼€æ¥
from tokenizers.pre_tokenizers import Whitespace
tokenizer.pre_tokenizer = Whitespace()

# è®­ç»ƒè‡ªå·±çš„åˆ†è¯å™¨
files = [f"data/wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]
tokenizer.train(files, trainer)

# ä¿å­˜
tokenizer.save("data/tokenizer-wiki.json")

tokenizer = Tokenizer.from_file("data/tokenizer-wiki.json")

output = tokenizer.encode("Hello, y'all! How are you ğŸ˜ ?")
print(output.tokens)
# ["Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?"]
print(output.ids)
# [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]

# post-process
from tokenizers.processors import TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",   # å¥å­æ¨¡æ¿ï¼Œå…¶ä¸­Aä¸ºå¥å­
    pair="[CLS] $A [SEP] $B:1 [SEP]:1", # å¥å­å¯¹æ¨¡æ¿
    special_tokens=[
        ("[CLS]", tokenizer.token_to_id("[CLS]")), # æŒ‡å®šæˆ‘ä»¬ä½¿ç”¨çš„ç‰¹æ®Šæ ‡è®°åŠå…¶ ID
        ("[SEP]", tokenizer.token_to_id("[SEP]")), # æŒ‡å®šæˆ‘ä»¬ä½¿ç”¨çš„ç‰¹æ®Šæ ‡è®°åŠå…¶ ID
    ],
)

output = tokenizer.encode("Hello, y'all! How are you ğŸ˜ ?")
print(output.tokens)
# ["[CLS]", "Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?", "[SEP]"]
output = tokenizer.encode("Hello, y'all!", "How are you ğŸ˜ ?")
print(output.tokens)
# ["[CLS]", "Hello", ",", "y", "'", "all", "!", "[SEP]", "How", "are", "you", "[UNK]", "?", "[SEP]"]
print(output.type_ids)
# [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]

# using pretained tokenizer
from tokenizers import Tokenizer
tokenizer = Tokenizer.from_pretrained("bert-base-uncased")
```



