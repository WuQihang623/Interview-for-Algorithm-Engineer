# 基础篇

### Prefix LM和casual LM

- prefix LM（前缀语言模型）：在输入序列的开头添加一个可学习的任务相关的前缀，然后使用这个前缀和输入序列一起生成输出。这种方法可以引导模型生成适应特定任务的输出。
- causal LM（因果语言模型）：也称为自回归语言模型，它根据之前生成的 token 预测下一个 token。在生成文本时，模型只能根据已经生成的部分生成后续部分，不能访问未来的信息。



### 什么是涌现能力

涌现能力（Emergent Ability）是指模型在训练过程中突然表现出的新的、之前未曾预料到的能力。这种现象通常发生在大型模型中，**原因是大型模型具有更高的表示能力和更多的参数，可以更好地捕捉数据中的模式和关联**。随着模型规模的增加，它们能够自动学习到更复杂、更抽象的概念和规律，从而展现出涌现能力。



### 主流的开源大模型

- GPT系列：由OpenAI开发的生成式预训练模型，如GPT-3。（解码器结构）
- BERT系列：由Google开发的自回归式预训练模型，如BERT、RoBERTa等。（编码器结构）
- T5系列：由Google开发的基于Transformer的编码器-解码器模型，如T5、mT5等。（编码器-解码器结构）



### prefix LM，Causal LM, encoder-decoder的区别

- prefix LM：通过在输入序列前添加可学习的任务相关前缀，引导模型生成适应特定任务的输出。优点是可以**减少对预训练模型参数的修改，降低过拟合风险**；缺点是可能**受到前缀表示长度的限制，无法充分捕捉任务相关的信息**。
- causal LM：根据之前生成的 token 预测下一个 token，可以生成连贯的文本。优点是可以**生成灵活的文本，适应各种生成任务**；缺点是**无法访问未来的信息，可能生成不一致或有误的内容**。
- encoder-decoder：由编码器和解码器组成，编码器将输入序列编码为固定长度的向量，解码器根据编码器的输出生成输出序列。优点是**可以处理输入和输出序列不同长度的任务**，如机器翻译；缺点是**模型结构较为复杂，训练和推理计算量较大**。



